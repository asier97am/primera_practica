{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a193e8ba",
   "metadata": {},
   "source": [
    "# 11 - Fill Mask (Relleno de espacios)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/fill_mask.webp\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e01b0b",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---Modelos-Fill-Mask\" data-toc-modified-id=\"1---Modelos-Fill-Mask-1\">1 - Modelos Fill Mask</a></span></li><li><span><a href=\"#2---Pipeline-de-Transformers-para-rellenar\" data-toc-modified-id=\"2---Pipeline-de-Transformers-para-rellenar-2\">2 - Pipeline de Transformers para rellenar</a></span></li><li><span><a href=\"#3---Usando-el-modelo-de-rellenado\" data-toc-modified-id=\"3---Usando-el-modelo-de-rellenado-3\">3 - Usando el modelo de rellenado</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1-Tokenizador\" data-toc-modified-id=\"3.1-Tokenizador-3.1\">3.1 Tokenizador</a></span></li><li><span><a href=\"#3.2---Modelo-de-rellenado-(fill-mask)\" data-toc-modified-id=\"3.2---Modelo-de-rellenado-(fill-mask)-3.2\">3.2 - Modelo de rellenado (fill mask)</a></span></li></ul></li><li><span><a href=\"#4---Otro-modelo-de-relleno\" data-toc-modified-id=\"4---Otro-modelo-de-relleno-4\">4 - Otro modelo de relleno</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e10a10",
   "metadata": {},
   "source": [
    "## 1 - Modelos Fill Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3706f946",
   "metadata": {},
   "source": [
    "Los modelos Fill Mask, o de rellenado de espacios, son un tipo de modelo de NLP que se utiliza principalmente para predecir palabras faltantes en una oración. Estos modelos son entrenados para entender el contexto de una oración y seleccionar la palabra que mejor completa el espacio en blanco designado por una máscara.\n",
    "\n",
    "**Funcionamiento de los modelos Fill Mask**:\n",
    "\n",
    "1. Preentrenamiento: Durante la fase de preentrenamiento, estos modelos son expuestos a grandes cantidades de texto. Un porcentaje de las palabras en el texto se oculta (se \"enmascara\") y el modelo debe predecir la palabra correcta para cada máscara basándose en el contexto proporcionado por las palabras restantes.\n",
    "\n",
    "\n",
    "2. Entrenamiento de Tareas Específicas: Después del preentrenamiento, los modelos pueden ser ajustados para tareas específicas que requieran comprensión del lenguaje, aunque el uso principal sigue siendo la predicción de texto.\n",
    "\n",
    "\n",
    "\n",
    "**Ejemplos de modelos Fill Mask**:\n",
    "\n",
    "\n",
    "1. BERT (Bidirectional Encoder Representations from Transformers): Uno de los modelos más conocidos que utiliza la técnica de máscara. BERT introduce tokens de máscara en el texto y entrena el modelo para predecir la palabra original en la posición de la máscara.\n",
    "\n",
    "\n",
    "2. RoBERTa (Robustly Optimized BERT Approach): Una variante de BERT que ha sido optimizada y ajustada para mejorar el rendimiento en tareas de predicción de palabras enmascaradas.\n",
    "\n",
    "\n",
    "\n",
    "**Aplicaciones de los modelos Fill Mask**:\n",
    "\n",
    "\n",
    "+ Mejora de Sistemas de Recomendación de Palabras: Utilizados en editores de texto y software de procesamiento de palabras para ofrecer sugerencias de palabras o para completar automáticamente frases.\n",
    "\n",
    "\n",
    "+ Entrenamiento de Modelos de Lenguaje: Ayuda a mejorar la capacidad de los modelos para entender y generar lenguaje natural.\n",
    "\n",
    "\n",
    "+ Análisis de Sentimiento y Extracción de Información: Al entender mejor el contexto de las palabras dentro de las frases, estos modelos pueden ayudar en la extracción de información relevante y en la evaluación de sentimientos en textos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db3cf5",
   "metadata": {},
   "source": [
    "## 2 - Pipeline de Transformers para rellenar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c62411",
   "metadata": {},
   "source": [
    "Vamos a usar un modelo [BERT](https://huggingface.co/google-bert/bert-base-multilingual-cased), creado por Google, para realizar la tarea de rellenado de espacios vacíos usando el pipeline de transformers. Tiene un aproximado de 720Mb. Está preentrenado con 104 idiomas usando las Wikipedia. Fue presentado por primera vez en este [paper](https://arxiv.org/abs/1810.04805). \n",
    "\n",
    "\n",
    "El modelado de lenguaje enmascarado (MLM) toma una oración, el modelo enmascara aleatoriamente el 15% de las palabras en la entrada, luego ejecuta la oración enmascarada completa a través del modelo y tiene que predecir las palabras enmascaradas. Esto es diferente de las redes neuronales recurrentes tradicionales (RNN) que generalmente ven las palabras una tras otra, o de modelos autorregresivos como GPT que internamente enmascaran los tokens futuros. Esto permite que el modelo aprenda una representación bidireccional de la oración.\n",
    "\n",
    "Para la predicción de la siguiente oración, estos modelos concatenan dos oraciones enmascaradas como entradas durante el preentrenamiento. A veces corresponden a oraciones que estaban una al lado de la otra en el texto original, a veces no. Luego, el modelo tiene que predecir si las dos oraciones seguían una a la otra o no.\n",
    "De esta manera, el modelo aprende una representación interna de los idiomas en el conjunto de entrenamiento que luego puede ser utilizada para extraer características útiles para tareas posteriores, si tenemos un conjunto de datos de oraciones etiquetadas, por ejemplo, podemos entrenar un clasificador estándar usando las características producidas por el modelo BERT como entradas.\n",
    "\n",
    "\n",
    "Para usar el modelo y hacer que rellene una frase incompleta, debemos usar el token especial `[MASK]` para indicarle que posición ocupa la palabra que tiene que generar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "693ba8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "860be537",
   "metadata": {},
   "outputs": [],
   "source": [
    "tarea = 'fill-mask'\n",
    "\n",
    "modelo = 'google-bert/bert-base-multilingual-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d783b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "fill_pipe = pipeline(task=tarea, model=modelo, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb12da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = 'La capital de España es [MASK].'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed9f49a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.6894896030426025,\n",
       "  'token': 11727,\n",
       "  'token_str': 'Madrid',\n",
       "  'sequence': 'La capital de España es Madrid.'},\n",
       " {'score': 0.039343804121017456,\n",
       "  'token': 24174,\n",
       "  'token_str': 'Zaragoza',\n",
       "  'sequence': 'La capital de España es Zaragoza.'},\n",
       " {'score': 0.020669730380177498,\n",
       "  'token': 40603,\n",
       "  'token_str': 'Málaga',\n",
       "  'sequence': 'La capital de España es Málaga.'},\n",
       " {'score': 0.020436637103557587,\n",
       "  'token': 35527,\n",
       "  'token_str': 'Valladolid',\n",
       "  'sequence': 'La capital de España es Valladolid.'},\n",
       " {'score': 0.01399324368685484,\n",
       "  'token': 41774,\n",
       "  'token_str': 'Santander',\n",
       "  'sequence': 'La capital de España es Santander.'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_pipe(frase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18edd3c3",
   "metadata": {},
   "source": [
    "Vemos que salida del pipeline nos devuelve una lista de diccionarios. Cada uno de ellos tiene cuatro keys:\n",
    "\n",
    "\n",
    "+ score: Probabilidad de respuesta correcta. Es un float.\n",
    "\n",
    "\n",
    "+ token: Id del token predicho. Es un número entero.\n",
    "\n",
    "\n",
    "+ token_str: Token predicho en formato string.\n",
    "\n",
    "\n",
    "+ sequence: Frase completa rellenada con el token predicho en formato string.\n",
    "\n",
    "\n",
    "Los diccionarios dentro de la lista están ordenados según la probabilidad de la respuesta correcta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbff3e91",
   "metadata": {},
   "source": [
    "## 3 - Usando el modelo de rellenado\n",
    "\n",
    "Usemos el modelo fuera del pipeline. Como siempre, necesitamos el tokenizador y el modelo preentrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8de56013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8d8c81",
   "metadata": {},
   "source": [
    "### 3.1 Tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "557ac945",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizador = AutoTokenizer.from_pretrained(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1dd6398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='google-bert/bert-base-multilingual-cased', vocab_size=119547, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73c74ee",
   "metadata": {},
   "source": [
    "Ya vimos anteriormente la descripción de este objeto tokenizador `BertTokenizerFast`. Los tokens especiales son: [PAD], [UNK], [CLS], [SEP] y [MASK]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce01f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = tokenizador(frase, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65975595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 10159, 12185, 10104, 12413, 10196,   103,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3db2f091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'La', 'capital', 'de', 'España', 'es', '[MASK]', '.', '[SEP]']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0d6e84",
   "metadata": {},
   "source": [
    "### 3.2 - Modelo de rellenado (fill mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a03e5744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "modelo_rellenado = AutoModelForMaskedLM.from_pretrained(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fe02ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=119547, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_rellenado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4a9d7c",
   "metadata": {},
   "source": [
    "**Componentes de BertForMaskedLM**:\n",
    "\n",
    "\n",
    "1. **BertModel**: Es el modelo base que contiene la arquitectura fundamental de BERT, incluyendo embeddings y capas de encoder.\n",
    "\n",
    "+ Embeddings: Procesa las entradas de texto en vectores antes de pasar por las capas del encoder.\n",
    "    + word_embeddings: Transforma los tokens de entrada en vectores.\n",
    "    + position_embeddings: Agrega información posicional a los embeddings de los tokens para mantener el orden de la secuencia.\n",
    "    + token_type_embeddings: Utilizado para diferenciar entre tipos de tokens (por ejemplo, en tareas que involucran dos secuencias diferentes como preguntas y respuestas).\n",
    "    + LayerNorm y Dropout: Son técnicas de normalización y regularización, respectivamente, para mejorar el entrenamiento y evitar el sobreajuste.\n",
    "\n",
    "\n",
    "\n",
    "2. **BertEncoder**: Consiste en múltiples capas de BertLayer, cada una de las cuales contiene componentes para realizar la atención y la transformación de los datos:\n",
    "\n",
    "+ BertAttention: Se encarga de la atención multi-cabeza para capturar diferentes aspectos de la información en diferentes posiciones de los datos.\n",
    "    + BertSelfAttention: Realiza cálculos de atención sobre los embeddings, usando tres transformaciones lineales para crear consultas, claves y valores.\n",
    "+ BertIntermediate: Aplica una transformación lineal para expandir las dimensiones de los datos antes de aplicar una función de activación no lineal (GELU).\n",
    "+ BertOutput: Combina la salida de la atención y la capa intermedia, y luego la proyecta de nuevo al tamaño original de los embeddings.\n",
    "\n",
    "\n",
    "3. **cls**: Específico para tareas MLM, contiene componentes para la predicción de tokens:\n",
    "\n",
    "+ BertOnlyMLMHead: Cabeza del modelo diseñada específicamente para predecir los tokens ocultos en una tarea MLM.\n",
    "    + BertLMPredictionHead: Transforma la salida del encoder antes de predecir el token original.\n",
    "        + BertPredictionHeadTransform: Aplica transformaciones adicionales antes de la predicción final.\n",
    "        + decoder: Transforma la salida del modelo a un espacio de tamaño igual al vocabulario para predecir el token enmascarado.\n",
    "\n",
    "\n",
    "\n",
    "**Funcionamiento general**:\n",
    "El proceso comienza con la entrada de texto, que se tokeniza y se convierte en embeddings. Estos embeddings pasan a través de múltiples capas de atención y transformaciones lineales dentro del encoder. La salida del encoder se procesa luego para predecir tokens enmascarados, que es el objetivo principal del modelo BertForMaskedLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2240d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta = modelo_rellenado(**vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02f0fc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 119547])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = respuesta.logits\n",
    "\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d397fc8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False,  True, False, False]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.input_ids == tokenizador.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "626183a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_index = (vector.input_ids == tokenizador.mask_token_id)[0].nonzero()[0]\n",
    "\n",
    "mask_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18863905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11727])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0, mask_index].argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b267fcf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Madrid'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = tokenizador.decode(logits[0, mask_index].argmax(axis=-1))\n",
    "\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a8bb1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La capital de España es Madrid.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frase.replace('[MASK]', resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55391279",
   "metadata": {},
   "source": [
    "## 4 - Otro modelo de relleno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b3c8f6",
   "metadata": {},
   "source": [
    "Probemos otro modelo, [XLM-RoBERTa](https://huggingface.co/FacebookAI/xlm-roberta-large) desarrollado por Facebook. Este modelo es más grande que el anterior, pesa aproximadamente 2.25Gb. Vamos a usarlo a través del pipeline. Hay que decir que este modelo usa como token especial `<mask>`, distinto del anterior. El modelo XLM-RoBERTa está preentrenado en 2.5TB de datos de CommonCrawl filtrados que contienen 100 idiomas. Fue presentado en el siguiente [paper](https://arxiv.org/abs/1911.02116).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b91833ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = 'FacebookAI/xlm-roberta-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "92555544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FacebookAI/xlm-roberta-large were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(task=tarea, model=modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d871e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = 'La capital de España es <mask>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d0f942a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.7915661931037903,\n",
       "  'token': 8884,\n",
       "  'token_str': 'Madrid',\n",
       "  'sequence': 'La capital de España es Madrid'},\n",
       " {'score': 0.1014159694314003,\n",
       "  'token': 5755,\n",
       "  'token_str': 'Barcelona',\n",
       "  'sequence': 'La capital de España es Barcelona'},\n",
       " {'score': 0.05767650902271271,\n",
       "  'token': 12,\n",
       "  'token_str': ':',\n",
       "  'sequence': 'La capital de España es:'},\n",
       " {'score': 0.009059983305633068,\n",
       "  'token': 73015,\n",
       "  'token_str': 'Valencia',\n",
       "  'sequence': 'La capital de España es Valencia'},\n",
       " {'score': 0.00565499160438776,\n",
       "  'token': 27,\n",
       "  'token_str': '...',\n",
       "  'sequence': 'La capital de España es...'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(frase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42abf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "599px",
    "left": "136px",
    "top": "111.141px",
    "width": "239.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
