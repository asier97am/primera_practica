{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57d8872b",
   "metadata": {},
   "source": [
    "# 9 - Resumen de texto\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/resumen.webp\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16b0d72",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---Modelos-de-resumen\" data-toc-modified-id=\"1---Modelos-de-resumen-1\">1 - Modelos de resumen</a></span></li><li><span><a href=\"#2---Pipeline-de-Transformers-para-resumir\" data-toc-modified-id=\"2---Pipeline-de-Transformers-para-resumir-2\">2 - Pipeline de Transformers para resumir</a></span></li><li><span><a href=\"#3---Usando-el-modelo-de-resumen\" data-toc-modified-id=\"3---Usando-el-modelo-de-resumen-3\">3 - Usando el modelo de resumen</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1-Tokenizador\" data-toc-modified-id=\"3.1-Tokenizador-3.1\">3.1 Tokenizador</a></span></li><li><span><a href=\"#3.2---Modelo-de-resumen\" data-toc-modified-id=\"3.2---Modelo-de-resumen-3.2\">3.2 - Modelo de resumen</a></span></li><li><span><a href=\"#3.3---Resumen-funcional\" data-toc-modified-id=\"3.3---Resumen-funcional-3.3\">3.3 - Resumen funcional</a></span></li></ul></li><li><span><a href=\"#4---Más-modelos-de-resumen\" data-toc-modified-id=\"4---Más-modelos-de-resumen-4\">4 - Más modelos de resumen</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd1cc22",
   "metadata": {},
   "source": [
    "## 1 - Modelos de resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7396ed72",
   "metadata": {},
   "source": [
    "Los modelos de resumen de texto son herramientas de NLP diseñadas para condensar información extensa en textos más cortos que mantienen los puntos clave del contenido original. Estos modelos pueden ser clasificados principalmente en dos tipos: extractivos y abstractivos.\n",
    "\n",
    "1. **Resumen Extractivo**\n",
    "\n",
    "Los modelos extractivos seleccionan y concatenan las frases o segmentos más importantes del texto original para formar un resumen. Estos modelos no generan nuevo texto, sino que extraen partes del texto original.\n",
    "\n",
    "+ Técnicas comunes del resumen extractivo:\n",
    "    + Análisis de frecuencia: Identifica las palabras y frases más frecuentes para determinar la importancia.\n",
    "    + Algoritmos de clustering: Agrupan frases similares y seleccionan representantes de cada grupo.\n",
    "    + Redes neuronales: Utilizan técnicas como LSTM o GRU para evaluar la importancia de las oraciones en el contexto del documento completo.\n",
    "\n",
    "\n",
    "2. **Resumen Abstractivo**\n",
    "\n",
    "Los modelos abstractivos generan un nuevo texto que no necesariamente aparece en el original, utilizando técnicas avanzadas para reformular y sintetizar contenido que refleje los puntos clave.\n",
    "\n",
    "+ Técnicas comunes del resumen abstractivo:\n",
    "    + Modelos Seq2Seq (Secuencia a Secuencia): Utilizan arquitecturas de codificador-decodificador, donde el texto original se codifica en una representación vectorial y el decodificador genera el resumen.\n",
    "    + Transformers: Especialmente el modelo Transformer, que ha demostrado ser muy efectivo para resúmenes abstractivos debido a su capacidad para manejar largas dependencias en el texto.\n",
    "    + BERT y derivados: Modelos como BERT, GPT y T5 han sido adaptados para mejorar las tareas de generación de texto, incluyendo el resumen abstractivo.\n",
    "\n",
    "\n",
    "\n",
    "3. **Modelos populares de resumen**\n",
    "    + GPT: Estos modelos de OpenAI, basados en la arquitectura Transformer, tienen la capacidad para realizar resúmenes abstractivos mediante el ajuste fino en tareas específicas de resumen.\n",
    "    + BERTSUM: Una adaptación de BERT para resúmenes, que modifica la arquitectura BERT para hacerla más adecuada para seleccionar oraciones significativas.\n",
    "    + T5 (Text-to-Text Transfer Transformer): Un modelo diseñado para convertir todas las tareas de NLP en un problema de generación de texto, incluido el resumen.\n",
    "\n",
    "\n",
    "4. **Aplicaciones**\n",
    "    + Medios de comunicación: Resumen de noticias y artículos para proporcionar versiones condensadas rápidamente.\n",
    "    + Academia y educación: Resumen de documentos y literatura académica para facilitar la revisión y el estudio.\n",
    "    + Negocios y legal: Automatización del resumen de informes, contratos y documentos legales para mejorar la eficiencia y reducir la carga de trabajo.\n",
    "    \n",
    "    \n",
    "    \n",
    "Los modelos de resumen no solo ofrecen eficiencias significativas en la gestión de la información, sino que también permiten a los usuarios acceder rápidamente a la información relevante sin necesidad de revisar grandes volúmenes de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2370f42b",
   "metadata": {},
   "source": [
    "## 2 - Pipeline de Transformers para resumir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041d3235",
   "metadata": {},
   "source": [
    "Vamos a usar un modelo llamado [mt5-small-spanish-summarization](https://huggingface.co/josmunpen/mt5-small-spanish-summarization). Este es un modelo mt5-small afinado para generar titulares a partir del cuerpo de las noticias en español. El modelo fue entrenado con 58.425 noticias extraídas de periódicos, 31.477 del periódico La Razón y 26.948 del periódico Público. Estas noticias pertenecen a las siguientes categorías: \"España\", \"Cultura\", \"Economía\", \"Igualdad\" y \"Política\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4afedf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a939bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tarea = 'summarization'\n",
    "\n",
    "modelo = 'josmunpen/mt5-small-spanish-summarization'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9be9c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ia/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/opt/anaconda3/envs/ia/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "resumen_pipe = pipeline(task=tarea, model=modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c74b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo de articulo\n",
    "\n",
    "articulo = '''\n",
    "La Guardia Civil ha desarticulado un grupo organizado dedicado a copiar en los examenes teoricos para \n",
    "la obtencion del permiso de conducir. Para ello, empleaban receptores y camaras de alta tecnologia y \n",
    "operaban desde la misma sede del Centro de examenes de la Direccion General de Trafico (DGT) en Mostoles. \n",
    "Es lo que han llamado la Operacion pinga. \n",
    "El grupo desarticulado ofrecia el servicio de transporte y tecnologia para copiar y \n",
    "poder aprobar. Por dicho servicio cobraban 1.000 euros. Los investigadores sorprendieron in fraganti a una \n",
    "mujer intentando copiar en el examen. Portaba una chaqueta con dispositivos electronicos ocultos, \n",
    "concretamente un telefono movil al que estaba conectada una camara que habia sido insertada en la parte \n",
    "frontal de la chaqueta para transmitir online el examen y que orientada al ordenador del Centro de Examenes \n",
    "en el que aparecen las preguntas, permitia visualizar las imagenes en otro ordenador alojado en el interior \n",
    "de un vehiculo estacionado en las inmediaciones del centro. En este vehiculo, se encontraban el resto del \n",
    "grupo desarticulado con varios ordenadores portatiles y tablets abiertos y conectados a paginas de test de la\n",
    "DGT para consultar las respuestas. Estos, comunicaban con la mujer que estaba en el aula haciendo el examen \n",
    "a traves de un diminuto receptor bluetooth que portaba en el interior de su oido.  \n",
    "\n",
    "Luis de Lama, portavoz de la Guardia Civil de Trafico destaca que los ciudadanos, eran de origen chino, \n",
    "y copiaban en el examen utilizando la tecnologia facilitada por una organizacion. Destaca que, ademas de parte \n",
    "del fraude que supone copiar en un examen muchos de estos ciudadanos desconocian el idioma, no hablan ni \n",
    "entienden el español lo que supone un grave riesgo para la seguridad vial por desconocer las señales y \n",
    "letreros que avisan en carretera de muchas incidencias. \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0159747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articulo.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "744460c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'la guardia civil desarticula un grupo dedicado a copiar en los examenes'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumen = resumen_pipe(articulo, min_length=10, max_length=80)\n",
    "\n",
    "resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "179ab12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(resumen[0]['summary_text'].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a28f44",
   "metadata": {},
   "source": [
    "## 3 - Usando el modelo de resumen\n",
    "\n",
    "Usemos el modelo fuera del pipeline. Como siempre, necesitamos el tokenizador y el modelo preentrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dd3aabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb0b05d",
   "metadata": {},
   "source": [
    "### 3.1 Tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72c0189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizador = AutoTokenizer.from_pretrained(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3bf8a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5TokenizerFast(name_or_path='josmunpen/mt5-small-spanish-summarization', vocab_size=250100, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3853743f",
   "metadata": {},
   "source": [
    "Características principales del T5TokenizerFast:\n",
    "\n",
    "1. **vocab_size**: 50265 representa el tamaño del vocabulario del tokenizador, es decir, la cantidad de tokens únicos que puede reconocer.\n",
    "\n",
    "\n",
    "2. **model_max_length**: 1024 es la longitud máxima de tokens que el modelo puede manejar.\n",
    "\n",
    "\n",
    "3. **is_fast**: True indica que este tokenizador utiliza la implementación rápida de Hugging Face, optimizada para ser más eficiente en comparación con las versiones basadas puramente en Python.\n",
    "\n",
    "\n",
    "4. **padding_side y truncation_side**: Ambos configurados a right (derecha), lo que significa que el relleno y la truncación de secuencias se realizan al final de las secuencias de tokens.\n",
    "\n",
    "\n",
    "5. **special_tokens**:Define varios tokens especiales que el modelo necesita para operar correctamente:\n",
    "    + eos_token: `</s>` marca el fin de una secuencia.\n",
    "    + unk_token: `<unk>` se utiliza para tokens desconocidos.\n",
    "    + pad_token: `<pad>` se usa para rellenar secuencias hasta una longitud común.\n",
    "    + mask_token: `<mask>` se usa para enmascarar y predecir la palabra original que ha sido enmascarada.\n",
    "\n",
    "\n",
    "6. **clean_up_tokenization_spaces**: True indica que el tokenizador limpiará espacios de tokenización extras, ayudando a mantener la integridad del texto original en su conversión a tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1a25148",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = tokenizador(articulo, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8383cb80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 474])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f28a4f",
   "metadata": {},
   "source": [
    "### 3.2 - Modelo de resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "693fb6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_resumen = AutoModelForSeq2SeqLM.from_pretrained(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abb724ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MT5ForConditionalGeneration(\n",
       "  (shared): Embedding(250112, 512)\n",
       "  (encoder): MT5Stack(\n",
       "    (embed_tokens): Embedding(250112, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): MT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): MT5Stack(\n",
       "    (embed_tokens): Embedding(250112, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): MT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec39f07",
   "metadata": {},
   "source": [
    "**Componentes del modelo MT5ForConditionalGeneration**:\n",
    "\n",
    "\n",
    "1. **Shared Embedding**:\n",
    "\n",
    "+ Embedding(250112, 512): Una capa de embedding compartida para el encoder y el decoder, con un vocabulario de 250,112 tokens y cada token representado por un vector de 512 dimensiones.\n",
    "\n",
    "\n",
    "2. **Encoder (MT5Stack)**:\n",
    "\n",
    "+ embed_tokens: Utiliza la misma capa de embedding compartida para transformar los tokens de entrada en vectores.\n",
    "+ block: Una lista de módulos de tipo MT5Block, cada uno conteniendo capas de atención y capas de alimentación directa (feed-forward):\n",
    "    + MT5LayerSelfAttention: Capa de auto-atención que ayuda al modelo a enfocarse en diferentes partes de la entrada para entender mejor el contexto.\n",
    "    + MT5LayerFF (Feed Forward Layer): Capa que utiliza una activación no lineal (GELU) y operaciones lineales para transformar la salida de la capa de atención.\n",
    "\n",
    "\n",
    "3. **Decoder (MT5Stack)**: Similar al encoder en estructura, pero con capas adicionales para la atención cruzada:\n",
    "\n",
    "+ MT5LayerSelfAttention: Similar a la capa del encoder, se enfoca en la entrada del decoder.\n",
    "+ MT5LayerCrossAttention (EncDecAttention): Permite que el decoder acceda a la salida completa del encoder, crucial para tareas como la traducción donde el contexto del texto fuente es esencial para generar el texto objetivo.\n",
    "+ MT5LayerFF: Similar a la del encoder, procesa la salida de las capas de atención.\n",
    "\n",
    "\n",
    "4. **Componentes Adicionales**:\n",
    "\n",
    "+ Dropout(p=0.1): Utilizado en varias capas para evitar el sobreajuste mediante la desactivación aleatoria de neuronas durante el entrenamiento.\n",
    "+ MT5LayerNorm: Normalización de capa para estabilizar las salidas internas del modelo.\n",
    "+ lm_head: Una capa lineal que mapea la salida del decoder al espacio del vocabulario original, usada para generar la salida final del texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4aff8180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ia/lib/python3.9/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "respuesta = modelo_resumen.generate(**vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6dda3794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0,    283,    259, 125617,   9242,    498, 144352,    262,    335,\n",
       "           8237,    269,  94902,    259,    262,  57503,    286,    289,    595,\n",
       "          56125,    299]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f89d3514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'la guardia civil desarticula un grupo dedicado a copiar en los examenes'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizador.batch_decode(respuesta, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230684c1",
   "metadata": {},
   "source": [
    "### 3.3 - Resumen funcional\n",
    "Vamos a poner todo el código junto en una sola función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f5caebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# librerias\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "\n",
    "def resumen(texto: str, modelo: str) -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    Función para resumir un texto.\n",
    "    \n",
    "    Params:\n",
    "    + texto: string. Texto a ser resumido.\n",
    "    + modelo: string. Modelo de transformers para resumir.\n",
    "    \n",
    "    Return:\n",
    "    String. Resumen.\n",
    "    \"\"\"\n",
    "    \n",
    "    # con este objeto vectorizamos las palabras\n",
    "    tokenizador = AutoTokenizer.from_pretrained(modelo)\n",
    "    \n",
    "    \n",
    "    # creación del vector\n",
    "    vector = tokenizador(texto, return_tensors='pt')\n",
    "    \n",
    "    \n",
    "    # inicializacion del modelo traductor\n",
    "    modelo_traductor = AutoModelForSeq2SeqLM.from_pretrained(modelo)\n",
    "    \n",
    "    # tensor de respuesta del modelo\n",
    "    respuesta = modelo_traductor.generate(**vector)\n",
    "    \n",
    "    # respuesta del modelo\n",
    "    respuesta = tokenizador.batch_decode(respuesta, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70106108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'la guardia civil desarticula un grupo dedicado a copiar en los examenes'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumen(articulo, modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fcca33",
   "metadata": {},
   "source": [
    "## 4 - Más modelos de resumen \n",
    "\n",
    "Probemos ahora otros modelos, esta vez en inglés, puesto que la mayoría de modelos funcionan en este idioma. Vamos a usar un modelo de resumen de [Falconsai](https://huggingface.co/Falconsai/text_summarization), es un modelo T5 Small ajustado, una variante del modelo transformer T5, diseñado para la tarea de resumen de texto que pesa aproximadamente 250 Mb. Está adaptado y ajustado para generar resúmenes concisos y coherentes del texto de entrada. Este modelo está preentrenado en un corpus diverso de datos textuales, lo que le permite capturar información esencial y generar resúmenes significativos. El ajuste se realiza prestando especial atención a los ajustes de hiperparámetros, incluyendo el tamaño del lote y la tasa de aprendizaje, para garantizar un rendimiento óptimo en el resumen de texto.\n",
    "\n",
    "\n",
    "El conjunto de datos de ajuste consta de una variedad de documentos y sus resúmenes generados por humanos correspondientes. Este conjunto de datos diverso permite al modelo aprender el arte de crear resúmenes que capturen la información más importante mientras mantienen la coherencia y la fluidez. El objetivo del proceso de entrenamiento es dotar al modelo de la capacidad de generar resúmenes de texto de alta calidad, haciéndolo valioso para una amplia gama de aplicaciones que involucran la resumición de documentos y la condensación de contenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1e68b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# articulo de ejemplo\n",
    "\n",
    "articulo = ''' \n",
    "Hugging Face: Revolutionizing Natural Language Processing\n",
    "Introduction\n",
    "In the rapidly evolving field of Natural Language Processing (NLP), Hugging Face has emerged as a prominent \n",
    "and innovative force. This article will explore the story and significance of Hugging Face, a company that has\n",
    "made remarkable contributions to NLP and AI as a whole. From its inception to its role in democratizing AI, \n",
    "Hugging Face has left an indelible mark on the industry.\n",
    "The Birth of Hugging Face\n",
    "Hugging Face was founded in 2016 by Clément Delangue, Julien Chaumond, and Thomas Wolf. The name \"Hugging Face\" \n",
    "was chosen to reflect the company's mission of making AI models more accessible and friendly to humans, much \n",
    "like a comforting hug. Initially, they began as a chatbot company but later shifted their focus to NLP, \n",
    "driven by their belief in the transformative potential of this technology.\n",
    "Transformative Innovations\n",
    "Hugging Face is best known for its open-source contributions, particularly the \"Transformers\" library. \n",
    "This library has become the de facto standard for NLP and enables researchers, developers, and organizations \n",
    "to easily access and utilize state-of-the-art pre-trained language models, such as BERT, GPT-3, and more. \n",
    "These models have countless applications, from chatbots and virtual assistants to language translation and \n",
    "sentiment analysis.\n",
    "Key Contributions:\n",
    "1. **Transformers Library:** The Transformers library provides a unified interface for more than 50 \n",
    "pre-trained models, simplifying the development of NLP applications. It allows users to fine-tune these \n",
    "models for specific tasks, making it accessible to a wider audience.\n",
    "2. **Model Hub:** Hugging Face's Model Hub is a treasure trove of pre-trained models, making it simple for \n",
    "anyone to access, experiment with, and fine-tune models. Researchers and developers around the world can \n",
    "collaborate and share their models through this platform.\n",
    "3. **Hugging Face Transformers Community:** Hugging Face has fostered a vibrant online community where \n",
    "developers, researchers, and AI enthusiasts can share their knowledge, code, and insights. This collaborative \n",
    "spirit has accelerated the growth of NLP.\n",
    "Democratizing AI\n",
    "Hugging Face's most significant impact has been the democratization of AI and NLP. Their commitment to \n",
    "open-source development has made powerful AI models accessible to individuals, startups, and established \n",
    "organizations. This approach contrasts with the traditional proprietary AI model market, which often limits \n",
    "access to those with substantial resources.\n",
    "By providing open-source models and tools, Hugging Face has empowered a diverse array of users to innovate \n",
    "and create their own NLP applications. This shift has fostered inclusivity, allowing a broader range of voices \n",
    "to contribute to AI research and development.\n",
    "Industry Adoption\n",
    "The success and impact of Hugging Face are evident in its widespread adoption. Numerous companies and \n",
    "institutions, from startups to tech giants, leverage Hugging Face's technology for their AI applications. \n",
    "This includes industries as varied as healthcare, finance, and entertainment, showcasing the versatility \n",
    "of NLP and Hugging Face's contributions.\n",
    "Future Directions\n",
    "Hugging Face's journey is far from over. As of my last knowledge update in September 2021, the company was \n",
    "actively pursuing research into ethical AI, bias reduction in models, and more. Given their track record of \n",
    "innovation and commitment to the AI community, it is likely that they will continue to lead in ethical AI \n",
    "development and promote responsible use of NLP technologies.\n",
    "Conclusion\n",
    "Hugging Face's story is one of transformation, collaboration, and empowerment. Their open-source contributions \n",
    "have reshaped the NLP landscape and democratized access to AI. As they continue to push the boundaries of AI \n",
    "research, we can expect Hugging Face to remain at the forefront of innovation, contributing to a more \n",
    "inclusive and ethical AI future. Their journey reminds us that the power of open-source collaboration can \n",
    "lead to groundbreaking advancements in technology and bring AI within the reach of many.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "baf0534f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(task = \"summarization\", model=\"Falconsai/text_summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf6db719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (851 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "resumen_pipe = pipe(articulo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a04daa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hugging Face has emerged as a prominent and innovative force in NLP . From its inception to its role in democratizing AI, the company has left an indelible mark on the industry . The name \"Hugging Face\" was chosen to reflect the company\\'s mission of making AI models more accessible and friendly to humans .'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumen_pipe[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8574ea12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "608"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articulo.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76012fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(resumen_pipe[0]['summary_text'].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea25eb63",
   "metadata": {},
   "source": [
    "Probemos otro modelo, un modelo [BART](https://huggingface.co/facebook/bart-large-cnn), preentrenado en inglés, ajustado con el dataset [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail) y desarrollado por Facebook. Pesa 1.65Gb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "21fe647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = 'facebook/bart-large-cnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ecc0774",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = resumen(articulo, modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ec6162a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hugging Face was founded in 2016 by Clément Delangue, Julien Chaumond, and Thomas Wolf. The company's most significant impact has been the democratization of AI and NLP. Their open-source contributions have reshaped the NLP landscape and democratized access to AI.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc0b3b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48d09a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1737f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "599px",
    "left": "106px",
    "top": "111.141px",
    "width": "266.267px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
