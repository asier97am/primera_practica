{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de2b2128",
   "metadata": {},
   "source": [
    "# 3 - Clasificación de texto (análisis de sentimiento)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/nlp_clasi.webp\" style=\"width:400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c71e1a",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---Modelos-de-clasificación-de-texto-o-análisis-de-sentimiento\" data-toc-modified-id=\"1---Modelos-de-clasificación-de-texto-o-análisis-de-sentimiento-1\">1 - Modelos de clasificación de texto o análisis de sentimiento</a></span></li><li><span><a href=\"#2---Pipeline-de-Transformers\" data-toc-modified-id=\"2---Pipeline-de-Transformers-2\">2 - Pipeline de Transformers</a></span></li><li><span><a href=\"#3---Usando-el-modelo-directamente\" data-toc-modified-id=\"3---Usando-el-modelo-directamente-3\">3 - Usando el modelo directamente</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1---Tokenizador\" data-toc-modified-id=\"3.1---Tokenizador-3.1\">3.1 - Tokenizador</a></span></li><li><span><a href=\"#3.2---Modelo-clasificador\" data-toc-modified-id=\"3.2---Modelo-clasificador-3.2\">3.2 - Modelo clasificador</a></span></li></ul></li><li><span><a href=\"#4---Funciones-custom\" data-toc-modified-id=\"4---Funciones-custom-4\">4 - Funciones custom</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1965d5b6",
   "metadata": {},
   "source": [
    "## 1 - Modelos de clasificación de texto o análisis de sentimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bba66de",
   "metadata": {},
   "source": [
    "El análisis de sentimiento es un proceso de procesamiento del NLP que determina la inclinación emocional de un texto. Su objetivo es identificar y categorizar las opiniones expresadas en un fragmento de texto para determinar si la actitud del hablante es positiva, negativa o neutral. Este análisis puede extenderse a veces a emociones más específicas como la felicidad, la ira, la tristeza, etc...\n",
    "\n",
    "Algunas aplicaciones del análisis de sentimiento son:\n",
    "\n",
    "1. **Monitoreo de la Marca y Gestión de la Reputación**: \n",
    "\n",
    "Las empresas utilizan el análisis de sentimientos para monitorear y analizar las opiniones de los clientes sobre sus productos y servicios en redes sociales, foros y otros canales de comunicación. Esto les ayuda a mejorar su servicio al cliente y a ajustar sus estrategias de marketing.\n",
    "\n",
    "2. **Análisis de Redes Sociales**: \n",
    "\n",
    "Al analizar el sentimiento de los posts y comentarios en las redes sociales, las organizaciones pueden obtener una mejor comprensión de la opinión pública y las tendencias actuales, lo cual es útil para la planificación de campañas y la gestión de crisis.\n",
    "\n",
    "3. **Mercados Financieros**:\n",
    "\n",
    "Algunos analistas usan el análisis de sentimientos para predecir los movimientos del mercado basados en el tono de las noticias y los informes financieros, lo que puede ser un indicador de cómo se comportarán las acciones.\n",
    "\n",
    "4. **Soporte de Políticas y Gobernanza**:\n",
    "\n",
    "En el sector público, el análisis de sentimientos puede ayudar a comprender las reacciones del público hacia las políticas o decisiones gubernamentales y ajustar las comunicaciones o políticas en consecuencia.\n",
    "\n",
    "5. **Productos de Consumo y Análisis de Reseñas**:\n",
    "\n",
    "Empresas de e-commerce utilizan análisis de sentimientos para identificar y categorizar opiniones en las reseñas de productos, ayudando a otros consumidores en sus decisiones de compra y a las empresas en el desarrollo de productos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb96c1cb",
   "metadata": {},
   "source": [
    "![hf_webpage_models](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/hf_webpage_models.png)\n",
    "\n",
    "\n",
    "En el hub de [modelos](https://huggingface.co/models?sort=trending) de Hugging Face podemos encontrar los modelos de [clasificación de texto](https://huggingface.co/models?pipeline_tag=text-classification&sort=trending) o análisis de sentimiento. Veamos algunos ejemplos, empezando por el que carga Hugging Face para el análisis de sentimiento por defecto, `distilbert-base-uncased-finetuned-sst-2-english`, aqui el [link](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english). Este modelo es un punto de control ajustado de [DistilBERT-base-uncased](https://huggingface.co/distilbert/distilbert-base-uncased), un modelo basado en Bert. Alcanza una precisión del 91.3 en el conjunto de desarrollo, la versión Bert bert-base-uncased alcanza una precisión del 92.7. Este modelo pesa unos 270Mb y se guarda en la ruta `~/.cache/huggingface/hub`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fb84f9",
   "metadata": {},
   "source": [
    "## 2 - Pipeline de Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73787856",
   "metadata": {},
   "source": [
    "Primero, trabajaremos con el objeto `pipeline` de la librería `transformers`, una manera fácil y cómoda de usar los modelos de Hugging Face. El pipeline de transformers facilita la implementación rápida y fácil de modelos de procesamiento de NLP y otros modelos de inteligencia artificial. Proporciona una interfaz sencilla para llevar a cabo una variedad de tareas comunes en NLP, como análisis de sentimientos, reconocimiento de entidades nombradas, clasificación de texto, respuesta a preguntas, traducción automática, y más.\n",
    "\n",
    "Algunas características del pipeline son:\n",
    "\n",
    "+ Simplicidad de Uso: El pipeline encapsula complejidades como la carga de modelos, la tokenización de textos y la ejecución de inferencias, permitiendo a los usuarios trabajar con modelos avanzados de NLP con solo unas pocas líneas de código.\n",
    "\n",
    "+ Flexibilidad: Soporta diversos modelos preentrenados disponibles en el Hub de Hugging Face, permitiendo a los usuarios seleccionar el modelo que mejor se ajuste a sus necesidades específicas.\n",
    "\n",
    "+ Personalización: Aunque los pipelines vienen con configuraciones predeterminadas, estos pueden ser personalizados en términos de modelos, tokenizadores y configuraciones de tareas específicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6756cb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eccca785",
   "metadata": {},
   "outputs": [],
   "source": [
    "tarea = 'text-classification'\n",
    "\n",
    "modelo = 'distilbert/distilbert-base-uncased-finetuned-sst-2-english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dadd0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ia/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "clasificador = pipeline(task=tarea, model=modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3bf32dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9833884239196777}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasificador('me gusta aprender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63617d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9833884239196777},\n",
       " {'label': 'POSITIVE', 'score': 0.994017481803894},\n",
       " {'label': 'NEGATIVE', 'score': 0.9924824833869934}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = ['me gusta aprender', 'hola, buenos dias', 'no me agradas']\n",
    "\n",
    "clasificador(lista)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e68ec7",
   "metadata": {},
   "source": [
    "Podemos ver que el modelo nos clasifica nuestros textos en positivo y negativo, y además nos devuelve el \"score\" de dicha clasificación, la probabilidad de que dicho texto tenga esa etiqueta que pone el clasificador. Así ya podemos usar el análisis de sentimiento en cualquier aplicación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77e10c7",
   "metadata": {},
   "source": [
    "## 3 - Usando el modelo directamente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c43938",
   "metadata": {},
   "source": [
    "Hay otra manera de usar este modelo desde la librería transformers, usando los objetos `AutoTokenizer` y `AutoModelForSequenceClassification`. La salida de estos objetos no será una lista de diccionarios con las etiquetas, sino que directamente trabajaremos con los tokenizadores, objetos que vectorizan las palabras, y los tensores de salida de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8f05ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10afdd58",
   "metadata": {},
   "source": [
    "### 3.1 - Tokenizador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad778705",
   "metadata": {},
   "source": [
    "Un tokenizador es una herramienta utilizada en NLP para dividir el texto en unidades más pequeñas llamadas tokens. Estos tokens pueden ser palabras individuales, subpalabras, caracteres o incluso partes más grandes del texto, dependiendo del diseño del tokenizador y el propósito específico para el que se esté utilizando. Por supuesto tendrán una representación numérica.\n",
    "\n",
    "La tarea principal de un tokenizador es convertir el texto en una secuencia de tokens que pueda ser procesada por un modelo de NLP. Esto es importante porque muchos modelos de NLP trabajan a nivel de tokens y necesitan entradas tokenizadas para funcionar correctamente.\n",
    "\n",
    "Algunas tareas comunes que puede realizar un tokenizador incluyen:\n",
    "\n",
    "1. Dividir el texto en palabras individuales.\n",
    "2. Separar puntuaciones y caracteres especiales.\n",
    "3. Segmentar el texto en subpalabras, especialmente útil para idiomas con flexión y derivación morfológica.\n",
    "4. Aplicar codificación de caracteres, como codificación UTF-8.\n",
    "5. Añadir tokens especiales, como tokens de inicio y fin de secuencia, tokens de relleno, tokens desconocidos, etc.\n",
    "\n",
    "\n",
    "Los tokenizadores pueden variar en complejidad y sofisticación dependiendo de la tarea y el idioma para el que se utilicen. Algunos tokenizadores simples pueden basarse simplemente en reglas heurísticas, mientras que otros, como los tokenizadores utilizados en modelos de lenguaje modernos como BERT o GPT, pueden ser modelos especializados entrenados para esta tarea. En nuestro caso, DistilBERT tiene su propio modelo preentrenado de tokenización.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0821dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizador = AutoTokenizer.from_pretrained(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cb1ff56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased-finetuned-sst-2-english', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5db409",
   "metadata": {},
   "source": [
    "Vemos en la descripción del tokenizador varios elementos que pasamos a explicar. \n",
    "\n",
    "+ `vocab_size`: Se refiere al tamaño del vocabulario del que maneja, define el número de diferentes tokens que se pueden representar.\n",
    "\n",
    "+ `model_max_length`: Número máximo de tokens de salida del tokenizador.\n",
    "\n",
    "+ `special_tokens`: Estos son tokens especiales utilizados en la arquitectura de los modelos Transformer, especialmente en modelos como BERT y sus variantes, como la versión DistilBert que estamos usando. Aquí está el significado de cada uno:\n",
    "\n",
    "    + [PAD]: Este token se utiliza para hacer que todas las secuencias de entrada tengan la misma longitud, rellenando las secuencias más cortas con este token hasta alcanzar la longitud máxima.\n",
    "    \n",
    "    + [UNK]: Representa palabras desconocidas o fuera del vocabulario durante el entrenamiento o la inferencia. Se utiliza cuando el modelo encuentra una palabra que no está en su vocabulario predefinido.\n",
    "    \n",
    "    + [CLS]: Este token se agrega al principio de cada secuencia de entrada. Se utiliza en tareas de clasificación de texto para representar la clase de toda la secuencia.\n",
    "    \n",
    "    + [SEP]: Se utiliza para separar dos oraciones o fragmentos de texto en una sola secuencia de entrada. También se agrega al final de cada secuencia de entrada.\n",
    "    \n",
    "    + [MASK]: Este token se utiliza en tareas de llenado de espacios ([cloze tasks](https://es.wikipedia.org/wiki/Prueba_cloze)) durante el entrenamiento del modelo. Se sustituye aleatoriamente por una palabra en la secuencia de entrada, y el modelo debe predecir la palabra original."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c903a4",
   "metadata": {},
   "source": [
    "Ahora, vamos a escribir una frase y pasarla por el tokenizador para convertirla a un vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f8e2982",
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = 'me gusta el aprendizaje'\n",
    "\n",
    "vector = tokenizador(frase, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d97b95a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2033, 26903,  2050,  3449, 19804, 10497, 21335,  6460,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882c03dd",
   "metadata": {},
   "source": [
    "En el tokenizador introducimos la frase y le pedimos que devuelva la salida como un tensor de [pytorch](https://pytorch.org/). Como se puede ver el objeto vector que hemos creado es un diccionario, cuyas keys son:\n",
    "\n",
    "+ `input_ids`: Son simplemente las representaciones numéricas de los tokens, las palabras convertidas a números, como tensor de pytorch. \n",
    "\n",
    "+ `attention_mask`: La máscara de atención es útil cuando agregamos relleno a los tokens de entrada, nos indica qué input_ids corresponden al relleno. Cuando se añade relleno es porque queremos que todas las oraciones de entrada tengan la misma longitud para poder formar los vectores adecuadamente. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93edc826",
   "metadata": {},
   "source": [
    "### 3.2 - Modelo clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2373b1c8",
   "metadata": {},
   "source": [
    "Ahora cargamos el modelo clasificador preentrenado `distilbert-base-uncased-finetuned-sst-2-english`. Recordemos que este modelo realiza un análisis de sentimiento de la frase que le hemos pasado al tokenizador, es decir, es un modelo de clasificación binaria. Nos dirá si la frase es positiva o negativa, solo tiene dos etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec766ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador = AutoModelForSequenceClassification.from_pretrained(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b5503e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce703a8",
   "metadata": {},
   "source": [
    "Veamos una explicación de los 4 bloques que presenta el clasificador DistilBERT:\n",
    "\n",
    "1. `distilbert`: Esta parte define la arquitectura de la red DistilBERT. Comienza con una capa de embeddings, que convierte las palabras de entrada en vectores de alta dimensión. Incluye una capa de atención multi-cabeza, seguida de una capa de FeedForward Network (FFN) para procesamiento adicional de las características de la entrada. La arquitectura de DistilBERT tiene varias capas de TransformerBlock, en este caso son 6, que procesan la secuencia de entrada.\n",
    "\n",
    "2. `pre_classifier`: Después de la representación de la secuencia de entrada obtenida de DistilBERT, pasa por una capa lineal (linear layer) para reducir la dimensionalidad de los vectores de características.\n",
    "\n",
    "3. `classifier`: Finalmente, la salida de la capa anterior se pasa por otra capa lineal que produce las predicciones de clasificación. En este caso, el modelo tiene dos clases de salida.\n",
    "\n",
    "4. `dropout`: Entre las capas lineales, se aplica una técnica de regularización llamada dropout para evitar el sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423a0776",
   "metadata": {},
   "source": [
    "Como ya que hemos definido nuestro clasificador, le damos como entrada el vector que nos había devuelto el tokenizador. Recordemos que el vector en realidad era un diccionario con un tensor de pytorch como uno de sus valores. La manera de dárselo al clasificador es como argumentos keyword, `**kwargs`. Veamos otra vez el vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f430af6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2033, 26903,  2050,  3449, 19804, 10497, 21335,  6460,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85311c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4359,  2.5601]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasificador(**vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5bee2d",
   "metadata": {},
   "source": [
    "El objeto que nos devuelve el modelo es un objeto `SequenceClassifierOutput` de transformers. Sin embargo, nosotros lo que queremos es el tensor `logits` que hay en su interior. Para obtenerlo usamos directamente el atributo logits del objeto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77ee1f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4359,  2.5601]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasificador(**vector).logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32afa1aa",
   "metadata": {},
   "source": [
    "Este tensor contiene dos números. La pregunta surge naturalmente, ¿cuál es la etiqueta?, ¿cuál es realmente el resultado?, ¿positivo o negativo?. La respuesta está en la posición del mayor de los dos números, es decir, su índice. La etiqueta `Negativo` se corresponde con el `0` y la etiqueta `Positivo` se corresponde con el `1`. Para obtener el índice del mayor de los números podemos usar el método `argmax()`. El [argmax](https://es.wikipedia.org/wiki/Argumentos_del_m%C3%A1ximo_y_el_m%C3%ADnimo) o argumento máximo, en matemáticas, se refiere a los puntos del dominio donde la función es máxima, en este caso, nos devuelve la posición dentro del tensor donde el número es mayor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9749d478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta = clasificador(**vector).logits.argmax()\n",
    "\n",
    "respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "931dc451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee149a1",
   "metadata": {},
   "source": [
    "Vemos que la respuesta es un tensor de pytorch. Podemos convertirla fácilmente a entero para poder manejarla más tarde con el método `item()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17a6dfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta = respuesta.item()  \n",
    "\n",
    "respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0a5af1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cc736c",
   "metadata": {},
   "source": [
    "## 4 - Funciones custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db671198",
   "metadata": {},
   "source": [
    "Podemos poner todo el código en una sola función para usarla posteriormente o importarla en otro archivo. Si nos sirve la respuesta del pipeline el código podría ser como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "960457ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "\n",
    "def hugging_pipeline(frase: Union[str,list], tarea: str, modelo: str) -> list:\n",
    "    \n",
    "    \n",
    "    clasificador = pipeline(task=tarea, model=modelo)\n",
    "    \n",
    "    respuesta = clasificador(frase)\n",
    "    \n",
    "    return respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5e86060",
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = 'me gusta aprender'\n",
    "\n",
    "tarea = 'text-classification'\n",
    "\n",
    "modelo = 'distilbert/distilbert-base-uncased-finetuned-sst-2-english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb8c0680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9833884239196777}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hugging_pipeline(frase, tarea, modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "345bf871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.6039350628852844},\n",
       " {'label': 'NEGATIVE', 'score': 0.9924824833869934}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hugging_pipeline(['hola que tal', 'no me agradas'], tarea, modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e754bee0",
   "metadata": {},
   "source": [
    "Probemos otro modelo de clasificación de texto con esta misma función. El modelo es un [RoBERTa](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest) basado en Twitter. El modelo es guardado en local la primera vez que lo usamos y ocupa aproximadamente 500Mb. Este clasificador tiene 3 etiquetas: \n",
    "\n",
    "+ Negativo -> 0\n",
    "+ Neutral  -> 1\n",
    "+ Positivo -> 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa1501a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = 'me gusta aprender'\n",
    "\n",
    "tarea = 'text-classification'\n",
    "\n",
    "modelo = 'cardiffnlp/twitter-roberta-base-sentiment-latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0d4a91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'neutral', 'score': 0.6550505757331848}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hugging_pipeline(frase, tarea, modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3de00eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'neutral', 'score': 0.783708393573761},\n",
       " {'label': 'neutral', 'score': 0.7603825330734253}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hugging_pipeline(['hola que tal', 'no me agradas'], tarea, modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "276a8a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.681026816368103}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hugging_pipeline('you are a dog', tarea, modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9194ecc",
   "metadata": {},
   "source": [
    "Como se puede observar, nos está dando un warning debido al punto de control de los archivos del modelo. Para silenciar estos warnings podemos importar el `logging` de transformer para establecer que no se muestren con el siguiente código: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2fa605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c089dc7d",
   "metadata": {},
   "source": [
    "Si queremos una respuesta customizada, que sea distinta de la salida standard que tiene el pipeline de transformers, podemos crear una función usando el tokenizador y el modelo directamente para obtener una respuesta en la forma que nos guste. Podría ser un entero o también un diccionario con todos los metadatos para un posterior análisis. Vamos a crear una función de esta manera: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e4b96e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "def hugging_model(frase: str, modelo: str) -> dict:\n",
    "    \n",
    "    \n",
    "    tokenizador = AutoTokenizer.from_pretrained(modelo)\n",
    "    \n",
    "    vector = tokenizador(frase, return_tensors='pt')\n",
    "    \n",
    "    \n",
    "    \n",
    "    clasificador = AutoModelForSequenceClassification.from_pretrained(modelo)\n",
    "    \n",
    "    \n",
    "    tensor = clasificador(**vector).logits\n",
    "    \n",
    "    respuesta = tensor.argmax().item()\n",
    "    \n",
    "    \n",
    "    return {'frase': frase,\n",
    "            'modelo': modelo,\n",
    "            'tensor': tensor,\n",
    "            'n_respuesta': respuesta,\n",
    "            'respuesta': 'positivo' if respuesta==1 else 'negativo'\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "776e5334",
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = 'no me agradas'\n",
    "\n",
    "modelo = 'distilbert/distilbert-base-uncased-finetuned-sst-2-english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3463aa9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frase': 'no me agradas',\n",
       " 'modelo': 'distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n",
       " 'tensor': tensor([[ 2.6752, -2.2078]], grad_fn=<AddmmBackward0>),\n",
       " 'n_respuesta': 0,\n",
       " 'respuesta': 'negativo'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hugging_model(frase, modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389d12ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "599px",
    "left": "65px",
    "top": "111.141px",
    "width": "266.259px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
