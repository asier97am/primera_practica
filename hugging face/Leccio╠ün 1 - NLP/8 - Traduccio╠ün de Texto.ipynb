{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a9a5a13",
   "metadata": {},
   "source": [
    "# 8 - Traducción de Texto\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/traductor.webp\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2222cc",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---Modelos-de-traducción\" data-toc-modified-id=\"1---Modelos-de-traducción-1\">1 - Modelos de traducción</a></span></li><li><span><a href=\"#2---Pipeline-de-Transformers-para-traducir\" data-toc-modified-id=\"2---Pipeline-de-Transformers-para-traducir-2\">2 - Pipeline de Transformers para traducir</a></span></li><li><span><a href=\"#3---Usando-el-modelo-traductor\" data-toc-modified-id=\"3---Usando-el-modelo-traductor-3\">3 - Usando el modelo traductor</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1-Tokenizador\" data-toc-modified-id=\"3.1-Tokenizador-3.1\">3.1 Tokenizador</a></span></li><li><span><a href=\"#3.2---Modelo-traductor\" data-toc-modified-id=\"3.2---Modelo-traductor-3.2\">3.2 - Modelo traductor</a></span></li><li><span><a href=\"#3.3-Resumen-funcional\" data-toc-modified-id=\"3.3-Resumen-funcional-3.3\">3.3 Resumen funcional</a></span></li></ul></li><li><span><a href=\"#4---Ejemplo-con-más-texto\" data-toc-modified-id=\"4---Ejemplo-con-más-texto-4\">4 - Ejemplo con más texto</a></span></li><li><span><a href=\"#5---Más-modelos-de-traducción\" data-toc-modified-id=\"5---Más-modelos-de-traducción-5\">5 - Más modelos de traducción</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305730f1",
   "metadata": {},
   "source": [
    "## 1 - Modelos de traducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a77a74",
   "metadata": {},
   "source": [
    "Los modelos de traducción son fundamentales en el campo del procesamiento de lenguaje, especialmente en la traducción automática, que permite convertir texto de un idioma a otro incluso en directo. Estos modelos han evolucionado considerablemente gracias a los avances en técnicas de NLP. A continuación, se describen algunos de los enfoques y modelos más prominentes en traducción automática:\n",
    "\n",
    "\n",
    "1. **Traducción Automática Basada en Reglas (RBMT)**\n",
    "+ Características: Utiliza reglas lingüísticas específicas para cada idioma, diseñadas por expertos.\n",
    "+ Ventajas: Buen manejo de la gramática y la sintaxis cuando las reglas son exhaustivas.\n",
    "+ Desventajas: Requiere mucho esfuerzo humano para construir y mantener las reglas; no escala bien con el aumento de la diversidad de idiomas.\n",
    "\n",
    "\n",
    "2. **Traducción Automática Estadística (SMT)**\n",
    "+ Características: Basada en modelos estadísticos que aprenden traducciones a partir de grandes corpus de textos bilingües.\n",
    "+ Ventajas: Más flexible y escalable que RBMT; puede aprender de los errores y adaptarse a nuevos lenguajes o dialectos.\n",
    "+ Desventajas: La calidad de la traducción depende enormemente de la cantidad y calidad del corpus de entrenamiento; las traducciones pueden ser literales y faltarles fluidez.\n",
    "\n",
    "\n",
    "3. **Traducción Automática Neuronal (NMT)**\n",
    "+ Características: Utiliza redes neuronales profundas, especialmente arquitecturas de tipo secuencia a secuencia (seq2seq) con mecanismos de atención.\n",
    "+ Ventajas: Traducciones más fluidas y naturales; maneja mejor las construcciones gramaticales complejas y el contexto a lo largo de oraciones más largas.\n",
    "+ Desventajas: Requiere grandes cantidades de datos de entrenamiento y recursos computacionales significativos.\n",
    "+ Modelos Destacados en NMT:\n",
    "    + Transformer: Revolucionó el NMT con su arquitectura basada exclusivamente en atención, eliminando la necesidad de redes recurrentes.\n",
    "    + BERT y GPT: Aunque son modelos de propósito general para PLN, han sido adaptados para mejorar las tareas de traducción, especialmente en configuraciones multilingües.\n",
    "\n",
    "\n",
    "4. **Traducción Automática Multilingüe**\n",
    "+ Características: Un solo modelo puede manejar la traducción entre múltiples idiomas, a menudo utilizando un idioma pivote como el inglés.\n",
    "+ Ventajas: Economía de escala al entrenar un solo modelo para muchos idiomas; útil para idiomas con pocos recursos.\n",
    "+ Desventajas: Puede haber pérdida de precisión en idiomas menos representados en el entrenamiento.\n",
    "\n",
    "\n",
    "5. **Traducción Zero-Shot y Few-Shot**\n",
    "+ Características: Capacidad de traducir entre pares de idiomas que no se vieron directamente durante el entrenamiento.\n",
    "+ Ventajas: Extremadamente útil para idiomas raros o combinaciones de idiomas con datos insuficientes para entrenamiento directo.\n",
    "+ Desventajas: A menudo menos precisa que los modelos entrenados específicamente para esos pares de idiomas.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Cada uno de estos modelos y métodos tiene sus propias fortalezas y debilidades, y la elección de uno sobre otro depende de las necesidades específicas de la tarea de traducción, así como de los recursos disponibles y el alcance del proyecto. La evolución continua de estos modelos y técnicas sigue empujando los límites de lo que es posible en la traducción automática."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac5fcd9",
   "metadata": {},
   "source": [
    "Vamos a usar los modelos contruidos por el grupo [Helsinki-NLP](https://huggingface.co/Helsinki-NLP), el grupo de NLP de la [Universidad de Helsinki](https://blogs.helsinki.fi/language-technology/). En el hub de Hugging Face tienen desplegados más de 1000 modelos distintos para decenas de lenguas distintas. Todos ellos tienen un peso aproximado de 315 Mb. Vamos a probar un modelo traductor de inglés a castellano, uno de castellano a inglés y un modelo multilingüe de este grupo de trabajo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339a043d",
   "metadata": {},
   "source": [
    "## 2 - Pipeline de Transformers para traducir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999fb6be",
   "metadata": {},
   "source": [
    "Usemos primero el pipeline de transformers para traducir del [inglés al castellano](https://huggingface.co/Helsinki-NLP/opus-mt-en-es)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a271255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "272a803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tarea = 'translation'\n",
    "\n",
    "modelo = 'Helsinki-NLP/opus-mt-en-es'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56c22451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ia/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/opt/anaconda3/envs/ia/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "traductor = pipeline(task=tarea, model=modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e205eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# al traductor le pasamos directamente la string que queremos traducir\n",
    "\n",
    "\n",
    "frase = '''Retrieval Augmented Generation (RAG) is a pattern that works with pretrained \n",
    "            Large Language Models (LLM) and your own data to generate responses.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e170df04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'La generación aumentada de recuperación (RAG) es un patrón que funciona con modelos de lenguaje grande (LLM) preentrenados y sus propios datos para generar respuestas.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traductor(frase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce20a9ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La generación aumentada de recuperación (RAG) es un patrón que funciona con modelos de lenguaje grande (LLM) preentrenados y sus propios datos para generar respuestas.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta = traductor(frase)[0]['translation_text']\n",
    "\n",
    "respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e415e633",
   "metadata": {},
   "source": [
    "## 3 - Usando el modelo traductor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af8c812",
   "metadata": {},
   "source": [
    "Vamos a ver como usar el modelo fuera del pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8bde2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59d8cad",
   "metadata": {},
   "source": [
    "### 3.1 Tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ec8023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizador = AutoTokenizer.from_pretrained(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc67e38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianTokenizer(name_or_path='Helsinki-NLP/opus-mt-en-es', vocab_size=65001, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t65000: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b7d958",
   "metadata": {},
   "source": [
    "Características y funcionalidad de MarianTokenizer:\n",
    "\n",
    "1. Tokenización especializada:\n",
    "MarianTokenizer está optimizado para preparar datos para su uso con los modelos Marian, una serie de modelos de traducción automática basados en la arquitectura de transformers. Esto incluye la conversión de texto en secuencias de tokens que pueden ser procesadas por los modelos de red neuronal.\n",
    "\n",
    "2. Compatibilidad con múltiples idiomas:\n",
    "Marian y su tokenizer son notables por su capacidad para manejar múltiples idiomas, lo que los hace adecuados para sistemas de traducción automática multilingüe.\n",
    "\n",
    "3. Integración de tokens especiales:\n",
    "Incluye tokens especiales necesarios para la traducción, como tokens de inicio y fin de secuencia, que son cruciales para que los modelos de traducción determinen cuándo comenzar y terminar la generación de texto.\n",
    "\n",
    "4. Preprocesamiento eficiente:\n",
    "Al igual que otros tokenizadores avanzados, MarianTokenizer realiza operaciones de preprocesamiento que son esenciales para la traducción automática, como la normalización de texto, la segmentación en tokens y la conversión de estos tokens a índices numéricos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70209c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retrieval Augmented Generation (RAG) is a pattern that works with pretrained \\n            Large Language Models (LLM) and your own data to generate responses.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1011175",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = tokenizador(frase, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8696ba3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[54428,   315, 33159,  1579,   118, 37802,    22,  7679,  1051,    39,\n",
       "            31,     8,  8921,    27,  2102,    41,   790, 44968, 18050,  8313,\n",
       "         32856,    22,   399, 15706,    39,    10,    83,   564,   633,    13,\n",
       "          4049,  8840,     3,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3564f67b",
   "metadata": {},
   "source": [
    "### 3.2 - Modelo traductor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc06dd",
   "metadata": {},
   "source": [
    "Una obtenido el vector del tokenizador, lo usamos como entrada del modelo traductor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33673998",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_traductor = AutoModelForSeq2SeqLM.from_pretrained(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d592249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(65001, 512, padding_idx=65000)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=65001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_traductor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0820a201",
   "metadata": {},
   "source": [
    "**Componentes del MarianMTModel**:\n",
    "\n",
    "1. Embedding compartido:\n",
    "\n",
    "+ (shared): Usa una capa de embeddings que transforma los tokens de entrada en vectores de 512 dimensiones. El índice de padding 65000 es usado para marcar los tokens que no son parte del texto real y deberían ser ignorados en el procesamiento.\n",
    "\n",
    "\n",
    "2. Encoder:\n",
    "\n",
    "+ (embed_tokens): Embeddings para los tokens de entrada, compartidos con el decoder.\n",
    "+ (embed_positions): Emplea embeddings posicionales sinusoidales para incorporar información sobre la posición de los tokens en la secuencia.\n",
    "+ (layers): Compuesto por 6 capas de tipo MarianEncoderLayer, que incluyen:\n",
    "    + MarianAttention: Mecanismo de atención para procesar la entrada y determinar la relevancia de diferentes partes de la secuencia de entrada.\n",
    "    + Linear: Transformaciones lineales para proyectar los vectores de atención.\n",
    "    + LayerNorm: Normalización de capa para estabilizar la salida de la red.\n",
    "    + SiLU (Swish): Función de activación no lineal.\n",
    "    + fc1 y fc2: Transformaciones lineales que expanden y luego reducen las dimensiones, ayudando en la extracción de características.\n",
    "\n",
    "\n",
    "3. Decoder:\n",
    "\n",
    "+ (embed_tokens y embed_positions): Similar al encoder, usa embeddings para tokens y posiciones.\n",
    "+ (layers): Similar a las capas del encoder pero con algunas diferencias clave:\n",
    "    + self_attn y encoder_attn: Dos mecanismos de atención, donde self_attn se enfoca en la secuencia que se está generando y encoder_attn en la salida del encoder para incorporar contexto de la entrada.\n",
    "    + encoder_attn_layer_norm: Normalización adicional para la atención que conecta con el encoder.\n",
    "\n",
    "\n",
    "4. Cabeza de modelo de lenguaje (lm_head):\n",
    "\n",
    "+ Linear: Transformación lineal que mapea la salida del decoder al tamaño del vocabulario (65001 tokens), que es usado para predecir el siguiente token en la secuencia de salida.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Funcionamiento general**:\n",
    "\n",
    "El modelo MarianMTModel procesa la entrada textual en un idioma, la codifica en una representación intermedia utilizando su encoder, y luego usa el decoder para transformar esa representación intermedia en texto en otro idioma. La eficiencia y efectividad del modelo se derivan de su capacidad para manejar dependencias complejas y largas distancias en el texto, haciendo uso de la atención y la normalización para manejar el flujo de información a través de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5149c00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta = modelo_traductor.generate(**vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d52791a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[65000,    69,  5511,  8653,   199,     4,  4308,    22,  7679,  1051,\n",
       "            39,    43,    28, 12956,    15,  4642,    29,  4730,     4,  9100,\n",
       "          2294,    22,   399, 15706,    39,   790, 16466,    61,  3494,    11,\n",
       "            84,  3735,   506,    24,  3548,  4103,     3,     0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e49bb646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La generación aumentada de recuperación (RAG) es un patrón que funciona con modelos de lenguaje grande (LLM) preentrenados y sus propios datos para generar respuestas.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizador.batch_decode(respuesta, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8d501d",
   "metadata": {},
   "source": [
    "### 3.3 Resumen funcional\n",
    "Vamos a poner todo el código junto en una sola función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53f99b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# librerias\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "\n",
    "def traductor(frase: str, modelo: str) -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    Función para traducir un texto.\n",
    "    \n",
    "    Params:\n",
    "    + frase: string. Frase o texto a ser traducido.\n",
    "    + modelo: string. Modelo de transformers para traducir.\n",
    "    \n",
    "    Return:\n",
    "    String. Frase o texto traducido.\n",
    "    \"\"\"\n",
    "    \n",
    "    # con este objeto vectorizamos las palabras\n",
    "    tokenizador = AutoTokenizer.from_pretrained(modelo)\n",
    "    \n",
    "    \n",
    "    # creación del vector\n",
    "    vector = tokenizador(frase, return_tensors='pt')\n",
    "    \n",
    "    \n",
    "    # inicializacion del modelo traductor\n",
    "    modelo_traductor = AutoModelForSeq2SeqLM.from_pretrained(modelo)\n",
    "    \n",
    "    # tensor de respuesta del modelo\n",
    "    respuesta = modelo_traductor.generate(**vector)\n",
    "    \n",
    "    # respuesta del modelo\n",
    "    respuesta = tokenizador.batch_decode(respuesta, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "784dcd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La generación aumentada de recuperación (RAG) es un patrón que funciona con modelos de lenguaje grande (LLM) preentrenados y sus propios datos para generar respuestas.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traductor(frase, modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f860e396",
   "metadata": {},
   "source": [
    "## 4 - Ejemplo con más texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c05041",
   "metadata": {},
   "source": [
    "En la carpeta de archivos tenemos uno llamado `shakespeare.txt`, que contiene la obra del autor. Vamos a cargar los primeros sonetos y traducirlos con la función que acabamos de escribir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91dcc6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../files/shakespeare.txt') as f:\n",
    "    \n",
    "    soneto = f.read().split('\\n\\n')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68e5c3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n                     2\\n  When forty winters shall besiege thy brow,\\n  And dig deep trenches in thy beauty's field,\\n  Thy youth's proud livery so gazed on now,\\n  Will be a tattered weed of small worth held:\\n  Then being asked, where all thy beauty lies,\\n  Where all the treasure of thy lusty days;\\n  To say within thine own deep sunken eyes,\\n  Were an all-eating shame, and thriftless praise.\\n  How much more praise deserved thy beauty's use,\\n  If thou couldst answer 'This fair child of mine\\n  Shall sum my count, and make my old excuse'\\n  Proving his beauty by succession thine.\\n    This were to be new made when thou art old,\\n    And see thy blood warm when thou feel'st it cold.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soneto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e711e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"2 Cuando cuarenta inviernos asediare tu frente, Y cavare profundas trincheras en el campo de tu hermosura, La altivez de tu juventud Mirada así ahora, Sera una mala hierba de poco valor sostenida: Entonces se preguntara, Donde yace toda tu hermosura, Donde todo el tesoro de tus días lujuriosos; Para decir dentro de tus propios ojos profundos hundidos, Era una vergüenza que todo comía, y alabanza sin provecho. Cuanta más alabanza merecía el uso de tu hermosura, Si pudieras responder 'Este hermoso hijo mío sumará mi cuenta, Y hará mi vieja excusa' Demostrando su hermosura por sucesión la tuya. Esto sería nuevo cuando seas viejo, Y verás tu sangre caliente cuando la sientas fría.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traductor(soneto, modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a8e801",
   "metadata": {},
   "source": [
    "## 5 - Más modelos de traducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6238edb1",
   "metadata": {},
   "source": [
    "Desde el mismo grupo de desarrollo, Helsinki-NLP, vamos a probar la tradcción inversa, es decir, del castellano al inglés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc59d0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Al dotor Francisco Díaz, de Miguel de Cervantes, soneto\n",
    "\n",
    "texto = '''\n",
    "Tú, que con nuevo y sin igual decoro\n",
    "tantos remedios para un mal ordenas,\n",
    "bien puedes esperar d'estas arenas,\n",
    "del sacro Tajo, las que son de oro,\n",
    "    y el lauro que se debe al que un tesoro\n",
    "halla de ciencia, con tan ricas venas\n",
    "de raro advertimiento y salud llenas,\n",
    "contento y risa del enfermo lloro;\n",
    "    que por tu industria una deshecha piedra\n",
    "mil mármoles, mil bronces a tu fama\n",
    "dará sin invidiosas competencias;\n",
    "    daráte el cielo palma, el suelo yedra,\n",
    "pues que el uno y el otro ya te llama\n",
    "espíritu de Apolo en ambas ciencias.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc722941",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = 'Helsinki-NLP/opus-mt-es-en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "805a9940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You, who with new and without equal decorum so many remedies for an evil order, can well expect these sands, of the tajo sacred, those that are of gold, and the laur that is due to the one that a treasure finds of science, with so rich veins of rare warning and health full, content and laughter of the sick weep; that by your industry a stone thrown away a thousand marbles, a thousand bronzes to your fame will give without invidious competitions; the sky will give you palm, the ground yedra, for the one and the other already calls you spirit of Apollo in both sciences.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traductor(texto, modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0bef70",
   "metadata": {},
   "source": [
    "También, el mismo equipo de trabajo, ha desarrollado un modelo para traducir desde cualquier lengua romance al inglés. Vamos a probarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "397d9a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L’horloge – Charles Baudelaire\n",
    "\n",
    "texto = '''\n",
    "Horloge! Dieu sinestre, effrayant, impassible,\n",
    "Dont le doigt nous menace et nous dit: Souviens-toi! \n",
    "Les vibrantes Douleaurs dants ton cœur plein d'effroi \n",
    "Se plateront bientôt comme dans une cible;\n",
    "\n",
    "Le Plaisir vaporeux fuira vers l'horizon\n",
    "Ainsi qu'une sylphide au fond de la coulisse;\n",
    "Chaque instante te dévore un morceau du délice\n",
    "A chaque homme accordé pour toute sa saison.\n",
    "\n",
    "Trois mille six cents fois par heure, le Seconde\n",
    "Chuchote: Souviens-toi! –Rapide, avec sa voix\n",
    "D'insecte, Maintenant dit: Je suis Autrefois,\n",
    "Et j'ai pompé ta vie avec ma trompe immnonde!\n",
    "\n",
    "Remember! Souvients-toi! prodique! Esto memor!\n",
    "(Mon gosier de métal parle toutes les langues).\n",
    "Les minutes, mortel folâtre, sont des gangues\n",
    "Qu'il ne faut pas lâcher sans un extraire l'or!\n",
    "\n",
    "Souviens-toi! que le temps est un joueur avide\n",
    "Qui gagne sans tricher, à tout coup! c'est la loi,\n",
    "Le jour décroit; la nuit augmente; souviens-toi!\n",
    "Le gouffre a toujours soif; la clepsydre se vide.\n",
    "\n",
    "Tantôt sonera l'heure où le divin Hasard,\n",
    " Où l'auguste Vertu, ton épouse encor vierge,\n",
    " Où le Repentir même (oh! la dernière auberge!),\n",
    " Où tout te dira: meurs, vieux  lâche! Il est trop tard!\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c5ef2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = 'Helsinki-NLP/opus-mt-ROMANCE-en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0545371c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Horloge! God sinister, frightening, impassable, whose finger threatens us and says to us: Remember! The vibrant Painers with your heart full of fear will soon flatten themselves as if in a target; The vaporous Pleasure will flee to the horizon as well as a sylphide at the bottom of the back; Every instant devours a piece of delight to every man granted for all his season. Three thousand and six hundred times an hour, the Second Chuchote: Remember! \"Rapid, with his voice of insect, Now said: I am old times, And I have pumped your life with my immendous trumpet! Remember! Remember! Remember! This memorize! (My metal mouth speaks all languages).'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traductor(texto, modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7415bc1b",
   "metadata": {},
   "source": [
    "Otro ejemplo en italiano:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48cd19b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dante Alighieri: Tutti li miei penser\n",
    "\n",
    "\n",
    "texto = '''\n",
    "Tutti li miei penser parlan d’Amore;\n",
    "e hanno in lor sì gran varietate,\n",
    "ch’altro mi fa voler sua potestate,\n",
    "4altro folle ragiona il suo valore,\n",
    "\n",
    "altro sperando m’apporta dolzore,\n",
    "altro pianger mi fa spesse fiate;\n",
    "e sol s’accordano in cherer pietate,\n",
    "8tremando di paura che è nel core.\n",
    "\n",
    "Ond’io non so da qual matera prenda;\n",
    "e vorrei dire, e non so ch’io mi dica:\n",
    "11così mi trovo in amorosa erranza!\n",
    "\n",
    "E se con tutti voi fare accordanza,\n",
    "convenemi chiamar la mia nemica,\n",
    "madonna la Pietà, che mi difenda.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d0ecbd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"All my thoughts speak of Love; and they have such great variety in them, that other makes me want his power, 4other folly reason its value, other hoping for me sweetness, other weeping makes me thick breaths; and they only agree in cherer pietate, 8fearing that it is in the core. Ond. I don't know which mother she takes from; and I would like to say, and I don't know what I say: 11so I find myself in loving errance! And if with all of you I agree, please call me my enemy, Mother of Mercy, who defends me.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traductor(texto, modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc459ffa",
   "metadata": {},
   "source": [
    "Otro ejemplo en castellano:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ecc029e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este poema pertenece a Sonetos del amor oscuro, Federico Garcia Lorca\n",
    "\n",
    "\n",
    "texto = '''\n",
    "Amor de mis entrañas, viva muerte,\n",
    "en vano espero tu palabra escrita\n",
    "y pienso, con la flor que se marchita,\n",
    "que si vivo sin mí quiero perderte.\n",
    "\n",
    "El aire es inmortal. La piedra inerte\n",
    "ni conoce la sombra ni la evita.\n",
    "Corazón interior no necesita\n",
    "la miel helada que la luna vierte.\n",
    "\n",
    "Pero yo te sufrí. Rasgué mis venas,\n",
    "tigre y paloma, sobre tu cintura\n",
    "en duelo de mordiscos y azucenas.\n",
    "\n",
    "Llena pues de palabras mi locura\n",
    "o déjame vivir en mi serena\n",
    "noche del alma para siempre oscura.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e8a0978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Love of my bowels, living death, in vain I wait for your written word and think, with the flower that fades, that if I live without me I want to lose you. The air is immortal. The inert stone neither knows the shadow nor avoids it. Inner heart does not need the frozen honey that the moon pours out. But I suffered you. I tore my veins, tiger and dove, on your waist in mourning of bites and sugars. Fill my madness with words or let me live in my serene night of the soul forever dark.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traductor(texto, modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e8adc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "599px",
    "left": "165px",
    "top": "111.141px",
    "width": "264.271px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
