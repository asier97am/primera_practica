{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215a59c8",
   "metadata": {},
   "source": [
    "# 7 - Clasificación Zero Shot (Etiquetar)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/zero_shot.webp\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6cf6e2",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---Modelos-de-clasificación-Zero-Shot\" data-toc-modified-id=\"1---Modelos-de-clasificación-Zero-Shot-1\">1 - Modelos de clasificación Zero Shot</a></span></li><li><span><a href=\"#2---Pipeline-de-Transformers-para-Zero-Shot\" data-toc-modified-id=\"2---Pipeline-de-Transformers-para-Zero-Shot-2\">2 - Pipeline de Transformers para Zero Shot</a></span></li><li><span><a href=\"#3---Usando-el-modelo-Zero-Shot\" data-toc-modified-id=\"3---Usando-el-modelo-Zero-Shot-3\">3 - Usando el modelo Zero Shot</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1-Tokenizador\" data-toc-modified-id=\"3.1-Tokenizador-3.1\">3.1 Tokenizador</a></span></li><li><span><a href=\"#3.2---Modelo-Zero-Shot\" data-toc-modified-id=\"3.2---Modelo-Zero-Shot-3.2\">3.2 - Modelo Zero Shot</a></span></li><li><span><a href=\"#3.3-Resumen-funcional\" data-toc-modified-id=\"3.3-Resumen-funcional-3.3\">3.3 Resumen funcional</a></span></li></ul></li><li><span><a href=\"#4---Otro-Modelo-Zero-Shot\" data-toc-modified-id=\"4---Otro-Modelo-Zero-Shot-4\">4 - Otro Modelo Zero Shot</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cd9566",
   "metadata": {},
   "source": [
    "## 1 - Modelos de clasificación Zero Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1d7862",
   "metadata": {},
   "source": [
    "Los modelos de clasificación zero-shot son una técnica avanzada en NLP que permiten a un modelo clasificar datos en categorías que no se han visto durante el entrenamiento. Esto es especialmente útil en situaciones donde es poco práctico o imposible etiquetar manualmente datos para todas las categorías posibles. La clasificación zero-shot se basa en la capacidad de un modelo para generalizar a partir de conocimientos previos sin necesidad de ejemplos específicos durante el entrenamiento. Esto se logra generalmente mediante el uso de representaciones semánticas ricas y la transferencia de conocimiento.\n",
    "\n",
    "[Yin et al.](https://arxiv.org/abs/1909.00161) propusieron un método para usar modelos NLI (Natural Language Inference) preentrenados como clasificadores de secuencias zero-shot. El método funciona planteando la secuencia que se va a clasificar construyendo una hipótesis a partir de cada etiqueta candidata. Por ejemplo, si queremos evaluar si una secuencia pertenece a la clase \"urgente\" u otra etiqueta. Luego se calculan las probabilidades de pertenencia a la etiqueta.\n",
    "\n",
    "Este método es sorprendentemente efectivo en muchos casos, especialmente cuando se usa con modelos preentrenados más grandes como BART y RoBERTa. \n",
    "\n",
    "Técnicas comunes en zero-shot:\n",
    "\n",
    "1. **Embeddings Semánticos**:\n",
    "\n",
    "Los modelos zero-shot comúnmente utilizan embeddings de palabras o frases, como los generados por modelos como BERT o GPT, que capturan el significado de los textos en un espacio vectorial. El modelo aprende a asociar estos embeddings con sus correspondientes etiquetas durante el entrenamiento, aunque esas etiquetas específicas no se presenten en los datos de entrenamiento.\n",
    "\n",
    "\n",
    "2. **Aprendizaje por Transferencia**:\n",
    "\n",
    "Los modelos preentrenados en grandes corpus de datos y en múltiples tareas pueden adaptarse a la clasificación zero-shot aprovechando su conocimiento previo. Este enfoque permite que el modelo haga inferencias sobre categorías no vistas utilizando la similitud semántica entre las categorías conocidas y desconocidas.\n",
    "\n",
    "\n",
    "3. **Modelos Generativos**:\n",
    "\n",
    "Algunos enfoques de zero-shot involucran modelos generativos que pueden simular cómo se vería un ejemplo de una clase no vista, basándose en su descripción o en ejemplos de clases similares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6477d71",
   "metadata": {},
   "source": [
    "Vamos a usar un modelo de Facebook zero-shot basado en BART para nuestro primer ejemplo. Aquí el [link](https://huggingface.co/facebook/bart-large-mnli)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f96dc0",
   "metadata": {},
   "source": [
    "## 2 - Pipeline de Transformers para Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49176b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1509bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tarea = 'zero-shot-classification'\n",
    "\n",
    "modelo = 'facebook/bart-large-mnli'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ccf4bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ia/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "zero_pipe = pipeline(task=tarea, model=modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0042f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "etiquetas = ['urgente', 'no urgente', 'reparacion', 'revision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "653d8ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tengo un problema con mi computadora que necesita ser resuelto lo antes posible. De ello depende mi trabajo'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta = 'Tengo un problema con mi computadora que necesita ser resuelto lo antes posible. De ello depende mi trabajo'\n",
    "\n",
    "pregunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d4d97ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Tengo un problema con mi computadora que necesita ser resuelto lo antes posible. De ello depende mi trabajo',\n",
       " 'labels': ['urgente', 'reparacion', 'revision', 'no urgente'],\n",
       " 'scores': [0.9995989799499512,\n",
       "  0.775673508644104,\n",
       "  0.42463093996047974,\n",
       "  0.0005951290368102491]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_pipe(sequences=pregunta, candidate_labels=etiquetas, multi_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42ba0505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Tengo un problema con mi computadora que necesita ser resuelto lo antes posible. De ello depende mi trabajo',\n",
       " 'labels': ['urgente', 'reparacion', 'revision', 'no urgente'],\n",
       " 'scores': [0.9559415578842163,\n",
       "  0.03261444345116615,\n",
       "  0.00963128637522459,\n",
       "  0.0018127404619008303]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = {'sequences': pregunta, 'candidate_labels': etiquetas}\n",
    "\n",
    "zero_pipe(**prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3123611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'urgente'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta = zero_pipe(**prompt)\n",
    "\n",
    "respuesta['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "647c4fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'revision'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = {'sequences': 'Hola, mi coche funciona pero hace un ruido', \n",
    "          'candidate_labels': etiquetas}\n",
    "\n",
    "respuesta = zero_pipe(**prompt)\n",
    "\n",
    "respuesta['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d94f5a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no urgente'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = {'sequences': 'Hola, mi telefono no suena', \n",
    "          'candidate_labels': etiquetas}\n",
    "\n",
    "respuesta = zero_pipe(**prompt)\n",
    "\n",
    "respuesta['labels'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a53a6f",
   "metadata": {},
   "source": [
    "## 3 - Usando el modelo Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13818c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dc58c2",
   "metadata": {},
   "source": [
    "### 3.1 Tokenizador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d82bfaa",
   "metadata": {},
   "source": [
    "El tokenizador vuelve a ser el BartTokenizerFast que ya hemos visto anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cd0e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizador = AutoTokenizer.from_pretrained(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee7595ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartTokenizerFast(name_or_path='facebook/bart-large-mnli', vocab_size=50265, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a336b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = tokenizador.encode(pregunta, etiquetas[0], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7851943e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   565,  3314,   139,   542,   936,   102,  2764, 11163, 44316,\n",
       "           625,  4330,  1192,  3087,  4643,  3119,  6821,  5032,  8129,   560,\n",
       "          4600,  9876,   293,  8593,  4748,     4,   926,   364,  9905,  6723,\n",
       "           242, 11163,  2664,   873, 13745,     2,     2,  7150,  8530,     2]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a3f2f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b9c67",
   "metadata": {},
   "source": [
    "Podemos usar el propio tokenizador para ver el texto que representa dicho vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70e7e91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Tengo un problema con mi computadora que necesita ser resuelto lo antes posible. De ello depende mi trabajo</s></s>urgente</s>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizador.decode(vector[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88bc8c3",
   "metadata": {},
   "source": [
    "### 3.2 - Modelo Zero Shot\n",
    "\n",
    "Carguemos ahora el modelo de zero shot para extraer la probabilidad de pertenencia a una clase de un texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0bf3e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_modelo = AutoModelForSequenceClassification.from_pretrained(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbfda657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForSequenceClassification(\n",
       "  (model): BartModel(\n",
       "    (shared): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (classification_head): BartClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0ab49d",
   "metadata": {},
   "source": [
    "BartForSequenceClassification es una variante del modelo BART (Bidirectional and Auto-Regressive Transformers) diseñado para tareas de clasificación de secuencias. \n",
    "\n",
    "Sus componentes principales son:\n",
    "\n",
    "1. BartModel:\n",
    "\n",
    "+ shared: Una capa de embedding compartida para los tokens de entrada, que transforma los tokens (palabras) en vectores de 1024 dimensiones.\n",
    "\n",
    "\n",
    "+ encoder: Encargado de procesar la secuencia de entrada.\n",
    "    + embed_tokens: Utiliza el mismo embedding compartido para codificar los tokens.\n",
    "    + embed_positions: Embeddings posicionales para entender el orden de los tokens dentro de la secuencia.\n",
    "    + layers: Una serie de 12 capas de encoder (BartEncoderLayer), cada una con componentes de atención y normalización:\n",
    "        + self_attn: Atención auto-dirigida que ayuda al modelo a enfocarse en diferentes partes de la entrada.\n",
    "        + fc1 y fc2: Transformaciones lineales para procesar la salida de la atención.\n",
    "        + final_layer_norm y self_attn_layer_norm: Capas de normalización para estabilizar el aprendizaje.\n",
    "\n",
    "\n",
    "+ layernorm_embedding: Normalización adicional aplicada a los embeddings de entrada.\n",
    "\n",
    "\n",
    "2. decoder: Similar al encoder, pero diseñado para generar salida a partir de la representación codificada.\n",
    "\n",
    "+ embed_tokens y embed_positions: Análogos a los del encoder.\n",
    "\n",
    "+ layers: Cada capa del decoder (BartDecoderLayer) tiene funcionalidades similares a las del encoder, además de una atención adicional dirigida hacia la salida del encoder.\n",
    "\n",
    "\n",
    "3. classification_head: Una cabeza de clasificación específica para tareas de clasificación.\n",
    "\n",
    "+ dense: Una capa densa que transforma las características del decoder.\n",
    "\n",
    "+ dropout: Capa de Dropout para reducir el sobreajuste.\n",
    "\n",
    "+ out_proj: Capa lineal final que mapea la salida a las categorías de clasificación deseadas (en este caso, parece ser 3 categorías).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Funcionalidad general:\n",
    "\n",
    "+ BartForSequenceClassification toma una secuencia de entrada, procesa a través de su arquitectura de encoder y decoder para entender y generar una representación interna, y luego utiliza la cabeza de clasificación para determinar la categoría a la que pertenece la secuencia.\n",
    "\n",
    "+ Este modelo es capaz de manejar relaciones complejas y largas distancias dentro de textos, haciéndolo ideal para tareas como análisis de sentimientos, identificación de temas, y más, donde la secuencia completa de texto necesita ser entendida para hacer una clasificación efectiva.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "484d8661",
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta = zero_modelo(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b203242b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9446,  1.8553, -0.7670]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa17459d",
   "metadata": {},
   "source": [
    "Con `softmax`, un clasificador multietiqueta, podemos extraer la probabilidad de pertenencia a la clase. La probabilidad de ser cierta la etiqueta es el segundo elemento de este tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67c3343e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0204, 0.9132, 0.0663]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta.logits.softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7dc83b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913235604763031"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta.logits.softmax(dim=1)[0][1].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee49b3e",
   "metadata": {},
   "source": [
    "### 3.3 Resumen funcional\n",
    "\n",
    "Vamos a poner todo el código junto en una sola función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75e93945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# librerias \n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "def zero_shot(pregunta: str, etiqueta: str, modelo: str) -> float:\n",
    "    \n",
    "    \"\"\"\n",
    "    Función para zero shot\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # con este objeto vectorizamos las palabras\n",
    "    tokenizador = AutoTokenizer.from_pretrained(modelo)\n",
    "    \n",
    "    \n",
    "    # creación del vector\n",
    "    vector = tokenizador.encode(pregunta, etiqueta, return_tensors='pt')\n",
    "    \n",
    "    \n",
    "    # inicializacion del modelo Zero Shot\n",
    "    zero_modelo = AutoModelForSequenceClassification.from_pretrained(modelo)\n",
    "    \n",
    "    \n",
    "    # respuesta del modelo al pasarle el vector\n",
    "    respuesta = zero_modelo(vector)\n",
    "    \n",
    "    \n",
    "    # probabilidad de pertenencia\n",
    "    probabilidad = respuesta.logits.softmax(dim=1)[0, 1].item()\n",
    "    \n",
    "    \n",
    "    return probabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae387f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos pregunta, etiqueta y modelo\n",
    "\n",
    "pregunta = 'Tengo un problema con mi computadora que necesita ser resuelto lo antes posible. De ello depende mi trabajo'\n",
    "\n",
    "etiqueta = 'revision'\n",
    "\n",
    "modelo = 'facebook/bart-large-mnli'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9447ef26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9071726202964783"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot(pregunta, etiqueta, modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45d0fad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urgente 0.913235604763031\n",
      "no urgente 0.058300167322158813\n",
      "reparacion 0.8391153812408447\n",
      "revision 0.9071726202964783\n"
     ]
    }
   ],
   "source": [
    "for et in etiquetas:\n",
    "    \n",
    "    res = zero_shot(pregunta, et, modelo)\n",
    "    \n",
    "    print(et, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc65ce7d",
   "metadata": {},
   "source": [
    "## 4 - Otro Modelo Zero Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb311f1",
   "metadata": {},
   "source": [
    "Probemos ahora otro [modelo](https://huggingface.co/MoritzLaurer/deberta-v3-large-zeroshot-v2.0), que según su ficha en el hub de Hugging Face, tiene un 15% de mejora en rendimiento, a pesar de que pesa un 50% menos. Veamos como usarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c01ab46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(task = \"zero-shot-classification\", \n",
    "                model=\"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b33aabca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['urgente', 'no urgente', 'reparacion', 'revision']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e54ea73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tengo un problema con mi computadora que necesita ser resuelto lo antes posible. De ello depende mi trabajo'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0bdefdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 787 ms, sys: 384 ms, total: 1.17 s\n",
      "Wall time: 846 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Tengo un problema con mi computadora que necesita ser resuelto lo antes posible. De ello depende mi trabajo',\n",
       " 'labels': ['urgente', 'reparacion', 'revision', 'no urgente'],\n",
       " 'scores': [0.9465862512588501,\n",
       "  0.6432700157165527,\n",
       "  0.00859528873115778,\n",
       "  0.0009899158030748367]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe(sequences=pregunta, candidate_labels=etiquetas, multi_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80b416df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 447 ms, sys: 204 ms, total: 651 ms\n",
      "Wall time: 488 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Tengo un problema con mi computadora que necesita ser resuelto lo antes posible. De ello depende mi trabajo',\n",
       " 'labels': ['urgente', 'reparacion', 'revision', 'no urgente'],\n",
       " 'scores': [0.9995989799499512,\n",
       "  0.775673508644104,\n",
       "  0.42463093996047974,\n",
       "  0.0005951290368102491]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "zero_pipe(sequences=pregunta, candidate_labels=etiquetas, multi_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd743a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "599px",
    "left": "80px",
    "top": "0px",
    "width": "260.278px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
