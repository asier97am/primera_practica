{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6127aab",
   "metadata": {},
   "source": [
    "# 6 - Respuesta a Preguntas de Tablas (Table Question Answering)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/tqa.webp\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abd1a24",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---Modelos-de-respuesta-a-preguntas-de-tablas-(TQA)\" data-toc-modified-id=\"1---Modelos-de-respuesta-a-preguntas-de-tablas-(TQA)-1\">1 - Modelos de respuesta a preguntas de tablas (TQA)</a></span></li><li><span><a href=\"#2---Carga-de-datos\" data-toc-modified-id=\"2---Carga-de-datos-2\">2 - Carga de datos</a></span></li><li><span><a href=\"#3---Pipeline-de-Transformers-para-TQA\" data-toc-modified-id=\"3---Pipeline-de-Transformers-para-TQA-3\">3 - Pipeline de Transformers para TQA</a></span></li><li><span><a href=\"#4---Usando-el-modelo-TQA\" data-toc-modified-id=\"4---Usando-el-modelo-TQA-4\">4 - Usando el modelo TQA</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.1---Tokenizador\" data-toc-modified-id=\"4.1---Tokenizador-4.1\">4.1 - Tokenizador</a></span></li><li><span><a href=\"#4.2---Modelo-TQA\" data-toc-modified-id=\"4.2---Modelo-TQA-4.2\">4.2 - Modelo TQA</a></span></li><li><span><a href=\"#4.3---Resumen-de-código\" data-toc-modified-id=\"4.3---Resumen-de-código-4.3\">4.3 - Resumen de código</a></span></li></ul></li><li><span><a href=\"#5---Otro-modelo-TQA\" data-toc-modified-id=\"5---Otro-modelo-TQA-5\">5 - Otro modelo TQA</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dccb0d",
   "metadata": {},
   "source": [
    "## 1 - Modelos de respuesta a preguntas de tablas (TQA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b526b",
   "metadata": {},
   "source": [
    "Los modelos de respuesta a preguntas de tablas (Table Question Answering, TQA) son una especialización dentro del campo de respuesta a preguntas (Question Answering, QA) que se centra en interpretar y responder preguntas basadas en datos presentados en formatos tabulares. Estos modelos están diseñados para entender y manipular la información contenida en tablas, lo que implica habilidades tanto de comprensión del lenguaje natural como de manejo de datos estructurados. Hay que decir que este tipo de modelos se están generalizando con la estructura del RAG y están dejando de ser desarrollados.\n",
    "\n",
    "Las características de los modelos TQA son:\n",
    "\n",
    "1. **Comprensión de Datos Estructurados**:\n",
    "\n",
    "A diferencia de los modelos de QA tradicionales que se enfocan en textos continuos, los modelos TQA deben entender la organización y la relación entre datos en un formato tabular, como filas, columnas y celdas que pueden contener números, texto o fechas.\n",
    "\n",
    "2. **Interpretación Contextual**:\n",
    "\n",
    "Estos modelos necesitan interpretar las preguntas en el contexto de la información tabular presentada. Esto incluye entender términos relacionados con operaciones tabulares, como \"máximo\", \"mínimo\", \"promedio\", \"total\", y cómo se aplican estos términos a los datos específicos de una tabla.\n",
    "\n",
    "3. **Manipulación de Datos**: \n",
    "\n",
    "Además de interpretar la tabla, estos modelos a menudo realizan operaciones sobre los datos, como sumas, promedios, o comparaciones, para extraer o calcular la respuesta correcta.\n",
    "\n",
    "\n",
    "Tecnologías que utilizan los modelos TQA:\n",
    "\n",
    "Los modelos TQA a menudo combinan técnicas de procesamiento del lenguaje natural con métodos de procesamiento de datos. Algunos enfoques incluyen:\n",
    "\n",
    "+ Modelos basados en Transformers: Utilizan modelos de lenguaje preentrenados que han sido adaptados para trabajar con datos tabulares.\n",
    "+ Parsing de Consultas: Convertir preguntas del lenguaje natural en comandos que pueden ejecutarse sobre los datos, similar a cómo se formulan consultas en lenguajes de bases de datos como SQL.\n",
    "+ Redes Neuronales que Modelan Relaciones entre Entidades: Redes que pueden captar y modelar las relaciones complejas entre diferentes entidades en las tablas.\n",
    "\n",
    "Algunas aplicaciones son:\n",
    "+ Análisis de Negocios y Finanzas: Automatización de la generación de informes y respuestas a preguntas sobre datos financieros almacenados en tablas.\n",
    "+ Asistencia en Salud: Responder a preguntas sobre datos de pacientes almacenados en registros médicos electrónicos.\n",
    "+ Investigación Científica y Académica: Automatización de la búsqueda y extracción de datos de tablas en publicaciones científicas y académicas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f7b704",
   "metadata": {},
   "source": [
    "## 2 - Carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4b58d9",
   "metadata": {},
   "source": [
    "Vamos a usar un archivo csv proporcionado por Renfe. Los datos son de [volumen de viajeros por estación en cercanías de Asturias en el año 2018](https://data.renfe.com/dataset/volumen-de-viajeros-por-franja-horaria-asturias). Cargaremos en primer lugar estos en forma tabular usando la librería `pandas` de python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13047dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "417e4ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CODIGO_ESTACION</th>\n",
       "      <th>NOMBRE_ESTACION</th>\n",
       "      <th>NUCLEO_CERCANIAS</th>\n",
       "      <th>TRAMO_HORARIO</th>\n",
       "      <th>VIAJEROS_SUBIDOS</th>\n",
       "      <th>VIAJEROS_BAJADOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15118</td>\n",
       "      <td>PUENTE DE LOS FIERROS</td>\n",
       "      <td>ASTURIAS</td>\n",
       "      <td>07:00 - 07:30</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15118</td>\n",
       "      <td>PUENTE DE LOS FIERROS</td>\n",
       "      <td>ASTURIAS</td>\n",
       "      <td>07:30 - 08:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15118</td>\n",
       "      <td>PUENTE DE LOS FIERROS</td>\n",
       "      <td>ASTURIAS</td>\n",
       "      <td>08:00 - 08:30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15118</td>\n",
       "      <td>PUENTE DE LOS FIERROS</td>\n",
       "      <td>ASTURIAS</td>\n",
       "      <td>11:00 - 11:30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15118</td>\n",
       "      <td>PUENTE DE LOS FIERROS</td>\n",
       "      <td>ASTURIAS</td>\n",
       "      <td>11:30 - 12:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CODIGO_ESTACION        NOMBRE_ESTACION NUCLEO_CERCANIAS  TRAMO_HORARIO  \\\n",
       "0            15118  PUENTE DE LOS FIERROS         ASTURIAS  07:00 - 07:30   \n",
       "1            15118  PUENTE DE LOS FIERROS         ASTURIAS  07:30 - 08:00   \n",
       "2            15118  PUENTE DE LOS FIERROS         ASTURIAS  08:00 - 08:30   \n",
       "3            15118  PUENTE DE LOS FIERROS         ASTURIAS  11:00 - 11:30   \n",
       "4            15118  PUENTE DE LOS FIERROS         ASTURIAS  11:30 - 12:00   \n",
       "\n",
       "   VIAJEROS_SUBIDOS  VIAJEROS_BAJADOS  \n",
       "0                 0                 2  \n",
       "1                 1                 0  \n",
       "2                 1                 0  \n",
       "3                 1                 1  \n",
       "4                 0                 0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabla = pd.read_csv('../../../files/asturias_viajeros_por_franja_horaria.csv', sep=';')\n",
    "\n",
    "tabla.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac0e1c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1381 entries, 0 to 1380\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   CODIGO_ESTACION   1381 non-null   int64 \n",
      " 1   NOMBRE_ESTACION   1381 non-null   object\n",
      " 2   NUCLEO_CERCANIAS  1381 non-null   object\n",
      " 3   TRAMO_HORARIO     1381 non-null   object\n",
      " 4   VIAJEROS_SUBIDOS  1381 non-null   int64 \n",
      " 5   VIAJEROS_BAJADOS  1381 non-null   int64 \n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 310.1 KB\n"
     ]
    }
   ],
   "source": [
    "tabla.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49621b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['PUENTE DE LOS FIERROS', 'LA FRECHA', 'CAMPOMANES',\n",
       "       'LA COBERTORIA', 'POLA DE LENA', 'VILLALLANA', 'UJO', 'SANTULLANO',\n",
       "       'MIERES-PUENTE', 'ABLAÑA', 'LA PEREDA-RIOSA', 'OLLONIEGO',\n",
       "       'SOTO DE REY', 'LAS SEGADAS', 'EL CALEYO', 'OVIEDO', 'LUGONES',\n",
       "       'LA CORREDORIA', 'LLAMAQUIQUE', 'LUGO DE LLANERA',\n",
       "       'VILLABONA DE ASTURIAS', 'SERIN', 'MONTEANA',\n",
       "       'VILLABONA TABLADIELLO', 'VERIÑA', 'CALZADA DE ASTURIAS',\n",
       "       'GIJON-SANZ CRESPO', 'SANTA EULALIA DE MANZANEDA', 'TUDELA-VEGUIN',\n",
       "       'PEÑA RUBIA', 'BARROS', 'LA FELGUERA', 'SAMA', 'CIAÑO',\n",
       "       'EL ENTREGO', 'FERROÑES', 'CANCIENES', 'NUBLEDO', 'VILLALEGRE',\n",
       "       'LA ROCICA', 'AVILES', 'SAN JUAN DE NIEVA', 'LOS CAMPOS'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabla.NOMBRE_ESTACION.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b30e53f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['07:00 - 07:30', '07:30 - 08:00', '08:00 - 08:30', '11:00 - 11:30',\n",
       "       '11:30 - 12:00', '13:30 - 14:00', '16:00 - 16:30', '16:30 - 17:00',\n",
       "       '17:30 - 18:00', '19:00 - 19:30', '21:00 - 21:30', '21:30 - 22:00',\n",
       "       '22:30 - 23:00', '19:30 - 20:00', '22:00 - 22:30', '08:30 - 09:00',\n",
       "       '12:00 - 12:30', '14:00 - 14:30', '17:00 - 17:30', '10:30 - 11:00',\n",
       "       '15:30 - 16:00', '06:30 - 07:00', '09:00 - 09:30', '09:30 - 10:00',\n",
       "       '10:00 - 10:30', '12:30 - 13:00', '13:00 - 13:30', '14:30 - 15:00',\n",
       "       '15:00 - 15:30', '18:00 - 18:30', '18:30 - 19:00', '20:00 - 20:30',\n",
       "       '20:30 - 21:00', '23:00 - 23:30', '23:30 - 00:00', '06:00 - 06:30',\n",
       "       '00:00 - 00:30', '05:30 - 06:00', '05:00 - 05:30'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabla.TRAMO_HORARIO.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5f8c0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43 entries, 0 to 42\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   NOMBRE_ESTACION   43 non-null     object\n",
      " 1   VIAJEROS_SUBIDOS  43 non-null     int64 \n",
      " 2   VIAJEROS_BAJADOS  43 non-null     int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1.1+ KB\n"
     ]
    }
   ],
   "source": [
    "tabla_estacion = tabla.groupby('NOMBRE_ESTACION').agg({'VIAJEROS_SUBIDOS': 'sum',\n",
    "                                                       'VIAJEROS_BAJADOS': 'sum'}).reset_index()\n",
    "\n",
    "tabla_estacion.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "157a380d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39 entries, 0 to 38\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   TRAMO_HORARIO     39 non-null     object\n",
      " 1   VIAJEROS_SUBIDOS  39 non-null     int64 \n",
      " 2   VIAJEROS_BAJADOS  39 non-null     int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1.0+ KB\n"
     ]
    }
   ],
   "source": [
    "tabla_hora = tabla.groupby('TRAMO_HORARIO').agg({'VIAJEROS_SUBIDOS': 'sum',\n",
    "                                                       'VIAJEROS_BAJADOS': 'sum'}).reset_index()\n",
    "\n",
    "tabla_hora.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "219447d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TRAMO_HORARIO': ['00:00 - 00:30',\n",
       "  '05:00 - 05:30',\n",
       "  '05:30 - 06:00',\n",
       "  '06:00 - 06:30',\n",
       "  '06:30 - 07:00'],\n",
       " 'VIAJEROS_SUBIDOS': [1, 10, 33, 119, 380],\n",
       " 'VIAJEROS_BAJADOS': [11, 3, 13, 33, 91]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabla_hora.head().to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36c0a1",
   "metadata": {},
   "source": [
    "## 3 - Pipeline de Transformers para TQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa861f6e",
   "metadata": {},
   "source": [
    "El modelo que vamos a usar es [TAPEX](https://huggingface.co/microsoft/tapex-large-finetuned-wtq\n",
    ") (Table Pre-training via Execution) de Microsoft, un modelo que pesa 1.7Gb. Su enfoque de pre-entrenamiento es conceptualmente simple y empíricamente potente usado para potenciar modelos existentes con habilidades de razonamiento de tablas. TAPEX realiza el pre-entrenamiento de tablas aprendiendo un ejecutor neural SQL sobre un corpus sintético, el cual se obtiene mediante la síntesis automática de consultas SQL ejecutables.\n",
    "\n",
    "TAPEX se basa en la arquitectura BART, el modelo codificador-decodificador (seq2seq) transformer con un codificador bidireccional (similar a BERT) y un decodificador autoregresivo (similar a GPT).\n",
    "\n",
    "Este modelo es el modelo tapex-base ajustado finamente en el conjunto de datos [WikiTableQuestions](https://huggingface.co/datasets/wikitablequestions).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6459ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c04a9998",
   "metadata": {},
   "outputs": [],
   "source": [
    "tarea = 'table-question-answering'\n",
    "\n",
    "modelo = 'microsoft/tapex-large-finetuned-wtq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "046d69f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ia/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "tqa_pipe = pipeline(task=tarea, model=modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32b8847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pregunta = 'quitando Oviedo, en que estacion se suben más viajeros'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d162756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': ' llamaquique'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tqa_pipe(query=pregunta, table=tabla_estacion.to_dict(orient='list'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a481a92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': ' llamaquique'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = {'query': pregunta, 'table': tabla_estacion.to_dict(orient='list')}\n",
    "\n",
    "tqa_pipe(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3277e596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOMBRE_ESTACION</th>\n",
       "      <th>VIAJEROS_SUBIDOS</th>\n",
       "      <th>VIAJEROS_BAJADOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>OVIEDO</td>\n",
       "      <td>4261</td>\n",
       "      <td>4128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LLAMAQUIQUE</td>\n",
       "      <td>2662</td>\n",
       "      <td>2893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LUGONES</td>\n",
       "      <td>1674</td>\n",
       "      <td>1662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GIJON-SANZ CRESPO</td>\n",
       "      <td>1450</td>\n",
       "      <td>1334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LA CORREDORIA</td>\n",
       "      <td>1332</td>\n",
       "      <td>1205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      NOMBRE_ESTACION  VIAJEROS_SUBIDOS  VIAJEROS_BAJADOS\n",
       "26             OVIEDO              4261              4128\n",
       "18        LLAMAQUIQUE              2662              2893\n",
       "21            LUGONES              1674              1662\n",
       "10  GIJON-SANZ CRESPO              1450              1334\n",
       "12      LA CORREDORIA              1332              1205"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabla_estacion.sort_values(by='VIAJEROS_SUBIDOS', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b19b76",
   "metadata": {},
   "source": [
    "## 4 - Usando el modelo TQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9600b2",
   "metadata": {},
   "source": [
    "Vamos a ver como se usa el modelo TQA fuera del pipeline y describimos tanto el tokenizador con el propio modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "856542db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3691f1",
   "metadata": {},
   "source": [
    "### 4.1 - Tokenizador\n",
    "\n",
    "Vectorizamos tanto la tabla como la pregunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c62ae27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ia/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizador = AutoTokenizer.from_pretrained(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3093616a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TapexTokenizer(name_or_path='microsoft/tapex-large-finetuned-wtq', vocab_size=50265, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fefa8ec",
   "metadata": {},
   "source": [
    "La descripción del tokenizador es la siguiente:\n",
    "\n",
    "1. Nombre o ruta: microsoft/tapex-large-finetuned-wtq indica que el tokenizador es una parte del modelo TAPEX, específicamente una versión grande (\"large\") que ha sido afinada para trabajar bien con la tarea \"WikiTableQuestions\".\n",
    "\n",
    "2. Tamaño del vocabulario: vocab_size=50265 significa que el modelo utiliza un vocabulario de 50,265 tokens diferentes.\n",
    "\n",
    "3. Longitud máxima del modelo: model_max_length=1024 señala que el tokenizador puede manejar secuencias de hasta 1024 tokens.\n",
    "\n",
    "4. Rapidez: is_fast=False indica que este tokenizador no utiliza la implementación rápida de tokenizadores como la que ofrece Hugging Face con Rust, sino una versión más tradicional en Python.\n",
    "\n",
    "5. Configuración de padding y truncamiento: Ambos se realizan hacia la 'derecha' (right), lo que significa que si una secuencia es demasiado corta o larga, los ajustes se aplicarán al final de la secuencia.\n",
    "\n",
    "6. Tokens especiales: Estos incluyen varios tokens útiles para diferentes propósitos en el modelado, como:\n",
    "\n",
    "    + `<s>` y `</s>`: Inicio y fin de secuencia.\n",
    "    \n",
    "    + `<unk>`: Token para palabras desconocidas.\n",
    "    \n",
    "    + `<pad>`: Padding.\n",
    "    \n",
    "    + `<mask>`: Máscara, usado en tareas como el entrenamiento de modelos de lenguaje enmascarado.\n",
    "    \n",
    "    + `<cls>`: Usado comúnmente para representar el comienzo de una secuencia en ciertos modelos como BERT.\n",
    "    \n",
    "\n",
    "7. Limpieza de espacios en la tokenización: clean_up_tokenization_spaces=True implica que se eliminarán espacios extra que puedan haber sido introducidos durante el proceso de tokenización.\n",
    "\n",
    "8. Diccionario de tokens añadidos: Este diccionario muestra la configuración de algunos tokens especiales, destacando si estos tokens deben tener espacio antes o después cuando se insertan en un texto, si deben considerarse como una palabra completa, y si son \"especiales\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b4de246",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = tokenizador(table=tabla_estacion, query=pregunta, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6d50f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  6602,  5502, 19414,  2550,   139,     6,  1177,  1192,  3304,\n",
       "          8647,   842,  2849,   225,   475,  6417,  1241,   267, 22070, 11311,\n",
       "          4832,   295,  5223,   241,  1215,   990,  8647,  1721,  1241,   267,\n",
       "         22070,  1215, 10936,   808,   366,  1721,  1241,   267, 22070,  1215,\n",
       "           428,  1176,  6510,  3236,   112,  4832,  4091,  2560, 14379,  1721,\n",
       "          5595,  1721,  5169,  3236,   132,  4832,  6402,  4755,  1721,   231,\n",
       "          1922,  1721,   231,  3414,  3236,   155,  4832,  2003,  3985,  1721,\n",
       "           316,  1721,   316,  3236,   204,  4832, 13011,   329,  2095,   263,\n",
       "         12976,   710,  5003,  1721,   262,  2831,  1721,   290,  5352,  3236,\n",
       "           195,  4832,  2205, 16187,   293,  1721,   753,  1721,   564,  3236,\n",
       "           231,  4832,    64,  2520, 39268,  1721,  7994,  1721,  7994,  3236,\n",
       "           262,  4832,   740,   493, 14182,  1721,  6657,  1721,  5549,  3236,\n",
       "           290,  4832,  1615,   740,  1627,  9839,  1721,   504,  1721,   753,\n",
       "          3236,   361,  4832,  1615,  3838,   241,  2977,  1721, 30527,  1721,\n",
       "         31488,  3236,   158,  4832, 16022,  1001,  6303,   293,  1721,   501,\n",
       "          1721,   508,  3236,   365,  4832,   821,  2161,   261,    12, 14832,\n",
       "           329,   740, 36217,   139,  1721,   501,  1096,  1721,   508,  3079,\n",
       "          3236,   316,  4832,   897,  1029,  6747,  7228,  1721,   204,  1721,\n",
       "           204,  3236,   508,  4832,   897,  9240,  2050,  7228,  1721,   508,\n",
       "          2881,  1721,   316,  2546,  3236,   501,  4832,   897, 14383,  5521,\n",
       "          3843,  1721, 40156,  1721, 27264,  3236,   379,  4832,   897,  7619,\n",
       "          7794,  1721,   112,  1721,   132,  3236,   545,  4832,   897,  3723,\n",
       "          2050,   102,    12,  1069,  5166,  1721,  4059,  1721,   971,  3236,\n",
       "           601,  4832,   897,  4533,   438,  2426,  1721, 36202,  1721, 27012,\n",
       "          3236,   504,  4832,  5573,   842,   571, 13920,  1721, 25664,  1721,\n",
       "         25728,  3236,   753,  4832, 19385,  2583,  2253,  5150,  1721,   973,\n",
       "          5379,  1721,   971,  6478,  3236,   291,  4832,  3774,  2205,   366,\n",
       "          1721, 27965,  1721, 26289,  3236,   733,  4832, 33944,   139,   263,\n",
       "         19385,   260,  3843,  1721,   290,  4156,  1721,   290,  5243,  3236,\n",
       "           820,  4832, 33944,  6909,  1721,   545,  5243,  1721,   545,  5379,\n",
       "          3236,   883,  4832,   475,   324,  1535,    12, 30738,  8530,  1721,\n",
       "           262,   698,  1721,   290,  3079,  3236,   706,  4832,  6154,   859,\n",
       "          1113,  1721,  4893,  1721,  4268,  3236,   564,  4832,   295,  1792,\n",
       "          1329,   139,  1721,  2107,  1721,   706,  3236,   973,  4832,  1021,\n",
       "           890,   261,   324,  2977,  1721,   291,  1721,   973,  3236,   974,\n",
       "          4832, 19414,  2550,   139,  1721,   204, 32972,  1721,   204, 21540,\n",
       "          3236,   971,  4832,  3723, 14379, 14204,   493,  1721,   379,  1721,\n",
       "           706,  3236,  1132,  4832,  8385,   102,   263,   784,  4242,  1721,\n",
       "         27393,  1721,  2843,   176,  3236,   389,  4832, 18829,  8530,   263,\n",
       "          3774,   856,   906,  3985,  1721,   361,  1721,   501,  3236,  1105,\n",
       "          4832,   579,  2583,  1721, 18661,  1721, 25884,  3236,  2107,  4832,\n",
       "         15610,  1236,  7372,   263,   295,   324,  3952,  1721,   883,  1721,\n",
       "           291,  3236,  2357,  4832,   579, 11485,   364,   922,  6956,   263,\n",
       "           313, 13571, 14444,  1721,   601,  1721,   706,  3236,  2631,  4832,\n",
       "           579,   927,  5023,  2601,  1721,  2491,  1721,   843,  3236,  1718,\n",
       "          4832,  6821,   179,  1721,  4059,  1721,  1191,  3236,  2491,  4832,\n",
       "           579,  7087,   263,   769,   219,  1721,  5356,  1721,  3490,  3236,\n",
       "          2908,  4832,   326,  1906,  6658,    12,   548,  5521,   179,  1721,\n",
       "          5595,  1721,  4431,  3236,  2843,  4832,  1717,  3548,  1721, 23006,\n",
       "          1721, 15966,  3236,  3191,  4832,  4342,   118, 14379,  1721,  2631,\n",
       "          1721,   974,  3236,   843,  4832, 17676,   873,  4488,   263, 12976,\n",
       "           710,  5003,  1721,  8176,  1721,  5595,  3236,  3492,  4832, 17676,\n",
       "           873,  4488, 12207, 15356,   324,  9905,  1721,   883,  1721,   706,\n",
       "          3236,  3330,  4832, 17676,  1627, 28819,  1721, 42482,  1721, 40278,\n",
       "          3236,  3557,  4832, 17676,  1250,  1113,  1721,  1105,  1721,  2843,\n",
       "             2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "679087c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 551])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1d0eff",
   "metadata": {},
   "source": [
    "### 4.2 - Modelo TQA\n",
    "Usamos el modelo TQA con el vector que acabamos de sacar del tokenizador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d9bd7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_tqa = AutoModelForSeq2SeqLM.from_pretrained(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c35ae7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_tqa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82e154d",
   "metadata": {},
   "source": [
    "BartForConditionalGeneration es una variante de la arquitectura BART (Bidirectional and Auto-Regressive Transformers) diseñada para tareas de generación de texto condicional, como la traducción automática o resumen de texto. A continuación, explicamos cada parte relevante de la estructura del modelo:\n",
    "\n",
    "1. BartModel:\n",
    "\n",
    "    + shared: Un embedding compartido que se utiliza tanto en el encoder como en el decoder para representar las palabras. Tiene un tamaño de vocabulario de 50265 y cada palabra se representa con un vector de 1024 dimensiones. padding_idx=1 indica que el índice 1 en el vocabulario se utiliza para el token de padding.\n",
    "    + encoder: Es la parte del modelo que procesa la entrada de texto.\n",
    "        + embed_tokens: Usa el mismo embedding compartido para convertir tokens de entrada en vectores.\n",
    "        + embed_positions: Embeddings posicionales que ayudan al modelo a entender el orden de las palabras en la entrada.\n",
    "        + layers: Una lista de 12 capas de tipo BartEncoderLayer, cada una conteniendo:\n",
    "            + self_attn: Una capa de atención (Scaled Dot-Product Attention) que ayuda al modelo a enfocarse en diferentes partes de la entrada para cada palabra.\n",
    "            + activation_fn: Función de activación GELU utilizada en transformaciones lineales.\n",
    "            + fc1 y fc2: Capas lineales que transforman los datos entre diferentes espacios dimensionales.\n",
    "            + final_layer_norm y self_attn_layer_norm: Normalización de capa para estabilizar el aprendizaje.\n",
    "    + decoder: Similar al encoder pero con capacidades adicionales para la generación de texto.\n",
    "        + embed_tokens y embed_positions: Análogos a los del encoder.\n",
    "        + layers: Lista de 12 capas BartDecoderLayer, cada una con:\n",
    "            + self_attn y encoder_attn: Atención propia para entender la salida generada hasta ahora y atención del encoder para integrar la información de la entrada.\n",
    "            + activation_fn, fc1, fc2: Similares a las del encoder.\n",
    "            + final_layer_norm, self_attn_layer_norm, encoder_attn_layer_norm: Normalización para estabilizar el aprendizaje.\n",
    "\n",
    "\n",
    "2. lm_head: Una capa lineal que mapea la salida del decoder de nuevo al espacio del vocabulario para generar el texto final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8409e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,     0, 19385,  2583,  2253,  5150,     2]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado = modelo_tqa.generate(**vector)\n",
    "\n",
    "resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b6fc2",
   "metadata": {},
   "source": [
    "Le pedimos al modelo directamente que genere una respuesta. Su respuesta es un tensor que luego pasaremos por tokenizador para que nos devuelva un resultado en formato string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a2dcedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' llamaquique'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizador.batch_decode(resultado, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc159cf",
   "metadata": {},
   "source": [
    "### 4.3 - Resumen de código\n",
    "\n",
    "Vamos a poner todo el código junto en una sola celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b333ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ia/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' llamaquique'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tabla = pd.read_csv('../../../files/asturias_viajeros_por_franja_horaria.csv', sep=';')\n",
    "\n",
    "\n",
    "tabla_estacion = tabla.groupby('NOMBRE_ESTACION').agg({'VIAJEROS_SUBIDOS': 'sum',\n",
    "                                                       'VIAJEROS_BAJADOS': 'sum'}).reset_index()\n",
    "\n",
    "\n",
    "pregunta = 'quitando Oviedo, en que estacion se suben más viajeros'\n",
    "\n",
    "\n",
    "tokenizador = AutoTokenizer.from_pretrained(modelo)\n",
    "\n",
    "\n",
    "modelo_tqa = AutoModelForSeq2SeqLM.from_pretrained(modelo)\n",
    "\n",
    "\n",
    "vector = tokenizador(table=tabla_estacion, query=pregunta, return_tensors='pt')\n",
    "\n",
    "\n",
    "resultado = modelo_tqa.generate(**vector)\n",
    "\n",
    "tokenizador.batch_decode(resultado, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66879514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consulta(pregunta):\n",
    "    \n",
    "    \n",
    "    tabla = pd.read_csv('../../../files/asturias_viajeros_por_franja_horaria.csv', sep=';')\n",
    "\n",
    "\n",
    "    tabla_estacion = tabla.groupby('NOMBRE_ESTACION').agg({'VIAJEROS_SUBIDOS': 'sum',\n",
    "                                                           'VIAJEROS_BAJADOS': 'sum'}).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "    tokenizador = AutoTokenizer.from_pretrained(modelo)\n",
    "\n",
    "\n",
    "    modelo_tqa = AutoModelForSeq2SeqLM.from_pretrained(modelo)\n",
    "\n",
    "\n",
    "    vector = tokenizador(table=tabla_estacion, query=pregunta, return_tensors='pt')\n",
    "\n",
    "\n",
    "    resultado = modelo_tqa.generate(**vector)\n",
    "\n",
    "    return tokenizador.batch_decode(resultado, skip_special_tokens=True)[0]\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "924b0d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ia/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' llamaquique'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consulta('quitando Oviedo, en que estacion se suben más viajeros')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6101f5db",
   "metadata": {},
   "source": [
    "## 5 - Otro modelo TQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea5bb74",
   "metadata": {},
   "source": [
    "[TAPAS](https://huggingface.co/google/tapas-large-finetuned-wtq) es un modelo de Google entranado también con el conjunto de datos WikiTableQuestions. Pesa unos 1.35Gb. Usaremos directamente el pipeline para probarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3ec59a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ia/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(task=tarea, model=modelo)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "232a00f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1896</td>\n",
       "      <td>athens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1900</td>\n",
       "      <td>paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1904</td>\n",
       "      <td>st. louis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004</td>\n",
       "      <td>athens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008</td>\n",
       "      <td>beijing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2012</td>\n",
       "      <td>london</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year       city\n",
       "0  1896     athens\n",
       "1  1900      paris\n",
       "2  1904  st. louis\n",
       "3  2004     athens\n",
       "4  2008    beijing\n",
       "5  2012     london"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# usaremos una tabla pequeña para probarlo\n",
    "\n",
    "data = {'year': [1896, 1900, 1904, 2004, 2008, 2012],\n",
    "        'city': ['athens', 'paris', 'st. louis', 'athens', 'beijing', 'london']}\n",
    "\n",
    "\n",
    "\n",
    "tabla = pd.DataFrame.from_dict(data)\n",
    "\n",
    "\n",
    "tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1f130c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "pregunta = 'In which year did beijing host the Olympic Games'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "72d389c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2008"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = pipe(query=pregunta, table=tabla)['answer'].strip()\n",
    "\n",
    "int(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0578dfe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f10683d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "599px",
    "left": "131px",
    "top": "111.141px",
    "width": "263.273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
