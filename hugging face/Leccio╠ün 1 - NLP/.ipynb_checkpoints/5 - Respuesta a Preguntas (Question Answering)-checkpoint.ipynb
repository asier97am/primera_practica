{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b77980ee",
   "metadata": {},
   "source": [
    "# 5 - Respuesta a Preguntas (Question Answering)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/qa.webp\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d91a2",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---Modelos-de-respuesta-a-preguntas-(QA)\" data-toc-modified-id=\"1---Modelos-de-respuesta-a-preguntas-(QA)-1\">1 - Modelos de respuesta a preguntas (QA)</a></span></li><li><span><a href=\"#2---Pipeline-de-Transformers-para-QA\" data-toc-modified-id=\"2---Pipeline-de-Transformers-para-QA-2\">2 - Pipeline de Transformers para QA</a></span></li><li><span><a href=\"#3---Usando-el-modelo-QA\" data-toc-modified-id=\"3---Usando-el-modelo-QA-3\">3 - Usando el modelo QA</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1-Tokenizador\" data-toc-modified-id=\"3.1-Tokenizador-3.1\">3.1 Tokenizador</a></span></li><li><span><a href=\"#3.2---Modelo-QA\" data-toc-modified-id=\"3.2---Modelo-QA-3.2\">3.2 - Modelo QA</a></span></li><li><span><a href=\"#3.3-Resumen-funcional\" data-toc-modified-id=\"3.3-Resumen-funcional-3.3\">3.3 Resumen funcional</a></span></li></ul></li><li><span><a href=\"#4---Ejemplo-con-más-contexto\" data-toc-modified-id=\"4---Ejemplo-con-más-contexto-4\">4 - Ejemplo con más contexto</a></span></li><li><span><a href=\"#5---Otro-modelo-QA\" data-toc-modified-id=\"5---Otro-modelo-QA-5\">5 - Otro modelo QA</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2427a201",
   "metadata": {},
   "source": [
    "## 1 - Modelos de respuesta a preguntas (QA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345a9d88",
   "metadata": {},
   "source": [
    "El QA (Question Answering) es un campo del NLP que se enfoca en la construcción de sistemas capaces de responder preguntas formuladas en lenguaje natural. Estos sistemas buscan proporcionar respuestas precisas y directas a preguntas basadas en una variedad de fuentes de datos, incluyendo textos escritos, bases de datos, internet, etc...\n",
    "\n",
    "Tipos de QA:\n",
    "\n",
    "1. **QA Basado en Retriever/Extractor**: \n",
    "\n",
    "Este tipo de sistema primero recupera un conjunto de documentos o pasajes que son relevantes para la pregunta y luego extrae la respuesta de ese conjunto. Ejemplos comunes incluyen respuestas que son fragmentos exactos de texto extraídos de un documento fuente.\n",
    "\n",
    "2. **QA Generativo**: \n",
    "\n",
    "A diferencia de simplemente extraer respuestas, los sistemas generativos pueden componer respuestas más fluidas y en formato natural. Utilizan técnicas de comprensión del lenguaje y generación de texto para formular respuestas que no necesariamente aparecen textualmente en las fuentes de datos.\n",
    "\n",
    "3. **QA Basado en Conocimiento**:\n",
    "\n",
    "Estos sistemas utilizan bases de datos estructuradas o conocimientos codificados para responder preguntas. Son útiles en dominios específicos donde las respuestas requieren razonamiento basado en hechos conocidos o datos relacionados.\n",
    "\n",
    "4. **QA Basado en Contexto**:\n",
    "\n",
    "Algunos sistemas de QA pueden tener en cuenta el contexto adicional o el historial de la conversación para entender y responder preguntas de manera más efectiva. Esto es común en asistentes virtuales y chatbots.\n",
    "\n",
    "\n",
    "Tecnologías utilizadas en QA:\n",
    "\n",
    "+ Modelos Transformers: Tecnologías como BERT (Bidirectional Encoder Representations from Transformers) y GPT (Generative Pretrained Transformer) han revolucionado el campo de QA al proporcionar una profunda comprensión del contexto y la habilidad de generar respuestas precisas.\n",
    "\n",
    "+ Búsqueda y Extracción de Información: Algoritmos avanzados de recuperación de información para buscar rápidamente grandes cantidades de datos y encontrar los fragmentos más relevantes.\n",
    "\n",
    "+ Redes Neuronales y Deep Learning: Redes neuronales especializadas que pueden entender y procesar el lenguaje natural para interpretar preguntas y generar respuestas adecuadas.\n",
    "\n",
    "\n",
    "Aplicaciones:\n",
    "\n",
    "+ Asistentes Virtuales: Como Siri, Alexa y Google Assistant, que utilizan QA para responder preguntas de los usuarios.\n",
    "\n",
    "+ Soporte al Cliente: Automatización de respuestas a preguntas frecuentes de clientes en sitios web y aplicaciones.\n",
    "\n",
    "+ Educación y e-Learning: Sistemas de tutoría virtual que proporcionan respuestas y explicaciones a estudiantes.\n",
    "\n",
    "+ Análisis de Datos y BI: Herramientas que permiten a los usuarios hacer preguntas sobre datos empresariales y recibir respuestas en forma de análisis o visualizaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601e9b06",
   "metadata": {},
   "source": [
    "En el hub de [modelos](https://huggingface.co/models?sort=trending) de Hugging Face buscaremos [modelos QA en español](https://huggingface.co/models?pipeline_tag=question-answering&sort=trending&search=spanish). Existen modelos en varios lenguajes, aunque la mayoría de ellos están en inglés. Vamos a probar algunos modelos en castellano y luego en inglés.\n",
    "\n",
    "El primer modelo que probaremos es `mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es`, aquí el [link](https://huggingface.co/mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es). Este modelo está basado en [BETO](https://huggingface.co/ignacio-ave/beto-sentiment-analysis-spanish), un modelo de análisis de sentimiento en castellano basado en BERT, y ajustado con el dataset [SQuAD-es-v2.0](https://github.com/ccasimiro88/TranslateAlignRetrieve) para problemas de QA. Veamos como funciona."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae561d8",
   "metadata": {},
   "source": [
    "## 2 - Pipeline de Transformers para QA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5801993e",
   "metadata": {},
   "source": [
    "Ya hemos visto que usar el pipeline es la manera más fácil de usar los modelos de Hugging Face. Tan solo tenemos que definir la tarea y el modelo y podemos usar nuestro modelo. Vamos a probar el pipeline con este modelo de respuesta a preguntas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "261ac779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging para quitar warnings de actualización de pesos del modelo\n",
    "\n",
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "704adea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos desde la librería transformers el pipeline\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "132d08d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos la tarea y el modelo\n",
    "\n",
    "tarea = 'question-answering'  \n",
    "\n",
    "modelo = 'mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42a3d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iniciamos el modelo QA, los modelos se descargan en local, en este caso son unos 440Mb\n",
    "\n",
    "qa_pipe = pipeline(task=tarea, model=modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e61555",
   "metadata": {},
   "source": [
    "Para usar los modelos de QA, le tenemos que dar al modelo tanto el contexto como la pregunta. Esto lo podemos hacer de dos maneras. Podemos crear un diccionario cuyas keys sean `question` y `context`, donde los valores sean las respectivas strings. También podemos darle cada una de las strings como argumentos de entrada por separado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3591f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos contexto y pregunta\n",
    "\n",
    "contexto = 'El español es el segundo idioma más hablado del mundo con más de 442 millones de hablantes'\n",
    "\n",
    "pregunta = '¿Cuántas personas hablan español?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86ad61db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos el diccionario con las keys necesarias: context y question\n",
    "\n",
    "prompt = {'context': contexto,\n",
    "          'question': pregunta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf878ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.252778023481369, 'start': 65, 'end': 77, 'answer': '442 millones'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llamada al pipeline con el diccionario\n",
    "\n",
    "qa_pipe(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35a08b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.252778023481369, 'start': 65, 'end': 77, 'answer': '442 millones'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llamada al pipeline con argumentos de entrada\n",
    "\n",
    "qa_pipe(context=contexto, question=pregunta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019c79bf",
   "metadata": {},
   "source": [
    "Se puede ver que ambas salidas son idénticas, así que podemos usar el pipeline como más nos guste, ya sea contruyendo el diccionario primero o pasándole directamente los argumentos de entrada a la función.\n",
    "\n",
    "La respuesta del pipeline es un diccionario con las siguietes keys:\n",
    "\n",
    "+ score: Probabilidad asociada a la respuesta.\n",
    "+ start: Indice del caracter de inicio de la respuesta, en la versión tokenizada de la entrada.\n",
    "+ end: Indice del caracter final de la respuesta, en la versión tokenizada de la entrada.\n",
    "+ answer: Respuesta a la pregunta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4b428e",
   "metadata": {},
   "source": [
    "## 3 - Usando el modelo QA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284b09a9",
   "metadata": {},
   "source": [
    "Vamos a usar ahora el tokenizador y el modelo QA directamente. Ya vimos que el uso del modelo de esta manera es un poco más complicado, pero nos permite crear una salida con la forma que nosotros queramos. Veamos se usa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aead305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos los objetos tokenizador y modelo de QA\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4297cd0e",
   "metadata": {},
   "source": [
    "### 3.1 Tokenizador\n",
    "\n",
    "Convertimos la pregunta y contexto a un vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1fa6c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# con este objeto vectorizamos las palabras\n",
    "\n",
    "tokenizador = AutoTokenizer.from_pretrained(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ed96b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es', vocab_size=31002, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# descripcion del objeto\n",
    "\n",
    "tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d23fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creación del vector\n",
    "\n",
    "vector = tokenizador(pregunta, contexto, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a634449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    4,  1067, 14578,  1858, 10219, 30062,  1101,  1064,     5,  1040,\n",
       "         30062,  1101,  1058,  1040,  2740,  8134,  2437,  7856,  1072,  1863,\n",
       "          1051,  2437,  1008,  4974, 30973,  2439,  1008, 10219,  1205,     5]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a143241a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '¿',\n",
       " 'cuantas',\n",
       " 'personas',\n",
       " 'hablan',\n",
       " 'espan',\n",
       " '##ol',\n",
       " '?',\n",
       " '[SEP]',\n",
       " 'el',\n",
       " 'espan',\n",
       " '##ol',\n",
       " 'es',\n",
       " 'el',\n",
       " 'segundo',\n",
       " 'idioma',\n",
       " 'mas',\n",
       " 'hablado',\n",
       " 'del',\n",
       " 'mundo',\n",
       " 'con',\n",
       " 'mas',\n",
       " 'de',\n",
       " '44',\n",
       " '##2',\n",
       " 'millones',\n",
       " 'de',\n",
       " 'hablan',\n",
       " '##tes',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens generados\n",
    "\n",
    "tokens = vector.tokens()\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c23a30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nº de tokens\n",
    "\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd2c538",
   "metadata": {},
   "source": [
    "### 3.2 - Modelo QA\n",
    "\n",
    "Usamos el modelo QA con el vector que acabamos de sacar del tokenizador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14799ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicializacion del modelo QA\n",
    "\n",
    "modelo_qa = AutoModelForQuestionAnswering.from_pretrained(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b7615d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# descripcion del objeto\n",
    "\n",
    "modelo_qa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdda266",
   "metadata": {},
   "source": [
    "El modelo `BertForQuestionAnswering` es una configuración especializada de BERT (Bidirectional Encoder Representations from Transformers) diseñada específicamente para tareas de QA. Este modelo es capaz de predecir las posiciones de inicio y fin de la respuesta en un texto dado, basado en una pregunta formulada. Vamos a desglosar los componentes clave de esta configuración del modelo para entender mejor cómo funciona:\n",
    "\n",
    "1. BertModel: La base del modelo que proporciona las principales capacidades de procesamiento del lenguaje.\n",
    "    + BertEmbeddings: Se encarga de convertir los tokens de entrada en vectores de características, que son representaciones densas y entrenables. Incluye:\n",
    "        + word_embeddings: Mapea cada token a un vector de 768 dimensiones.\n",
    "        + position_embeddings: Añade información de la posición de cada token dentro de la secuencia para mantener la noción del orden de las palabras.\n",
    "        + token_type_embeddings: Diferencia entre varios tipos de tokens, por ejemplo, entre la pregunta y el contexto en QA.\n",
    "        + LayerNorm y Dropout: Estos son mecanismos para normalizar y prevenir el sobreajuste durante el entrenamiento.\n",
    "\n",
    "\n",
    "2. BertEncoder: Contiene múltiples capas de atención y transformaciones lineales para procesar las embeddings. Cada BertLayer realiza lo siguiente:\n",
    "    + BertAttention: Maneja la atención auto-dirigida que permite al modelo prestar atención a diferentes partes de la entrada basándose en la pregunta.\n",
    "        + BertSelfAttention: Utiliza transformaciones lineales (query, key, value) para generar las puntuaciones de atención y combina los resultados de acuerdo con estas puntuaciones.\n",
    "    + BertSelfOutput y BertOutput: Estas subunidades procesan la salida del mecanismo de atención y luego la combinan con la entrada original de la capa (conexión residual), seguida de normalización y dropout.\n",
    "    + BertIntermediate y BertOutput: Amplían y luego contraen las dimensiones de las características, respectivamente, y aplican la función de activación GELU en el proceso.\n",
    "\n",
    "\n",
    "3. qa_outputs: Una capa lineal específica que toma la salida final de cada token desde el último encoder de BERT y la proyecta a un espacio de 2 dimensiones. Estas dos dimensiones representan las puntuaciones de cada token siendo el inicio y el fin de la respuesta. Por ejemplo, para un par de tokens en una entrada, esta capa determinará qué tan probable es que cada token sea el inicio o el fin de la respuesta a la pregunta formulada.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Funcionamiento del modelo QA:\n",
    "\n",
    "+ Preparación: El texto de entrada se divide en tokens, y se añaden tokens especiales de BERT para marcar el inicio y el fin, así como para separar la pregunta del contexto.\n",
    "\n",
    "+ Procesamiento: El texto tokenizado pasa a través del modelo BERT, donde se aprende a prestar atención a las partes relevantes del texto relacionadas con la pregunta.\n",
    "\n",
    "+ Predicción de Respuesta: La última capa, qa_outputs, asigna puntuaciones a cada token indicando la probabilidad de que sean el inicio o el fin de la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d82bdf40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[  2.8641,  -7.7706,  -7.9311,  -7.2602,  -6.4896,  -7.1135, -10.0911,\n",
       "          -7.0915,  -3.6097,  -0.8238,  -0.8169,  -7.1940,  -3.6554,  -3.0292,\n",
       "          -1.4876,  -3.0732,  -3.2169,  -3.3532,  -3.2956,  -2.6370,   1.2541,\n",
       "           5.7786,  -0.6602,   6.2068,  -1.4530,   1.7676,  -0.7707,   0.3704,\n",
       "          -3.1695,  -3.6097]], grad_fn=<CloneBackward0>), end_logits=tensor([[ 2.8809, -7.5637, -6.8085, -5.1097, -6.2580, -8.3626, -5.9074, -6.4824,\n",
       "         -3.3196, -2.3541, -5.4132, -2.3080, -4.3779, -4.9579, -3.5450, -3.0491,\n",
       "         -1.4967, -1.5142, -3.5586, -1.3229, -0.6540, -1.5289,  0.0847, -0.1411,\n",
       "          2.2514,  5.3837,  5.2996, -1.5660,  3.9766, -3.3192]],\n",
       "       grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resultado del modelo QA al darle el vector\n",
    "\n",
    "resultado = modelo_qa(**vector)\n",
    "\n",
    "resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ac9d6d",
   "metadata": {},
   "source": [
    "El resultado del modelo tiene cinco atributos:\n",
    "\n",
    "+ loss: Representa la pérdida calculada durante el entrenamiento del modelo. La pérdida es un indicador de rendimiento del modelo, donde un valor más bajo indica un mejor rendimiento. loss=None nos dice que el modelo se usa en modo de inferencia, donde no se calcula la pérdida.\n",
    "\n",
    "+ start_logits: Tensor que contiene las puntuaciones para cada posición del texto como posible inicio de la respuesta.\n",
    "\n",
    "+ end_logits: Tensor que contiene las puntuaciones para cada posición del texto como posible final de la respuesta.\n",
    "\n",
    "+ hidden_states: Si se incluyera, representaría los estados ocultos de la salida de cada capa del modelo. Se suelen usar para análisis más detallados y para tareas avanzadas de procesamiento del lenguaje. Son las representaciones intermedias de la entrada que se generan dentro de las capas del modelo durante el proceso de transformación de los datos de entrada a la salida final.\n",
    "\n",
    "+ attentions: Si se incluyeran, proporcionarían las matrices de atención de cada capa del modelo, que muestran cómo se enfoca el modelo en diferentes partes del texto para hacer sus predicciones. Son útiles para visualizar y entender cómo el modelo procesa y relaciona diferentes partes del texto.\n",
    "\n",
    "Ahora, para conocer la respuesta, debemos calcular al argumento máximo tanto de start_logits como de end_logits y saber así que posiciones dentro de los tokens ocupa nuestra respuesta, porque eso es lo que hace el modelo, buscar dentro del contexto que parte es la respuesta y nos devuelve su posición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1d74198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(23)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# posición inicial de la respuesta\n",
    "\n",
    "indice_inicio = resultado.start_logits.argmax()\n",
    "\n",
    "indice_inicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4754952a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# posición final de la respuesta\n",
    "\n",
    "indice_final = resultado.end_logits.argmax()\n",
    "\n",
    "indice_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c68341",
   "metadata": {},
   "source": [
    "Podríamos sacar directamente la respuesta desde los tokens, simplemente extrayendo desde la posición 23 a la 26. Sin embargo, recordemos que los tokens tiene algunas entidades separadas. Veamos como sería el proceso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9456aad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '¿', 'cuantas', 'personas', 'hablan', 'espan', '##ol', '?', '[SEP]', 'el', 'espan', '##ol', 'es', 'el', 'segundo', 'idioma', 'mas', 'hablado', 'del', 'mundo', 'con', 'mas', 'de', '44', '##2', 'millones', 'de', 'hablan', '##tes', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20620472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['44', '##2', 'millones']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# respuesta del modelo\n",
    "\n",
    "tokens[23:26]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a523eae",
   "metadata": {},
   "source": [
    "Esta respuesta necesita una transformación para estar correctamante escrita. Hay otra manera de obtener la respuesta del modelo. El vector que sale del tokenizador puede ser convertido de vuelta en texto, sería el texto completo que entra en el modelo. Simplemente filtrado dicho vector y dándoselo al tokenizador de vuelta tendríamos perfectamente escrita nuestra respuesta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ed0ad1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    4,  1067, 14578,  1858, 10219, 30062,  1101,  1064,     5,  1040,\n",
       "        30062,  1101,  1058,  1040,  2740,  8134,  2437,  7856,  1072,  1863,\n",
       "         1051,  2437,  1008,  4974, 30973,  2439,  1008, 10219,  1205,     5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor que sale del tokenizador y entra al modelo, representa todo el texto\n",
    "\n",
    "vector.input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43957b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] ¿ cuantas personas hablan espanol? [SEP] el espanol es el segundo idioma mas hablado del mundo con mas de 442 millones de hablantes [SEP]'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# de vector a texto con el tokenizador\n",
    "\n",
    "tokenizador.decode(vector.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d0ffa1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4974, 30973,  2439])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extraemos el pedazo del tensor que corresponde con la respuesta\n",
    "\n",
    "tensor_respuesta = vector.input_ids[0, indice_inicio : indice_final + 1]\n",
    "\n",
    "tensor_respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3e330a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'442 millones'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# respuesta en formato texto, skip_special_tokens=True elimina los tokens especiales si los hubiera\n",
    "\n",
    "tokenizador.decode(tensor_respuesta, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f05eb4",
   "metadata": {},
   "source": [
    "Podemos ver ahora la pérdida (loss) que da el modelo, la métrica de rendimiento del modelo. Para ello importamos `torch` para crear los tensores con las dimensiones adecuadas y a continuación perdirle al modelo QA la pérdida que tiene para la respuesta. Esta pérdida será tanto mejor cuanto más cercana a cero sea la métrica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fbb5ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos torch\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c2e0cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# respuesta del modelo para ese pedazo del texto (23-25)\n",
    "\n",
    "respuesta = modelo_qa(**vector, \n",
    "                      start_positions=torch.tensor([23]), \n",
    "                      end_positions=torch.tensor([25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8cda79ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6877423524856567"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pérdida\n",
    "\n",
    "respuesta.loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396d95af",
   "metadata": {},
   "source": [
    "### 3.3 Resumen funcional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87adcee",
   "metadata": {},
   "source": [
    "Vamos a poner todo el código junto en una sola función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3635a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# librerías\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "\n",
    "\n",
    "def qa(pregunta:str, contexto:str, modelo:str) -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    Función para modelo QA.\n",
    "    \n",
    "    Params:\n",
    "    + pregunta: string. Pregunta que queremos hacer al modelo.\n",
    "    + contexto: string. Texto al cual preguntamos\n",
    "    + modelo: string. Nombre del modelo que vamos a usar, lo encontraremos el en hub de Hugging Face.\n",
    "    \n",
    "    Return:\n",
    "    Esta función devuelve la respuesta del modelo en formato string.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    # inicio tokenizador\n",
    "    tokenizador = AutoTokenizer.from_pretrained(modelo)\n",
    "    \n",
    "    \n",
    "    # creación del vector\n",
    "    vector = tokenizador(pregunta, contexto, return_tensors='pt')\n",
    "    \n",
    "    \n",
    "    # inicializacion del modelo QA\n",
    "    modelo_qa = AutoModelForQuestionAnswering.from_pretrained(modelo)\n",
    "    \n",
    "    \n",
    "    # resultado del modelo QA al darle el vector\n",
    "    resultado = modelo_qa(**vector)\n",
    "    \n",
    "    \n",
    "    # posición inicial de la respuesta\n",
    "    indice_inicio = resultado.start_logits.argmax()\n",
    "    \n",
    "    \n",
    "    # posición final de la respuesta\n",
    "    indice_final = resultado.end_logits.argmax()\n",
    "    \n",
    "    \n",
    "    # respuesta formato tensor\n",
    "    tensor_respuesta = vector.input_ids[0, indice_inicio : indice_final + 1]\n",
    "    \n",
    "    \n",
    "    # respuesta en formato texto\n",
    "    respuesta = tokenizador.decode(tensor_respuesta, skip_special_tokens=True)\n",
    "    \n",
    "    return respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f22c34a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos pregunta, contexto y modelo\n",
    "\n",
    "pregunta = '¿Cuántas personas hablan español?'\n",
    "\n",
    "\n",
    "contexto = 'El español es el segundo idioma más hablado del mundo con más de 442 millones de hablantes'\n",
    "\n",
    "\n",
    "modelo = 'mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d79bea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'442 millones'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(pregunta, contexto, modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a93e7e7",
   "metadata": {},
   "source": [
    "## 4 - Ejemplo con más contexto\n",
    "\n",
    "Vamos a ver un ejemplo dándole al modelo un contexto con más de una frase. Copiamos un texto del [Canal de Suez](https://es.wikipedia.org/wiki/Canal_de_Suez) del artículo en Wikipedia y realizamos varias preguntas sobre ese texto. Usamos directamente el pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b7257fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexto = '''\n",
    "\n",
    "El canal de Suez (en árabe, قناة السويس qanat al-Suways) es un canal artificial navegable situado en Egipto, \n",
    "que une el mar Mediterráneo con el golfo de Suez, en el mar Rojo, a través del istmo de Suez. \n",
    "El canal convirtió a la región del Sinaí en una nueva península, \n",
    "constituyendo la frontera entre los continentes de África y Asia. \n",
    "Su longitud es de 193 km entre Puerto Saíd, en la ribera mediterránea, y Suez, en la costa del mar Rojo.\n",
    "\n",
    "Fue impulsado entre 1859 y 1869 por Fernando de Lesseps bajo la dirección del ingeniero \n",
    "Louis Maurice Adolphe Linant de Bellefonds. Pertenece a Egipto desde la nacionalización de la \n",
    "compañía franco-británica Suez Canal Company en 1956 y la posterior guerra del Sinaí en 1957.\n",
    "El canal es de vital importancia para el abastecimiento europeo de petróleo y el comercio mundial en general, \n",
    "puesto que permite la comunicación entre Europa y el sur de Asia sin rodear el continente africano \n",
    "por el cabo de Buena Esperanza.\n",
    "\n",
    "El canal se inauguró oficialmente el 17 de noviembre de 1869. \n",
    "Ofrece a los buques una ruta directa entre el Atlántico Norte y el Índico Norte \n",
    "a través del mar Mediterráneo y el mar Rojo, evitando el Atlántico Sur y el Índico Sur y \n",
    "reduciendo la distancia del viaje desde el mar Arábigo a Londres en aproximadamente \n",
    "8900 kilómetros (5500 mi), o de 10 días a 20 nudos (37 km/h; 23 mph) \n",
    "a 8 días a 24 nudos (44 km/h; 28 mph). El canal se extiende desde el extremo septentrional de \n",
    "Puerto Saíd hasta el extremo meridional de Port Tewfik, en la ciudad de Suez. \n",
    "Su longitud es de 193,30 kilómetros, incluidos los canales de acceso norte y sur. \n",
    "En 2020, más de 18 500 barcos atravesaron el canal (una media de 51,5 al día).\n",
    "\n",
    "El canal original presentaba una vía de agua de un solo carril con lugares de paso en la \n",
    "Circunvalación de Ballah y el Gran Lago Amargo. Según los planes de Alois Negrelli, \n",
    "no contenía sistemas de esclusas, y el agua del mar fluía libremente por él. \n",
    "En general, el agua del canal al norte de los Lagos Amargos fluye hacia el norte en invierno y \n",
    "hacia el sur en verano. Al sur de los lagos, la corriente cambia con la marea en Suez.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02c5f84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'17 de noviembre de 1869'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta = '¿Cuándo se inauguró el canal de Suez?'\n",
    "\n",
    "qa_pipe(question=pregunta, context=contexto)['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5fb0716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'193 km'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta = '¿Cuánto mide el canal?'\n",
    "\n",
    "qa_pipe(question=pregunta, context=contexto)['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9d8c36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fernando de Lesseps'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta = '¿Quién construyó el canal?'\n",
    "\n",
    "qa_pipe(question=pregunta, context=contexto)['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7bbd03a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Egipto'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta = '¿En qué país está el canal?'\n",
    "\n",
    "qa_pipe(question=pregunta, context=contexto)['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8bf8de41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'18 500'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta = '¿Cuántos barcos atravesaron el canal en 2020 el canal?'\n",
    "\n",
    "qa_pipe(question=pregunta, context=contexto)['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2bfd810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Puerto Saíd'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta = '¿Dónde empieza el canal en el mediterráneo?'\n",
    "\n",
    "qa_pipe(question=pregunta, context=contexto)['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9659ee52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'golfo de Suez'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta = '¿Dónde acaba el canal en el mar rojo?'\n",
    "\n",
    "qa_pipe(question=pregunta, context=contexto)['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f587883d",
   "metadata": {},
   "source": [
    "## 5 - Otro modelo QA\n",
    "\n",
    "Ahora vamos a probar otro modele, esta vez en inglés. El modelo es un [RoBERTa](https://huggingface.co/deepset/roberta-base-squad2) diseñado para QA, tiene un peso aproximado de 500Mb. De nuevo usaremos un artículo de Wikipedia como contexto, está vez de la compañía [Apple Inc](https://en.wikipedia.org/wiki/Apple_Inc.). Usaremos directamente el pipeline igual que hicimos en el ejemplo anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fafce546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos la tarea y el modelo\n",
    "\n",
    "tarea = 'question-answering'  \n",
    "\n",
    "modelo = 'deepset/roberta-base-squad2'   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91db4481",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pipe = pipeline(task=tarea, model=modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9ce8b861",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexto = '''\n",
    "\n",
    "Apple Inc. (formerly Apple Computer, Inc.) is an American multinational corporation and technology \n",
    "company headquartered in Cupertino, California, in Silicon Valley. It designs, develops, \n",
    "and sells consumer electronics, computer software, and online services. \n",
    "Devices include the iPhone, iPad, Mac, Apple Watch, Vision Pro, and Apple TV; \n",
    "operating systems include iOS, iPadOS, and macOS; and software applications and services include iTunes, \n",
    "iCloud, Apple Music, and Apple TV+.\n",
    "\n",
    "For most of 2011 to 2024, Apple became the world's largest company by market capitalization until \n",
    "Microsoft assumed the position in January 2024. In 2022, Apple was the largest technology company by revenue, \n",
    "with US$394.3 billion. As of 2023, Apple was the fourth-largest personal computer vendor by unit sales,\n",
    "the largest manufacturing company by revenue, and the largest vendor of mobile phones in the world.\n",
    "It is one of the Big Five American information technology companies, alongside Alphabet \n",
    "(the parent company of Google), Amazon, Meta (the parent company of Facebook), and Microsoft.\n",
    "\n",
    "Apple was founded as Apple Computer Company on April 1, 1976, to produce and market Steve Wozniak's \n",
    "Apple I personal computer. The company was incorporated by Wozniak and Steve Jobs in 1977. \n",
    "Its second computer, the Apple II, became a best seller as one of the first mass-produced microcomputers. \n",
    "Apple introduced the Lisa in 1983 and the Macintosh in 1984, as some of the first computers to \n",
    "use a graphical user interface and a mouse. By 1985, the company's internal problems included the high \n",
    "cost of its products and power struggles between executives. That year Jobs left Apple to form NeXT, Inc., \n",
    "and Wozniak withdrew to other ventures. The market for personal computers expanded and evolved throughout \n",
    "the 1990s, and Apple lost considerable market share to the lower-priced Wintel duopoly of the \n",
    "Microsoft Windows operating system on Intel-powered PC clones.\n",
    "\n",
    "In 1997, Apple was weeks away from bankruptcy. To resolve its failed operating system strategy and entice \n",
    "Jobs's return, it bought NeXT. Over the next decade, Jobs guided Apple back to profitability through \n",
    "several tactics including introducing the iMac, iPod, iPhone, and iPad to critical acclaim, launching the \n",
    "\"Think different\" campaign and other memorable advertising campaigns, opening the Apple Store retail chain, \n",
    "and acquiring numerous companies to broaden its product portfolio. Jobs resigned in 2011 for health reasons, \n",
    "and died two months later. He was succeeded as CEO by Tim Cook.\n",
    "\n",
    "Apple has received criticism regarding its contractors' labor practices, its environmental practices, \n",
    "and its business ethics, including anti-competitive practices and materials sourcing. \n",
    "Nevertheless, it has a large following and a high level of brand loyalty. \n",
    "It has been consistently ranked as one of the world's most valuable brands.\n",
    "\n",
    "Apple became the first publicly traded U.S. company to be valued at over $1 trillion in August 2018, \n",
    "then at $2 trillion in August 2020, and at $3 trillion in January 2022. In June 2023, \n",
    "it was valued at just over $3 trillion.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8e729f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wozniak and Steve Jobs'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta = \"Who are Apple's founders?\"\n",
    "\n",
    "qa_pipe(question=pregunta, context=contexto)['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd39268b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1976'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta = 'When was founded?'\n",
    "\n",
    "qa_pipe(question=pregunta, context=contexto)['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ab467f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iMac, iPod, iPhone, and iPad'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta = 'What are the devices sold by Apple?'\n",
    "\n",
    "qa_pipe(question=pregunta, context=contexto)['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fffbd7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'high level of brand loyalty'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta = 'What is the benefit?'\n",
    "\n",
    "qa_pipe(question=pregunta, context=contexto)['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f8dee69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'over $1 trillion'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta = \"What is the company's value in 2018?\"\n",
    "\n",
    "qa_pipe(question=pregunta, context=contexto)['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cb284e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$3 trillion'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta = \"What is the company's value in 2023?\"\n",
    "\n",
    "qa_pipe(question=pregunta, context=contexto)['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "599px",
    "left": "89px",
    "top": "111.141px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
