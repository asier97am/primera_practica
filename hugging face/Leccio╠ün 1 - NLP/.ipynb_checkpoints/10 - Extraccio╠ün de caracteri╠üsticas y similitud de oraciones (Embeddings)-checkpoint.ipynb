{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1369f24b",
   "metadata": {},
   "source": [
    "# 10 - Extracción de características y similitud de oraciones (Embeddings)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/embeddings.webp\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f132b0",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---Modelos-de-extracción-de-características-(Embeddings)\" data-toc-modified-id=\"1---Modelos-de-extracción-de-características-(Embeddings)-1\">1 - Modelos de extracción de características (Embeddings)</a></span></li><li><span><a href=\"#2---Pipeline-de-Transformers-para-Embeddings\" data-toc-modified-id=\"2---Pipeline-de-Transformers-para-Embeddings-2\">2 - Pipeline de Transformers para Embeddings</a></span></li><li><span><a href=\"#3---Tokenizador\" data-toc-modified-id=\"3---Tokenizador-3\">3 - Tokenizador</a></span></li><li><span><a href=\"#4---Transformadores-de-frases\" data-toc-modified-id=\"4---Transformadores-de-frases-4\">4 - Transformadores de frases</a></span></li><li><span><a href=\"#5---Similitud-de-oraciones\" data-toc-modified-id=\"5---Similitud-de-oraciones-5\">5 - Similitud de oraciones</a></span></li><li><span><a href=\"#6---Ejemplo-de-predicción\" data-toc-modified-id=\"6---Ejemplo-de-predicción-6\">6 - Ejemplo de predicción</a></span><ul class=\"toc-item\"><li><span><a href=\"#6.1---Importamos-las-librerías\" data-toc-modified-id=\"6.1---Importamos-las-librerías-6.1\">6.1 - Importamos las librerías</a></span></li><li><span><a href=\"#6.2---Cargamos-los-datos\" data-toc-modified-id=\"6.2---Cargamos-los-datos-6.2\">6.2 - Cargamos los datos</a></span></li><li><span><a href=\"#6.3---Exploración-y-filtrado-de-datos\" data-toc-modified-id=\"6.3---Exploración-y-filtrado-de-datos-6.3\">6.3 - Exploración y filtrado de datos</a></span></li><li><span><a href=\"#6.4---Transformación-de-datos\" data-toc-modified-id=\"6.4---Transformación-de-datos-6.4\">6.4 - Transformación de datos</a></span></li><li><span><a href=\"#6.5---Separación-de-datos\" data-toc-modified-id=\"6.5---Separación-de-datos-6.5\">6.5 - Separación de datos</a></span></li><li><span><a href=\"#6.6---Modelo-y-evaluación\" data-toc-modified-id=\"6.6---Modelo-y-evaluación-6.6\">6.6 - Modelo y evaluación</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16795e2e",
   "metadata": {},
   "source": [
    "## 1 - Modelos de extracción de características (Embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06108334",
   "metadata": {},
   "source": [
    "Los modelos de extracción de características, modelos de embedding o modelos de incrustación son técnicas fundamentales en el campo del NLP, que permiten representar datos como vectores en un espacio de características continuo. Estos modelos capturan la semántica de los datos y facilitan el manejo de relaciones complejas dentro de grandes conjuntos de datos. Algunos de los modelos de embedding más conocidos son:\n",
    "\n",
    "\n",
    "1. **Word2Vec**: Desarrollado por Google, es uno de los modelos de embedding de palabras más populares. Utiliza redes neuronales para aprender representaciones vectoriales de palabras a partir de grandes corpora de texto. Tiene dos arquitecturas principales: CBOW (Continuous Bag of Words) y Skip-gram.\n",
    "\n",
    "2. **GloVe** (Global Vectors for Word Representation): Propuesto por investigadores de Stanford, GloVe es un método que genera embeddings de palabras al factorizar matrices que describen las co-ocurrencias estadísticas de palabras en un corpus. Se diferencia de Word2Vec en que no solo se basa en contextos locales de palabras sino en estadísticas globales del corpus.\n",
    "\n",
    "3. **FastText**: Desarrollado por Facebook Research, este modelo extiende Word2Vec para considerar no solo palabras completas sino también sub-palabras o n-gramas de caracteres. Esto permite que el modelo maneje mejor palabras desconocidas o raras.\n",
    "\n",
    "4. **BERT** (Bidirectional Encoder Representations from Transformers): Introducido por Google AI, BERT usa la arquitectura de Transformer para pre-entrenar embeddings de palabras en contextos bidireccionales. Esto permite capturar un contexto más rico y mejorar el rendimiento en tareas de NLP downstream.\n",
    "\n",
    "5. **ELMo** (Embeddings from Language Models): ELMo utiliza modelos de lenguaje basados en redes neuronales LSTM para aprender embeddings de palabras basados en el contexto de uso. Los embeddings resultantes son dinámicos, ajustándose según el contexto en el que aparecen las palabras.\n",
    "\n",
    "\n",
    "\n",
    "Estos modelos de embedding son la base de los modelos que hemos venido utilizando anteriormente. Los tokenizadores son fundamentalmente extractores de características, pues convierten los textos en vectores, que posteriormente son introducidos en los modelos de transformers. Recordemos que el tokenizador primero convierte el texto en tokens, las piezas mínimas de texto, y más tarde convierte esos tokens a vectores. La dimensión de esos vectores viene determinado por el propio modelo de embedding. \n",
    "\n",
    "Existen modelos que devuelven vectores de 384, 768, 1024, 1536 o incluso 4096 elementos. Generalmente, un embedding de mayor dimensión puede capturar más información y matices sobre los datos. Esto puede resultar en una mejor capacidad para distinguir entre conceptos más sutiles o complejos. Sin embargo, usar embeddings de mayor dimensión incrementa la carga computacional. Esto significa más uso de memoria, tiempos de entrenamiento más largos, y puede requerir hardware más potente. Además, los embeddings de mayor dimensión pueden ser más propensos al overfitting, especialmente si el tamaño del dataset de entrenamiento es pequeño."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035f5a72",
   "metadata": {},
   "source": [
    "## 2 - Pipeline de Transformers para Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103efef4",
   "metadata": {},
   "source": [
    "Veamos algunos de los modelos de embeddings que proporciona Hugging Face. En primer lugar, [gte-small](https://huggingface.co/Supabase/gte-small), un modelo que nos devuelve un vector de 384 elementos para cada token y que tiene un peso de 70Mb. GTE significa General Text Embeddings, y son modelos entrenados por la [Academia DAMO de Alibaba](https://www.alibabagroup.com/en-US/about-alibaba-businesses-1496657217451982848). Se basan principalmente en el marco de trabajo de BERT y actualmente ofrecen tres tamaños diferentes de modelos, incluyendo GTE-large (1024), GTE-base (768) y GTE-small (384). Los modelos GTE están entrenados en un corpus de gran escala de pares de texto relevantes, cubriendo una amplia gama de dominios y escenarios. Esto permite que los modelos GTE se apliquen a diversas tareas de procesamiento de texto, incluyendo la recuperación de información, la similitud textual semántica y la reordenación de textos. Vamos a cargarlo con el pipeline y ver su respuesta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5493cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c7165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05249cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7897ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bc1ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ee9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7063f47",
   "metadata": {},
   "source": [
    "Como vemos, las dimensiones del tensor han cambiado. El `1` se refiere a la secuencia, el `384` es la dimensión del vector por token. El `8` o el `27` se refiere al número de tokens. Este modelo tiene un máximo de 512 tokens, por lo que la dimensión máxima del tensor es `[1, 512, 384]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f5cea9",
   "metadata": {},
   "source": [
    "Como dijimos, existen modelos de embedding que nos devuelven vectores con más dimensiones. Facebook tiene un modelo basado en BART, llamado [bart-base](https://huggingface.co/facebook/bart-base). Este modelo devuelve un vector de 768 elementos y pesa aproximadamente 560Mb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76642897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37964c72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa09f4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3bf119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70bd90c2",
   "metadata": {},
   "source": [
    "Vemos que el número de tokens de este modelo también es distinta, de 8 se pasó a 9 y de 27 se pasó a 34, además de tener los 768 elementos por token frente a los 384 del modelo anterior. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f89050a",
   "metadata": {},
   "source": [
    "## 3 - Tokenizador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669bc80e",
   "metadata": {},
   "source": [
    "Hemos visto en los capítulos anteriores, que este proceso de embedding es el que realizan los tokenizadores. Dado un texto, el tokenizador extrae los tokens y luego vectoriza. Veamos este último modelo, bart-base, usado a través del AutoTokenizer de transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af80f528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbbfb55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592747d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d36d9b0",
   "metadata": {},
   "source": [
    "Ya hemos visto este tokenizador, BartTokenizerFast. Es una implementación específica de un tokenizador para el modelo BART, optimizada que aprovecha la biblioteca Rust tokenizers para ofrecer un rendimiento más rápido en comparación con su versión en Python. Ofrece un mejor rendimiento sin sacrificar la calidad o la precisión de la tokenización, lo cual es crucial para la eficacia de los modelos de lenguaje en tareas de NLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd7fce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14301777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad297edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd75a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3350ca97",
   "metadata": {},
   "source": [
    "Las salidas del tokenizador tiene dimensiones distintas de las que devuelve el pipeline, solo el 1 de la secuencia y el número de tokens. Así pues el tokenizador saca un vector distinto, pero que básicamente significa los mismo. Veamos cuales son los tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6e2535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505a49e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36089116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "594a74d2",
   "metadata": {},
   "source": [
    "La dimensión del vector que sale del tokenizador coincide con el número de tokens que genera, es decir, tenemos un número para cada token generado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e6f9a",
   "metadata": {},
   "source": [
    "## 4 - Transformadores de frases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323d1103",
   "metadata": {},
   "source": [
    "Ahora vamos a transformar frases completas a un vector. Usaremos el modelo [all-roberta-large-v1](https://huggingface.co/sentence-transformers/all-roberta-large-v1) un modelo de sentence-transformers que mapea oraciones y párrafos a un espacio vectorial de 1024 dimensiones y puede ser utilizado para tareas como el agrupamiento o la búsqueda semántica. Tiene un peso aproximado de 1.5Gb.\n",
    "\n",
    "Usamos el objeto `SentenceTransformer`, que se utiliza para convertir oraciones y párrafos en vectores densos de alta dimensión, lo que permite comparar y medir la similitud semántica entre ellos. Vector denso se refiere a que el vector de salida no tiene ningún elemento 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad510a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667c3865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212be905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ab330d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94c61b8e",
   "metadata": {},
   "source": [
    "**Componentes del SentenceTransformer**\n",
    "\n",
    "1. **Transformer**: Este es el primer módulo en la cadena del SentenceTransformer y utiliza un modelo preentrenado de Transformer, específicamente RobertaModel. Aquí es donde se realiza la primera etapa de procesamiento del texto.\n",
    "\n",
    "    + max_seq_length: Define la longitud máxima de la secuencia de entrada que el modelo puede manejar. En este caso, está configurado para 256 tokens.\n",
    "    + do_lower_case: Indica si el texto de entrada debe convertirse a minúsculas antes de procesarlo. Aquí está configurado como False, lo que significa que el texto se procesará tal como se recibe.\n",
    "\n",
    "\n",
    "2. **Pooling**: Este módulo maneja la forma en que se combinan o resumen las representaciones de tokens individuales obtenidas del modelo RobertaModel para formar un único vector de embedding para toda la oración.\n",
    "\n",
    "    + word_embedding_dimension: La dimensión de los embeddings de cada palabra, en este caso, 1024.\n",
    "    + pooling_mode_cls_token: Si se usa, toma el token CLS, usado a menudo como un resumen de toda la entrada en modelos como BERT y RoBERTa, como el embedding de la oración. Aquí está desactivado.\n",
    "    + pooling_mode_mean_tokens: Calcula la media de todos los embeddings de tokens para obtener el embedding de la oración. Está activado, lo que significa que esta es la técnica de pooling utilizada.\n",
    "    + pooling_mode_max_tokens, pooling_mode_mean_sqrt_len_tokens, pooling_mode_weightedmean_tokens, pooling_mode_lasttoken: Otras técnicas de pooling que están disponibles pero no se utilizan en esta configuración.\n",
    "    + include_prompt: Específico para si se incluye o no algún prompt o guía en el cálculo del embedding.\n",
    "\n",
    "\n",
    "3. **Normalize**: Este último paso en la cadena del SentenceTransformer normaliza el vector de embedding resultante. La normalización suele implicar escalar el vector de modo que su norma sea 1. Esto es útil para muchas tareas de comparación de vectores, como la similitud coseno, ya que simplifica los cálculos y a menudo mejora el rendimiento del modelo en tareas de similitud o clasificación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf06bc89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed5b66f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3808195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e1c1f5c",
   "metadata": {},
   "source": [
    "Se puede observar que todas las frases, sea cual sea su número de tokens, el modelo lo convierte a un vector de 1024 elementos. En realidad realiza la media por token, según su configuración por defecto. Veamos que ocurre cuando le damos una lista de frases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8790717a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79b7ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3da2116",
   "metadata": {},
   "source": [
    "El transformador nos devuelve un vector para cada frase con 1024 elementos. Veamos algunos casos de uso, fuera de los modelos que ya hemos visto, de esta transformación del texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fad6c18",
   "metadata": {},
   "source": [
    "## 5 - Similitud de oraciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec4d76c",
   "metadata": {},
   "source": [
    "Uno de los usos habituales de los embeddings es comparar dos frases, cómo de parecidas son. La métrica que se suele usar para ello es la similitud del coseno. La similitud del coseno es una medida que se utiliza para calcular el grado de similitud entre dos vectores en un espacio multidimensional, ignorando su magnitud. Se calcula de la siguiente manera:\n",
    "\n",
    "$$coseno (\\vec{a}, \\vec{b}) = \\frac{\\vec{a} · \\vec{b}}{|\\vec{a}| · |\\vec{b}|}$$\n",
    "\n",
    "\n",
    "siendo:\n",
    "\n",
    "+ $\\vec{a}, \\vec{b}$: vectores \n",
    "+ $\\vec{a} · \\vec{b}$: producto escalar de $\\vec{a}$ y $\\vec{b}$\n",
    "+ $|\\vec{a}|$,  $|\\vec{b}|$: normas de los vectores (magnitudes)\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Características de la similitud del coseno**:\n",
    "\n",
    "1. Rango de Valores: La similitud del coseno puede tomar valores entre -1 y 1.\n",
    "\n",
    "    + 1 indica que los dos vectores son idénticos en orientación.\n",
    "    + 0 indica que los vectores son ortogonales (independientes).\n",
    "    + -1 indica que los vectores son diametralmente opuestos.\n",
    "\n",
    "\n",
    "2. Insensible a la Magnitud: Solo importa el ángulo entre los vectores, no su longitud. Esto es útil cuando las magnitudes no llevan información relevante o cuando solo la dirección de los vectores es importante.\n",
    "\n",
    "\n",
    "\n",
    "Por supuesto, existen otra maneras de calcular la similitud entre dos vectores, usando otras [métricas de distancias](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html), aunque si no existe un criterio específico, siempre se usa la similitud del coseno."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c64cb3",
   "metadata": {},
   "source": [
    "Calcularemos la similitud del coseno con scikit-learn. Es posible que necesitemos ejecutar el siguiente comando por terminal para instalar la librería:\n",
    "\n",
    "```bash\n",
    "pip install scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd91b9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a3ccbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1c26ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aac46d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff13dfde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb341cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a79b1ae",
   "metadata": {},
   "source": [
    "Desde aquí, ya tenemos un métrica de como de parecidas son dos frases, tanto más parecidas cuanto más cercano a 1 sea la similitud del coseno, y tanto más discordantes cuanto más cercano a -1 sea dicha similitud. Si la similitud está en torno a 0, esas frases no se parecen en nada."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "599px",
    "left": "81px",
    "top": "111.141px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
