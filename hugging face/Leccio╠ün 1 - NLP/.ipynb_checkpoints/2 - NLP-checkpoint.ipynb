{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a50b714",
   "metadata": {},
   "source": [
    "# 2 - NLP\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/nlp.webp\" style=\"width:400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cb024e",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---Definición\" data-toc-modified-id=\"1---Definición-1\">1 - Definición</a></span></li><li><span><a href=\"#2---¿Qué-es-Transformers?\" data-toc-modified-id=\"2---¿Qué-es-Transformers?-2\">2 - ¿Qué es Transformers?</a></span></li><li><span><a href=\"#3---Tipos-de-modelos-basados-en-Transformers\" data-toc-modified-id=\"3---Tipos-de-modelos-basados-en-Transformers-3\">3 - Tipos de modelos basados en Transformers</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1---Modelos-Encoder\" data-toc-modified-id=\"3.1---Modelos-Encoder-3.1\">3.1 - Modelos Encoder</a></span></li><li><span><a href=\"#3.2---Modelos-Decoder\" data-toc-modified-id=\"3.2---Modelos-Decoder-3.2\">3.2 - Modelos Decoder</a></span></li><li><span><a href=\"#3.3---Modelos-Encoder-Decoder\" data-toc-modified-id=\"3.3---Modelos-Encoder-Decoder-3.3\">3.3 - Modelos Encoder-Decoder</a></span></li></ul></li><li><span><a href=\"#4---Sesgos-y-limitaciones\" data-toc-modified-id=\"4---Sesgos-y-limitaciones-4\">4 - Sesgos y limitaciones</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d8b002",
   "metadata": {},
   "source": [
    "## 1 - Definición"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121b251",
   "metadata": {},
   "source": [
    "NLP significa \"Procesamiento del Lenguaje Natural\", del inglés \"Natural Language Processing\". Es una rama de la inteligencia artificial que se enfoca en la interacción entre los computadores y los humanos a través del lenguaje natural. El objetivo del NLP es leer, descifrar, entender y hacer sentido del lenguaje humano de una manera que sea valiosa. Hay que recordar que las máquinas no procesan la información de la misma manera que los humanos. Por ejemplo, cuando leemos la frase \"Estoy hambriento\", podemos entender fácilmente su significado. De manera similar, dadas dos frases como \"Estoy hambriento\" y \"Estoy triste\", somos capaces de determinar fácilmente cómo de similares son. Para los modelos de NLP, estas tareas son más difíciles. El texto necesita ser procesado de una manera que permita al modelo aprender de él. Y dado que el lenguaje es complejo, necesitamos pensar cuidadosamente sobre cómo debe hacerse este procesamiento.\n",
    "\n",
    "\n",
    "Las principales funciones del NLP son:\n",
    "\n",
    "1. **Comprensión**: El NLP ayuda a las máquinas a entender el significado o la intención detrás de las palabras y oraciones humanas, permitiéndoles responder de manera apropiada.\n",
    "\n",
    "2. **Generación de Lenguaje**: Puede generar texto humano natural, lo que es útil en aplicaciones como chatbots y asistentes virtuales.\n",
    "\n",
    "3. **Traducción Automática**: Permite la traducción de texto de un idioma a otro sin intervención humana directa.\n",
    "\n",
    "4. **Reconocimiento de Voz**: Convierte la voz humana en texto, lo cual es fundamental para aplicaciones como los asistentes de voz y la transcripción automatizada.\n",
    "\n",
    "5. **Análisis de Sentimientos**: Identifica el tono emocional detrás de las palabras, útil en análisis de redes sociales, atención al cliente y más.\n",
    "\n",
    "\n",
    "Algunas aplicaciones comunes del NLP:\n",
    "\n",
    "+ Asistentes Virtuales: Como Siri, Alexa y Google Assistant, que utilizan NLP para entender y responder a comandos de voz.\n",
    "\n",
    "+ Chatbots y Servicio al Cliente: Automatizan respuestas y ayudan a resolver consultas de clientes mediante la comprensión del lenguaje natural.\n",
    "\n",
    "+ Herramientas de Análisis de Texto: Utilizadas en el ámbito empresarial para extraer información, analizar tendencias y sentimientos en documentos y comunicaciones.\n",
    "\n",
    "+ Sistemas de Recomendación: Utilizan NLP para entender las preferencias de los usuarios y recomendar productos o servicios relevantes.\n",
    "\n",
    "+ Educación: Aplicaciones que ayudan en el aprendizaje de idiomas o proporcionan tutoría automatizada personalizada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4b7249",
   "metadata": {},
   "source": [
    "## 2 - ¿Qué es Transformers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca0eb9e",
   "metadata": {},
   "source": [
    "Transformers es una arquitectura de modelo para NLP que ha revolucionado el campo de la inteligencia artificial. Desarrollado inicialmente por Vaswani et al. en 2017 en el paper [Attention is All You Need](https://arxiv.org/abs/1706.03762), el modelo Transformer introduce varias innovaciones técnicas que permiten un procesamiento más eficiente y efectivo del lenguaje natural. Sus características principales son las siguientes:\n",
    "\n",
    "1. Mecanismo de Atención: Los Transformers utilizan lo que se conoce como \"atención auto-dirigida\" o \"self-attention\" para procesar secuencias de entrada. Este mecanismo permite al modelo evaluar y ponderar la importancia de todas las palabras en una oración, independientemente de su posición, lo que facilita la comprensión del contexto y las relaciones entre palabras.\n",
    "\n",
    "2. Procesamiento Paralelo: A diferencia de modelos anteriores basados en redes neuronales recurrentes (RNN) o en memorias a largo y corto plazo (LSTM), los Transformers procesan todos los elementos de la entrada de manera simultánea. Esto no solo mejora la velocidad del entrenamiento sino que también facilita el aprendizaje de dependencias a largo plazo en el texto.\n",
    "\n",
    "3. Escalabilidad: Debido a su eficiencia en el procesamiento paralelo y su efectividad en aprender relaciones complejas en los datos, los modelos basados en Transformers escalan muy bien con el aumento de datos y la capacidad computacional, permitiendo entrenar modelos sobre conjuntos de datos extremadamente grandes.\n",
    "\n",
    "4. Flexibilidad: Los Transformers son altamente versátiles y se han adaptado para una amplia gama de tareas de NLP, más allá de su uso original en la traducción automática. Ejemplos incluyen análisis de sentimientos, generación de texto, resumen de documentos, y reconocimiento de entidades nombradas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98675d1f",
   "metadata": {},
   "source": [
    "## 3 - Tipos de modelos basados en Transformers\n",
    "\n",
    "Existen varios tipos de modelos basados en la arquirtectura de Transfomers. Veámoslos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd127e94",
   "metadata": {},
   "source": [
    "### 3.1 - Modelos Encoder\n",
    "\n",
    "Los modelos encoder utilizan solo el codificador de un modelo Transformer, su primera parte. En cada etapa, las capas de atención pueden acceder a todas las palabras de la frase inicial. Estos modelos a menudo se caracterizan por tener una atención \"bidireccional\" y comúnmente se les llama modelos de auto-codificación.\n",
    "\n",
    "El preentrenamiento de estos modelos generalmente implica corromper de alguna manera una frase dada, por ejemplo, enmascarando palabras al azar en ella, y asignando al modelo la tarea de encontrar o reconstruir la frase inicial.\n",
    "\n",
    "Los modelos de codificador son más adecuados para tareas que requieren una comprensión de la frase completa, como la clasificación de frases, el reconocimiento de entidades nombradas y, más generalmente, la clasificación de palabras y la respuesta a preguntas extractivas.\n",
    "\n",
    "Algunos modelos encoder son:\n",
    "\n",
    "+ BERT (Bidirectional Encoder Representations from Transformers)\n",
    "+ ALBERT (A Lite BERT)\n",
    "+ DistilBERT (Distilled BERT)\n",
    "+ ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)\n",
    "+ RoBERTa (Robustly Optimized BERT Approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903f0d39",
   "metadata": {},
   "source": [
    "### 3.2 - Modelos Decoder \n",
    "\n",
    "Los modelos decoder utilizan solo el decodificador de un modelo Transformer. En cada etapa, para una palabra dada, las capas de atención solo pueden acceder a las palabras que están posicionadas antes de ella en la oración. Estos modelos a menudo se denominan modelos autorregresivos.\n",
    "\n",
    "El preentrenamiento de los modelos de decodificador generalmente gira en torno a predecir la próxima palabra en la oración.\n",
    "\n",
    "Estos modelos son más adecuados para tareas que involucran la generación de texto.\n",
    "\n",
    "Algunos modelos decoder son:\n",
    "\n",
    "+ GPT (Generative Pre-trained Transformer)\n",
    "+ CTRL (Conditional Transformer Language Model for Controllable Generation)\n",
    "+ Transformer XL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbb6c40",
   "metadata": {},
   "source": [
    "### 3.3 - Modelos Encoder-Decoder\n",
    "\n",
    "Los modelos encoder-decoder, también llamados modelos de secuencia a secuencia (sequence-to-sequence), utilizan ambas partes de la arquitectura de Transformer. En cada etapa, las capas de atención del codificador pueden acceder a todas las palabras de la frase inicial, mientras que las capas de atención del decodificador solo pueden acceder a las palabras posicionadas antes de una palabra dada en la entrada.\n",
    "\n",
    "El preentrenamiento de estos modelos puede realizarse utilizando los objetivos de los modelos de codificador o decodificador, pero generalmente implica algo un poco más complejo. Por ejemplo, T5 se preentrena reemplazando tramos aleatorios de texto, que pueden contener varias palabras, con una palabra especial de máscara única, y el objetivo es entonces predecir el texto que esta palabra de máscara reemplaza.\n",
    "\n",
    "Los modelos de secuencia a secuencia son más adecuados para tareas que giran en torno a generar nuevas oraciones dependiendo de una entrada dada, como la resumen, traducción o respuesta a preguntas generativas.\n",
    "\n",
    "Representantes de esta familia de modelos incluyen:\n",
    "\n",
    "+ T5 (Text-to-Text Transfer Transformer)\n",
    "+ BART (Bidirectional and Auto-Regressive Transformers)\n",
    "+ mBART (Multilingual BART)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c96a762",
   "metadata": {},
   "source": [
    "## 4 - Sesgos y limitaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285dce13",
   "metadata": {},
   "source": [
    "Si nuestra intención es utilizar un modelo preentrenado o una versión afinada en producción, hay que tener en cuenta que, si bien estos modelos son herramientas poderosas, también tienen limitaciones. La mayor de estas es que, para habilitar el preentrenamiento en grandes cantidades de datos, los investigadores a menudo recopilan todo el contenido que pueden encontrar, tomando tanto lo mejor como lo peor de lo que está disponible en internet.\n",
    "\n",
    "Para dar una ilustración rápida, veamos ejemplo de un pipeline de llenado de espacios con el modelo BERT. Este modelo es guardado en local la primera vez que lo usamos y ocupa aproximadamente 450Mb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da1b5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "modelo = pipeline(task='fill-mask', model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cedea19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta = modelo('This man works as a [MASK].')\n",
    "\n",
    "[e['token_str'] for e in respuesta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "663c10b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta = modelo('This woman works as a [MASK].')\n",
    "\n",
    "[e['token_str'] for e in respuesta]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aa06b8",
   "metadata": {},
   "source": [
    "Cuando se le pide que complete la palabra que falta en estas dos oraciones, el modelo proporciona solo dos respuesta de género neutro (camarero/camarera y profesor/profesora). Las demás son ocupaciones laborales generalmente asociadas con un género específico, y sí, fijaros en las 5 posibilidades principales que el modelo asocia con \"mujer\" y \"trabajo\". Esto sucede incluso aunque BERT sea uno de los raros modelos Transformer que no se construyó recopilando datos de todo internet, sino utilizando datos aparentemente neutrales, está entrenado en los conjuntos de datos de la [Wikipedia](https://huggingface.co/datasets/wikipedia) en inglés y [BookCorpus](https://huggingface.co/datasets/bookcorpus).\n",
    "\n",
    "Cuando usamos estas herramientas, por lo tanto, debemos tener en cuenta que el modelo original que estamos utilizando podría generar fácilmente contenido sexista, racista u homofóbico. Afinar el modelo con nuestros datos no hará que este sesgo intrínseco desaparezca."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "599px",
    "left": "76px",
    "top": "111.141px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
