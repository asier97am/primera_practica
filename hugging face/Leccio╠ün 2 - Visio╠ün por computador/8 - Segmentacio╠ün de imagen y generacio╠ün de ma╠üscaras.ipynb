{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8727d15",
   "metadata": {},
   "source": [
    "# 8 - Segmentación de imagen y generación de máscaras\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/img_segmentation.webp\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bca4a5",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---Modelos-de-segmentación-de-imagen\" data-toc-modified-id=\"1---Modelos-de-segmentación-de-imagen-1\">1 - Modelos de segmentación de imagen</a></span></li><li><span><a href=\"#2---Pipeline-de-Transformers-para-segmentación-de-imágenes\" data-toc-modified-id=\"2---Pipeline-de-Transformers-para-segmentación-de-imágenes-2\">2 - Pipeline de Transformers para segmentación de imágenes</a></span></li><li><span><a href=\"#3---Usando-el-modelo-segmentador\" data-toc-modified-id=\"3---Usando-el-modelo-segmentador-3\">3 - Usando el modelo segmentador</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1---Extractor-de-características\" data-toc-modified-id=\"3.1---Extractor-de-características-3.1\">3.1 - Extractor de características</a></span></li><li><span><a href=\"#3.2---Modelo-segmentador-de-imágenes\" data-toc-modified-id=\"3.2---Modelo-segmentador-de-imágenes-3.2\">3.2 - Modelo segmentador de imágenes</a></span></li><li><span><a href=\"#3.3---Resumen-del-código\" data-toc-modified-id=\"3.3---Resumen-del-código-3.3\">3.3 - Resumen del código</a></span></li></ul></li><li><span><a href=\"#4---Otro-modelo-de-segmentación\" data-toc-modified-id=\"4---Otro-modelo-de-segmentación-4\">4 - Otro modelo de segmentación</a></span></li><li><span><a href=\"#5---Modelos-de-generación-de-máscaras\" data-toc-modified-id=\"5---Modelos-de-generación-de-máscaras-5\">5 - Modelos de generación de máscaras</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb2a841",
   "metadata": {},
   "source": [
    "## 1 - Modelos de segmentación de imagen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0046450b",
   "metadata": {},
   "source": [
    "Los modelos de segmentación de imágenes son técnicas de visión por computador que tienen como objetivo dividir una imagen en múltiples segmentos o regiones para identificar y localizar objetos, bordes y otros elementos relevantes dentro de la imagen. Existen distintas técnicas para realizar esta tarea:\n",
    "\n",
    "\n",
    "1. **Segmentación semántica**: Se asigna una etiqueta a cada píxel de la imagen según la categoría del objeto al que pertenece, por ejemplo, coche, árbol o persona. Algunos modelos semánticos son:\n",
    "\n",
    "    + Fully Convolutional Networks (FCNs): Transforma las redes convolucionales clásicas en una red completamente convolucional para la segmentación.\n",
    "    + U-Net: Utilizada ampliamente en aplicaciones biomédicas, esta arquitectura es conocida por su capacidad para trabajar con un número limitado de datos de entrenamiento.\n",
    "    + DeepLab: Desarrollada por Google, incorpora múltiples mejoras como atrous convolutions y CRFs para mejorar la precisión.\n",
    "\n",
    "\n",
    "2. **Segmentación de instancia**: Cada instancia individual de un objeto se segmenta y se diferencia de otras instancias de la misma clase. Algunos modelos de segmentación de instancia son:\n",
    "\n",
    "    + Mask R-CNN: Extensión de Faster R-CNN que añade una rama para predecir un mapa de segmentación binario para cada región de interés.\n",
    "    + YOLACT: Combina la detección de objetos y la segmentación en una sola red, siendo más rápida que Mask R-CNN.\n",
    "\n",
    "\n",
    "3. **Segmentación panóptica**: Combina segmentación semántica y de instancia, proporcionando una segmentación unificada que cubre tanto las clases de fondo como las instancias de objetos. Algunos modelos son:\n",
    "\n",
    "    + Panoptic FPN (Feature Pyramid Networks): Combina FPN con la segmentación semántica para producir resultados panópticos.\n",
    "    + EfficientPS: Un modelo eficiente y preciso para segmentación panóptica, utilizando redes de eficiencia y técnicas de fusión de características.\n",
    "\n",
    "\n",
    "4. **Segmentación basada en clustering**: Divide la imagen en segmentos mediante técnicas de clustering. Algunos modelos basados en lustering son:\n",
    "\n",
    "    + K-means Clustering: Agrupa los píxeles en K clusters basados en sus características de color o intensidad.\n",
    "    + Mean Shift: Segmenta la imagen encontrando modos en el espacio de características, adecuado para segmentación basada en densidad.\n",
    "    + Graph-based Segmentation: Utiliza gráficos y técnicas de partición de gráficos para segmentar la imagen.\n",
    "\n",
    "\n",
    "5. **Segmentación superficial o por contornos**: Identifica bordes o contornos de los objetos dentro de una imagen. Algunos técnicas para ello son:\n",
    "\n",
    "    + Canny Edge Detector: Un algoritmo clásico para detectar bordes en imágenes.\n",
    "    + Active Contour Models (Snakes): Utiliza curvas que se mueven dentro de la imagen para encontrar contornos de objetos.\n",
    "\n",
    "\n",
    "\n",
    "**Aplicaciones Comunes**:\n",
    "\n",
    "+ Medicina: Segmentación de tejidos y órganos en imágenes médicas.\n",
    "+ Automoción: Identificación de peatones, vehículos y señales en sistemas de conducción autónoma.\n",
    "+ Seguridad: Detección de intrusos y análisis de video vigilancia.\n",
    "+ Agricultura: Monitoreo de cultivos y segmentación de plantas.\n",
    "\n",
    "\n",
    "\n",
    "**Desafíos y Consideraciones**\n",
    "+ Precisión: La precisión de los modelos puede variar según la calidad de los datos de entrenamiento y la complejidad de las escenas.\n",
    "+ Velocidad: Algunos modelos, como Mask R-CNN, pueden ser más lentos y requieren mayor capacidad de procesamiento.\n",
    "+ Datos de Entrenamiento: La disponibilidad y calidad de los datos etiquetados pueden afectar significativamente el rendimiento del modelo.\n",
    "\n",
    "\n",
    "\n",
    "Estos modelos y técnicas permiten abordar una variedad de problemas en segmentación de imágenes, cada uno con sus propias ventajas y limitaciones dependiendo de la aplicación específica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ebe869",
   "metadata": {},
   "source": [
    "## 2 - Pipeline de Transformers para segmentación de imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa2deec",
   "metadata": {},
   "source": [
    "Usaremos un modelo creado por Facebook, [maskformer-swin-base-coco](https://huggingface.co/facebook/maskformer-swin-base-coco), un modelo de segmentación de imágenes que combina dos poderosas arquitecturas: MaskFormer y Swin Transformer. Este modelo ha sido entrenado en el conjunto de datos COCO (Common Objects in Context), que es ampliamente utilizado en tareas de visión por computador. Este modelo tiene un peso aproximado de 415Mb.\n",
    "\n",
    "**Componentes Clave**\n",
    "\n",
    "\n",
    "+ **MaskFormer**:\n",
    "\n",
    "MaskFormer es un transformer para la segmentación de imágenes que unifica la segmentación de instancias, la segmentación semántica y la segmentación panóptica en una sola arquitectura. Utiliza un enfoque basado en el aprendizaje de máscaras y clasificadores para producir mapas de segmentación. En MaskFormer, la imagen se divide en segmentos y cada segmento se asocia con una máscara y una etiqueta de clase, permitiendo una segmentación precisa y coherente.\n",
    "\n",
    "\n",
    "+ **Swin Transformer**:\n",
    "\n",
    "Swin Transformer es un modelo de visión basado en transformers que utiliza una estructura jerárquica de ventanas deslizantes para procesar imágenes. \"Swin\" es una abreviatura de \"Shifted Windows\", que se refiere a cómo el modelo divide y procesa la imagen en pequeñas ventanas. Swin Transformer es conocido por su eficiencia computacional y su capacidad para capturar características a diferentes escalas, lo que lo hace adecuado para tareas de segmentación y detección de objetos.\n",
    "\n",
    "\n",
    "+ **COCO Dataset**:\n",
    "\n",
    "El conjunto de datos COCO es un gran conjunto de datos de referencia en la comunidad de visión por computador, que contiene más de 200000 imágenes etiquetadas con más de 80 categorías de objetos. COCO es conocido por su complejidad y diversidad, lo que hace que los modelos entrenados en este conjunto de datos sean robustos y generalizables a una amplia variedad de escenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76db19cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging para quitar warnings de actualización de pesos del modelo\n",
    "\n",
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d41b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos desde la librería transformers el pipeline\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03de0633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos la tarea y el modelo\n",
    "\n",
    "tarea = 'image-segmentation'  \n",
    "\n",
    "modelo = 'facebook/maskformer-swin-base-coco'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7c3022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iniciamos el segmentador de imagenes, los modelos se descargan en local, en este caso son unos 415MB\n",
    "\n",
    "seg_pipe = pipeline(task=tarea, model=modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e466879d",
   "metadata": {},
   "source": [
    "Vamos a usar otra vez la imagen de los [4 cachorros](https://i0.wp.com/planetamascotaperu.com/wp-content/uploads/2023/09/cachorros1.jpg) para usar el modelo de segmentación. Vamos a intentar extraer los perros de la imagen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37cee04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url de la imagen\n",
    "\n",
    "url = 'https://i0.wp.com/planetamascotaperu.com/wp-content/uploads/2023/09/cachorros1.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6cb8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# librerias PIL y requests\n",
    "from PIL import Image\n",
    "import requests as req\n",
    "\n",
    "\n",
    "# imagen como objeto PIL\n",
    "imagen = Image.open(fp=req.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f5d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualización\n",
    "\n",
    "imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17e27d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamada al pipeline para la segmentación de la imagen\n",
    "\n",
    "respuesta = seg_pipe(images=imagen)\n",
    "\n",
    "respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677d41af",
   "metadata": {},
   "source": [
    "La respuesta del pipeline es una lista de diccionarios cuyas keys son la etiqueta del objeto detectado, la probabilidad asociada y una máscara como objeto PIL, una imagen en blanco y negro que se utiliza para definir las áreas de la imagen donde se encuentran los objetos detectados. Veamos una de ellas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced26608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# máscara\n",
    "\n",
    "respuesta[0]['mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74590203",
   "metadata": {},
   "source": [
    "Vamos a juntar esta máscara con la imagen original para ver que zona es la que se ha detectado. Primero copiamos la imagen original para luego combinarla con la máscara en negativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561e272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las operaciones de PIL\n",
    "from PIL import ImageOps\n",
    "\n",
    "# hacemos una copia de la imagen\n",
    "im = imagen.copy()\n",
    "\n",
    "# ponemos en negativo la mascara\n",
    "mascara = ImageOps.invert(image=respuesta[0]['mask'])\n",
    "\n",
    "# combinamos ambas imágenes\n",
    "im.paste(im=mascara, mask=mascara)\n",
    "\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316279e4",
   "metadata": {},
   "source": [
    "Podemos combinar varias máscaras a la vez. Para hacer eso, primero convertimos las máscaras a arrays de numpy y las sumamos, para luego volver a un objeto PIL. Desde ahí, el proceso es igual que lo anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbf414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos numpy\n",
    "import numpy as np\n",
    "\n",
    "# unimos varias máscaras\n",
    "mascara = Image.fromarray(obj=np.array(respuesta[0]['mask'])+np.array(respuesta[3]['mask']))\n",
    "\n",
    "# hacemos una copia de la imagen\n",
    "im = imagen.copy()\n",
    "\n",
    "# ponemos en negativo la mascara\n",
    "mascara = ImageOps.invert(image=mascara)\n",
    "\n",
    "# combinamos ambas imágenes\n",
    "im.paste(im=mascara, mask=mascara)\n",
    "\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad8b109",
   "metadata": {},
   "source": [
    "Veámos otro ejmplo con una imagen de [un gato y un perro](https://huggingface.co/datasets/mishig/sample_images/resolve/main/dog-cat.jpg). El procezso es cargamos la imagen, llamamos al pipeline, invertimos la máscara y pegamos ambas imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f621da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url\n",
    "\n",
    "url = 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/dog-cat.jpg'\n",
    "\n",
    "# imagen como objeto PIL\n",
    "imagen = Image.open(fp=req.get(url, stream=True).raw)\n",
    "\n",
    "imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd04ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamada al pipeline para la segmentación de la imagen\n",
    "\n",
    "respuesta = seg_pipe(images=imagen)\n",
    "\n",
    "respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aff00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacemos una copia de la imagen\n",
    "im = imagen.copy()\n",
    "\n",
    "\n",
    "# ponemos en negativo la mascara\n",
    "mascara = ImageOps.invert(image=respuesta[2]['mask'])\n",
    "\n",
    "# combinamos ambas imágenes\n",
    "im.paste(im=mascara, mask=mascara)\n",
    "\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb68cb71",
   "metadata": {},
   "source": [
    "## 3 - Usando el modelo segmentador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa65509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos los objetos procesador y modelo de segmentación de imagenes\n",
    "\n",
    "from transformers import MaskFormerImageProcessor, MaskFormerForInstanceSegmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c91125",
   "metadata": {},
   "source": [
    "### 3.1 - Extractor de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54872c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# procesa las imágenes y extrae las características\n",
    "\n",
    "procesador = MaskFormerImageProcessor.from_pretrained('facebook/maskformer-swin-base-coco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc4d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descripcion del objeto\n",
    "\n",
    "procesador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c03e68",
   "metadata": {},
   "source": [
    "**Componentes de MaskFormerImageProcessor**\n",
    "\n",
    "\n",
    "+ **_max_size**: Valor máximo para la dimensión más grande de la imagen. Establecido en 1333 píxeles.\n",
    "\n",
    "\n",
    "+ **_valid_processor_keys**: Lista de claves válidas que pueden ser utilizadas en el procesador. Incluye:\n",
    "\n",
    "    + **images**: Imágenes a procesar.\n",
    "    + **segmentation_maps**: Mapas de segmentación correspondientes a las imágenes.\n",
    "    + **instance_id_to_semantic_id**: Mapeo de IDs de instancias a IDs semánticos.\n",
    "    + **do_resize**: Indica si se debe redimensionar la imagen. Configurado como true.\n",
    "    + **size**: Tamaño al cual redimensionar la imagen, especificado como un diccionario con longest_edge (1333 píxeles) y shortest_edge (800 píxeles).\n",
    "    + **size_divisor**: Divisor de tamaño utilizado para asegurar que las dimensiones sean múltiplos de este valor. Establecido en 32.\n",
    "    + **resample**: Método de remuestreo a utilizar al redimensionar la imagen. Establecido como 2, que corresponde a Image.BILINEAR en PIL.\n",
    "    + **do_rescale**: Indica si se debe reescalar la imagen. Configurado como true.\n",
    "    + **rescale_factor**: Factor de reescalado a aplicar a los valores de los píxeles. Establecido en 0.00392156862745098 (1/255).\n",
    "    + **do_normalize**: Indica si se deben normalizar los valores de los píxeles. Configurado como true.\n",
    "    + **image_mean**: Valores de media para la normalización de los píxeles, especificados como una lista de tres valores que corresponden a los canales de color (RGB), [0.485, 0.456, 0.406].\n",
    "    + **image_processor_type**: Tipo del procesador de imágenes. Aquí está establecido como MaskFormerImageProcessor.\n",
    "    + **image_std**: Valores de desviación estándar para la normalización de los píxeles, especificados como una lista de tres valores que corresponden a los canales de color (RGB), [0.229, 0.224, 0.225].\n",
    "    + **ignore_index**: Índice de los píxeles a ignorar en los mapas de segmentación. Establecido en 255.\n",
    "    + **do_reduce_labels**: Indica si se deben reducir las etiquetas de segmentación. Configurado como false.\n",
    "    + **return_tensors**: Tipo de tensores a devolver (pt para PyTorch, tf para TensorFlow, etc.).\n",
    "    + **data_format**: Formato de los datos de entrada.\n",
    "    + **input_data_format**: Formato esperado de los datos de entrada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a44588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# procesamiento de la imagen\n",
    "\n",
    "img_procesada = procesador(images=imagen, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8081494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensores en la imagen procesada\n",
    "\n",
    "img_procesada.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b0728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# una imagen, 3 canales, 800x1152 píxeles\n",
    "\n",
    "img_procesada['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7e794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# una máscara de 800x1152 píxeles\n",
    "\n",
    "img_procesada['pixel_mask'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe3f2e0",
   "metadata": {},
   "source": [
    "### 3.2 - Modelo segmentador de imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717c6119",
   "metadata": {},
   "source": [
    "El MaskFormerForInstanceSegmentation es un modelo de segmentación de instancias basado en la arquitectura MaskFormer, diseñado para realizar tareas de segmentación de instancias en imágenes. Este modelo combina técnicas avanzadas de procesamiento de imágenes con Transformers para generar máscaras de segmentación precisas y etiquetas de clase para cada instancia en la imagen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dd8e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicializacion del modelo maskformer\n",
    "\n",
    "modelo_segmentador = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cd79ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# descripcion del objeto\n",
    "\n",
    "modelo_segmentador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363f43ff",
   "metadata": {},
   "source": [
    "**Componentes Principales del Modelo**\n",
    "\n",
    "\n",
    "1. **MaskFormerModel**: Es la base del modelo que contiene los módulos de nivel de píxel y de Transformer.\n",
    "\n",
    "\n",
    "2. **MaskFormerPixelLevelModule**: Módulo encargado de procesar la imagen a nivel de píxel.\n",
    "    \n",
    "    + MaskFormerSwinModel: Modelo Swin Transformer que procesa la imagen en múltiples etapas.\n",
    "    \n",
    "    + MaskFormerSwinEmbeddings: Crea embeddings de la imagen dividiéndola en paquetes.\n",
    "    \n",
    "    + MaskFormerSwinPatchEmbeddings: Convolución para convertir la imagen en paquetes.\n",
    "    \n",
    "    + Norm: Normalización de los embeddings.\n",
    "    \n",
    "    + Dropout: Aplicación de dropout para regularización.\n",
    "    \n",
    "    + MaskFormerSwinEncoder: Contiene múltiples etapas de procesamiento para extraer características jerárquicas.\n",
    "    \n",
    "    + LayerNorm: Normalización de las características antes de la atención.\n",
    "    \n",
    "    + MaskFormerSwinAttention: Mecanismo de atención propio del Swin Transformer.\n",
    "    \n",
    "    + MaskFormerSwinSelfAttention: Atención propia con proyecciones de query, key y value.\n",
    "    \n",
    "    + Dropout: Regularización durante la atención.\n",
    "    \n",
    "    + MaskFormerSwinSelfOutput: Salida del mecanismo de atención.\n",
    "    \n",
    "    + DropPath: Regularización mediante el dropout.\n",
    "    \n",
    "    + LayerNorm: Normalización de las características después de la atención.\n",
    "    \n",
    "    + MaskFormerSwinIntermediate: Proyección a un espacio intermedio con activación GELU.\n",
    "    \n",
    "    + MaskFormerSwinOutput: Salida del bloque intermedio.\n",
    "    \n",
    "    + MaskFormerSwinPatchMerging: Reduce la resolución espacial y aumenta la profundidad de características.\n",
    "    \n",
    "    + LayerNorm: Normalización final de las características.\n",
    "\n",
    "    + Pooler: Pooling adaptativo para ajustar el tamaño de salida.\n",
    "\n",
    "    + MaskFormerPixelDecoder: Decodificador que toma las características extraídas y genera el mapa de máscaras.\n",
    "    \n",
    "    + MaskFormerFPNModel: Modelo de red piramidal de características (FPN) que combina características de múltiples resoluciones.\n",
    "    \n",
    "    + MaskFormerFPNConvLayer: Capas convolucionales utilizadas para procesar características.\n",
    "    \n",
    "    + MaskFormerFPNLayer: Capas individuales de la FPN con proyecciones y convoluciones.\n",
    "    \n",
    "    + MaskProjection: Convolución para proyectar las características finales a las máscaras.\n",
    "\n",
    "\n",
    "3. **MaskFormerTransformerModule**: Módulo Transformer encargado de refinar las máscaras y generar predicciones finales.\n",
    "\n",
    "    + PositionEmbedder: Embeddings posicionales sinusoidales.\n",
    "\n",
    "    + QueriesEmbedder: Embeddings para las queries del Transformer.\n",
    "\n",
    "    + InputProjection: Proyección de la entrada al espacio del Transformer.\n",
    "\n",
    "    + DetrDecoder: Decodificador basado en la arquitectura DETR.\n",
    "\n",
    "    + DetrDecoderLayer: Capas del decodificador con atención y proyecciones lineales.\n",
    "\n",
    "    + LayerNorm: Normalización en cada capa del decodificador.\n",
    "\n",
    "\n",
    "4. **ClassPredictor**: Capa lineal que predice las clases para cada instancia.\n",
    "\n",
    "\n",
    "5. **MaskEmbedder**: Contiene una cabeza MLP (Multi-Layer Perceptron) para refinar las máscaras de segmentación y varios bloques de predicción con capas lineales y activación ReLU.\n",
    "\n",
    "\n",
    "6. **Matcher**: Asigna las predicciones a las instancias verdaderas basado en costos predefinidos.\n",
    "\n",
    "\n",
    "7. **Criterion**: Función de pérdida que utiliza el matcher para calcular la pérdida total del modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0cf0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultado del modelo maskformer al darle la imagen procesada\n",
    "\n",
    "tensor = modelo_segmentador(**img_procesada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a039f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7075bf7",
   "metadata": {},
   "source": [
    "Una vez obtenido el resultado del modelo, el tensor se lo devolvemos al procesador para obtener las máscaras y las etiquetas de la segmentación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56939b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# post-procesamiento, devuelve máscaras y etiquetas\n",
    "\n",
    "segmentacion = procesador.post_process_panoptic_segmentation(tensor, target_sizes=[imagen.size[::-1]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e0fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor de máscaras\n",
    "\n",
    "mascaras = segmentacion['segmentation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c00927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# máximo y mínimo\n",
    "\n",
    "mascaras.max(), mascaras.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0645efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensión de las máscaras\n",
    "\n",
    "mascaras.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0400c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensión de la imagen original\n",
    "\n",
    "imagen.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d17f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediccion de etiquetas y probabilidad asociada\n",
    "\n",
    "predicciones = segmentacion['segments_info']\n",
    "\n",
    "predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4ce433",
   "metadata": {},
   "source": [
    "El tensor de máscaras que hemos definido, solo contiene los valores 1, 2 y 3, que corresponden con los ids de las predicciones de las etiquetas. Es decir, tenemos tres máscaras en el mismo tensor, cada una asociado a un id. Vamos a extraer una de ellas y la vamos a convertir en una imagén para verla. A continuación filtramos la imagen original con la máscara exactamente de la misma manera que hicimos antes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# máscara del gato\n",
    "\n",
    "gato = np.array(np.where(mascaras==2, 0, 255)).astype(np.uint8)\n",
    "\n",
    "gato = Image.fromarray(gato)\n",
    "\n",
    "gato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ddacd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtramos la imagen original\n",
    "\n",
    "# hacemos una copia de la imagen\n",
    "im = imagen.copy()\n",
    "\n",
    "# combinamos ambas imágenes\n",
    "im.paste(im=gato, mask=gato)\n",
    "\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1916a0",
   "metadata": {},
   "source": [
    "Las predicciones también nos devuelven la key label_id, que se corresponde con la etiqueta que predice el modelo. Dentro de la configuración del modelo, tenemos un diccionario que contiene todas las etiquetas que el modelo es capaz de detectar. Veamos cuales son esas etiquetas y cuales son las etiquetas que detecta el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fac459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todas las etiquetas\n",
    "\n",
    "etiquetas = modelo_segmentador.config.id2label\n",
    "\n",
    "print(etiquetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974e9f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# etiqueta 15\n",
    "\n",
    "etiquetas[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18216b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# etiqueta 116\n",
    "\n",
    "etiquetas[116]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf2345b",
   "metadata": {},
   "source": [
    "### 3.3 - Resumen del código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72268264",
   "metadata": {},
   "source": [
    "Vamos a poner todo el código junto en una sola función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacf44f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# librerías\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import MaskFormerImageProcessor, MaskFormerForInstanceSegmentation\n",
    "\n",
    "\n",
    "\n",
    "def segmentador_imagen(imagen: object) -> list:\n",
    "    \n",
    "    \"\"\"\n",
    "    Función para segmentar imagenes con modelo maskformer\n",
    "    \n",
    "    Params:\n",
    "    imagen: objeto PIL.JpegImagePlugin.JpegImageFile\n",
    "    \n",
    "    Return:\n",
    "    lista de diccionarios con keys etiquetas, probabilidades y máscaras \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # procesa las imágenes y extrae las características\n",
    "    procesador = MaskFormerImageProcessor.from_pretrained('facebook/maskformer-swin-base-coco')\n",
    "    \n",
    "    \n",
    "    # procesamiento de la imagen\n",
    "    img_procesada = procesador(images=imagen, return_tensors='pt')\n",
    "    \n",
    "    \n",
    "    # inicializacion del modelo maskformer\n",
    "    modelo_segmentador = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\n",
    "    \n",
    "    \n",
    "    # resultado del modelo maskformer al darle la imagen procesada\n",
    "    tensor = modelo_segmentador(**img_procesada)\n",
    "    \n",
    "    \n",
    "    # post-procesamiento, devuelve máscaras y etiquetas\n",
    "    segmentacion = procesador.post_process_panoptic_segmentation(tensor, target_sizes=[imagen.size[::-1]])[0]\n",
    "    \n",
    "    \n",
    "    # tensor de máscaras\n",
    "    mascaras = segmentacion['segmentation']\n",
    "    \n",
    "    \n",
    "    # prediccion de etiquetas y probabilidad asociada\n",
    "    predicciones = segmentacion['segments_info']\n",
    "\n",
    "    \n",
    "    # todas las etiquetas\n",
    "    etiquetas = modelo_segmentador.config.id2label\n",
    "    \n",
    "    \n",
    "    # lista de resultado\n",
    "    resultado = []\n",
    "    \n",
    "    for i in range(len(predicciones)):\n",
    "        \n",
    "        etiqueta = etiquetas[predicciones[i]['label_id']]\n",
    "        \n",
    "        prob = predicciones[i]['score']\n",
    "        \n",
    "        mascara = mascaras[i]\n",
    "        \n",
    "        mascara = np.array(np.where(mascaras==i+1, 0, 255)).astype(np.uint8)\n",
    "        \n",
    "        mascara = Image.fromarray(mascara)\n",
    "        \n",
    "        dictio = {'etiqueta': etiqueta, 'probabilidad': prob, 'mascara': mascara}\n",
    "        \n",
    "        resultado.append(dictio)\n",
    "        \n",
    "    \n",
    "    return resultado\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a163f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = segmentador_imagen(imagen)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59b9a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[1]['mascara']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314b1455",
   "metadata": {},
   "source": [
    "## 4 - Otro modelo de segmentación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f672842",
   "metadata": {},
   "source": [
    "Vamos a probar un modelo que elimina el fondo de la imagen. [RMBG v1.4](https://huggingface.co/briaai/RMBG-1.4) es nuestro modelo de eliminación de fondo, diseñado para separar eficazmente el primer plano del fondo en una variedad de categorías y tipos de imágenes. Este modelo ha sido entrenado en un conjunto de datos cuidadosamente seleccionado, que incluye imágenes de stock generales, comercio electrónico, contenido de juegos y publicidad, lo que lo hace adecuado para casos de uso comercial que impulsan la creación de contenido empresarial a gran escala. Su precisión, eficiencia y versatilidad rivalizan actualmente con los modelos líderes disponibles como código abierto. Es ideal cuando la seguridad del contenido, los conjuntos de datos legalmente licenciados y la mitigación de sesgos son fundamentales. El modelo pesa unos 180Mb, además requiere realizar la siguiente instalación:\n",
    "\n",
    "```bash\n",
    "pip install scikit-image\n",
    "``` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d3c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos la tarea y el modelo\n",
    "\n",
    "tarea = 'image-segmentation'  \n",
    "\n",
    "modelo = 'briaai/RMBG-1.4'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicialización del pipeline\n",
    "\n",
    "back_pipe = pipeline(task=tarea, model=modelo, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cb7afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# podemos pedirle la máscara...\n",
    "\n",
    "back_pipe(imagen, return_mask = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a275ebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o pedirle directamente la imagen filtrada\n",
    "\n",
    "back_pipe(imagen) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9fbdd6",
   "metadata": {},
   "source": [
    "## 5 - Modelos de generación de máscaras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf704e1",
   "metadata": {},
   "source": [
    "La segmentación de imágenes y la generación de máscaras están estrechamente relacionadas, pero tienen enfoques y objetivos ligeramente diferentes. Algunas diferencias se explican a continuación:\n",
    "\n",
    "**Segmentación de Imágenes**.\n",
    "La segmentación de imágenes es un proceso en el que una imagen se divide en varias regiones o segmentos con el objetivo de simplificar o cambiar la representación de una imagen en algo que sea más significativo y más fácil de analizar. En otras palabras, la segmentación de imágenes implica identificar y delimitar diferentes objetos o regiones dentro de una imagen. La segmentación de imágenes puede clasificarse en varias categorías:\n",
    "\n",
    "+ Segmentación Semántica: Asigna una etiqueta de clase a cada píxel de la imagen. Por ejemplo, en una imagen de una calle, los píxeles pueden ser clasificados como carretera, acera, coche o persona.\n",
    "+ Segmentación de Instancias: No solo clasifica cada píxel, sino que también distingue entre diferentes instancias de la misma clase. Por ejemplo, identificar múltiples personas en una imagen como individuos separados.\n",
    "+ Segmentación Panóptica: Combina la segmentación semántica y de instancias para proporcionar una vista completa de la imagen.\n",
    "\n",
    "\n",
    "**Generación de Máscaras**.\n",
    "La generación de máscaras, en el contexto de la visión por computador, se refiere al proceso de crear una máscara binaria, o multiclase, que indica la presencia y ubicación de un objeto específico dentro de una imagen. Las máscaras son imágenes binarias, o de múltiples valores en el caso de múltiples clases, donde los píxeles correspondientes a un objeto se marcan con un valor distinto, por ejemplo, 1 para el objeto y 0 para el fondo.\n",
    "\n",
    "+ Binarias o Multiclase: Una máscara binaria tiene solo dos valores posibles por píxel, por ejemplo, 0 y 1, mientras que una máscara multiclase puede tener varios valores para representar diferentes clases de objetos, como la que hemos visto anteriormente.\n",
    "+ Localización Precisa: Las máscaras proporcionan una localización exacta de los objetos, lo que es crucial para tareas como la segmentación de instancias y la detección de bordes.\n",
    "\n",
    "\n",
    "**Diferencias Clave**\n",
    "\n",
    "\n",
    "1. **Propósito**:\n",
    "\n",
    "    + Segmentación de Imágenes: Se utiliza para identificar y delimitar diferentes objetos o regiones dentro de una imagen. \n",
    "    + Generación de Máscaras: Específicamente se refiere a la creación de una imagen que resalta ciertas regiones de interés.\n",
    "\n",
    "\n",
    "2. **Salida**:\n",
    "\n",
    "    + Segmentación de Imágenes: La salida es una imagen segmentada donde cada píxel tiene una etiqueta de clase.\n",
    "    + Máscaras: La salida es una o más máscaras binarias o multiclase que indican la presencia de objetos específicos.\n",
    "\n",
    "\n",
    "3. **Aplicación**:\n",
    "\n",
    "    + Segmentación de Imágenes: Se utiliza en aplicaciones donde es necesario entender la estructura completa de la imagen, como en conducción autónoma o análisis médico.\n",
    "    + Máscaras: Utilizadas para tareas donde se requiere una identificación precisa de objetos específicos dentro de una imagen.\n",
    "\n",
    "\n",
    "4. **Ejemplos Prácticos**:\n",
    "\n",
    "    + Segmentación Semántica: En una imagen urbana, etiquetar cada píxel como carretera, edificio, coche o peatón.\n",
    "    + Máscaras de Segmentación: En una imagen de un conjunto de células, crear máscaras para cada célula individual para análisis posterior.\n",
    "\n",
    "La segmentación de imágenes es una técnica más amplia y general que puede incluir la generación de máscaras como una de sus técnicas específicas para identificar y localizar objetos dentro de una imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3800a88a",
   "metadata": {},
   "source": [
    "Usaremos el modelo [SAM](https://huggingface.co/facebook/sam-vit-base) (Segment Anything Model), creado por Facebook. El modelo SAM produce máscaras de objetos de alta calidad a partir de indicaciones de entrada como puntos o cajas, y puede usarse para generar máscaras para todos los objetos en una imagen. Ha sido entrenado en un conjunto de datos de 11 millones de imágenes y 1100 millones de máscaras, y tiene un rendimiento robusto en zero-shot en una variedad de tareas de segmentación.\n",
    "\n",
    "El modelo SAM está compuesto por los siguientes módulos:\n",
    "\n",
    "+ VisionEncoder: un codificador de imágenes basado en VIT. Calcula los embeddings de la imagen utilizando atención en paquetes de la imagen. Se utiliza el Embedding Posicional Relativo.\n",
    "\n",
    "+ PromptEncoder: genera embeddings para puntos y cajas delimitadoras.\n",
    "\n",
    "+ MaskDecoder: un transformer bidireccional que realiza atención cruzada entre el embedding de la imagen y los embeddings de los puntos y entre los embeddings de los puntos y los embeddings de la imagen. \n",
    "\n",
    "+ Neck: predice las máscaras de salida basadas en las máscaras contextualizadas producidas por el MaskDecoder.\n",
    "\n",
    "El modelo, al descargarlo en local, pesa unos 375Mb. Para probarlo, usaremos la imagen de [un coche aparcado](https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1de3f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url de la imagen \n",
    "\n",
    "url = 'https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f94d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagen como objeto PIL\n",
    "imagen = Image.open(fp=req.get(url, stream=True).raw)\n",
    "\n",
    "imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf30d8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos la tarea y el modelo\n",
    "\n",
    "tarea = 'mask-generation'  \n",
    "\n",
    "modelo = 'facebook/sam-vit-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d07806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicialización del pipeline\n",
    "\n",
    "generador = pipeline(task=tarea, model=modelo, points_per_batch = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d7a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# llamada al pipeline\n",
    "\n",
    "mascaras = generador(imagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b0325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nº de máscaras\n",
    "\n",
    "len(mascaras['masks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c481b1b",
   "metadata": {},
   "source": [
    "Creamos una función para pintar las máscaras, para luego realizar un bucle sobre las 48 máscaras que el modelo detecta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d81939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos pylab (matplotlib)\n",
    "import pylab as plt\n",
    "\n",
    "\n",
    "def pintar_mascara(mascara: object, eje: object) -> None:\n",
    "    \n",
    "    \"\"\"\n",
    "    Función que pinta cada máscara.\n",
    "    \n",
    "    Params:\n",
    "    mascara: np.array, la matriz de la máscara\n",
    "    eje: matplotlib.axes._axes.Axes, figura donde se pinta\n",
    "    \n",
    "    Return:\n",
    "    No devuelve nada, simplemente pinta la imagen de la máscara\n",
    "    \"\"\"\n",
    "    \n",
    "    # creación de colores de manera aleatoria\n",
    "    color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    \n",
    "    # alto y ancho de la máscara\n",
    "    alto, ancho = mascara.shape[-2:]\n",
    "    \n",
    "    # coloreado de la máscara\n",
    "    imagen_mascara = mascara.reshape(alto, ancho, 1) * color.reshape(1, 1, -1)\n",
    "    \n",
    "    # muestra la imagen en el gráfico creado\n",
    "    eje.imshow(imagen_mascara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4560fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# podemos pintar las máscaras sobre la imagen original\n",
    "\n",
    "plt.imshow(np.array(imagen))\n",
    "\n",
    "eje = plt.gca()\n",
    "\n",
    "for m in mascaras['masks']:\n",
    "    \n",
    "    pintar_mascara(mascara=m, eje=eje)\n",
    "    \n",
    "\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d867cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o podemos pintar solo las máscaras\n",
    "\n",
    "eje = plt.gca()\n",
    "\n",
    "for m in mascaras['masks']:\n",
    "    \n",
    "    pintar_mascara(mascara=m, eje=eje)\n",
    "    \n",
    "    \n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f86cccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "599px",
    "left": "84px",
    "top": "111.141px",
    "width": "267.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
