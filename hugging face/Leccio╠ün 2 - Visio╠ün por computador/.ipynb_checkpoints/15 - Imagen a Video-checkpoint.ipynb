{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d10d3975",
   "metadata": {},
   "source": [
    "# 15 - Imagen a Video\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/image2video.webp\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f11a7f",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---Modelos-de-imagen-a-video\" data-toc-modified-id=\"1---Modelos-de-imagen-a-video-1\">1 - Modelos de imagen a video</a></span></li><li><span><a href=\"#2---Pipeline-de-Diffusers-para-modelos-de-imagen-a-video\" data-toc-modified-id=\"2---Pipeline-de-Diffusers-para-modelos-de-imagen-a-video-2\">2 - Pipeline de Diffusers para modelos de imagen a video</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a94c7",
   "metadata": {},
   "source": [
    "## 1 - Modelos de imagen a video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81afd012",
   "metadata": {},
   "source": [
    "Los modelos de imagen a video son una categoría de algoritmos diseñados para generar secuencias de video a partir de imágenes estáticas. Estos modelos pueden transformar una o varias imágenes estáticas en un video coherente, creando la ilusión de movimiento y cambio temporal. Generan secuencias de cuadros, frames, que cuando se reproducen en sucesión, crean un video. Estos modelos utilizan imágenes estáticas como entrada y producen una serie de imágenes que representan un cambio continuo y coherente a lo largo del tiempo.\n",
    "\n",
    "\n",
    "\n",
    "**Tipos de modelos**:\n",
    "\n",
    "+ Redes neuronales tecurrentes (RNNs): Utilizadas para manejar datos secuenciales, estas redes pueden capturar dependencias temporales en la generación de videos.\n",
    "\n",
    "+ Redes generativas antagónicas (GANs): Específicamente, variantes como VideoGAN o TGAN (Temporal GAN) están diseñadas para generar secuencias de video.\n",
    "\n",
    "+ Modelos basados en Transformers: Como los transformers temporales, que se utilizan para capturar relaciones a largo plazo en los datos secuenciales.\n",
    "\n",
    "\n",
    "\n",
    "**Funcionamiento**:\n",
    "\n",
    "+ Entrada: Una o varias imágenes estáticas. A veces, una imagen inicial y una final se utilizan para generar un video que transicione de una a otra.\n",
    "\n",
    "+ Salida: Una secuencia de imágenes que juntas forman un video.\n",
    "\n",
    "\n",
    "\n",
    "**Aplicaciones**\n",
    "\n",
    "1.  Animación y Arte Digital:\n",
    "\n",
    "    + Creación de Animaciones: Generar animaciones a partir de dibujos o imágenes estáticas.\n",
    "    + Estilización de Videos: Aplicar estilos artísticos a secuencias de video.\n",
    "\n",
    "\n",
    "2. Mejora de Videos:\n",
    "\n",
    "    + Interpolación de Cuadros: Mejorar la suavidad de los videos generando cuadros intermedios.\n",
    "    + Aumento de Resolución: Mejorar la calidad de los videos generando cuadros de alta resolución a partir de secuencias de baja resolución.\n",
    "\n",
    "\n",
    "3. Realidad Aumentada y Virtual:\n",
    "\n",
    "    + Simulación de Movimientos: Crear simulaciones realistas en entornos virtuales a partir de imágenes estáticas.\n",
    "    + Cine y Producción de Medios: Generar efectos visuales complejos y transiciones suaves en postproducción.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Desafíos**\n",
    "\n",
    "+ Coherencia Temporal: Mantener la coherencia y continuidad en el tiempo es un desafío importante, ya que el video debe parecer fluido y natural.\n",
    "\n",
    "+ Calidad Visual: Generar cuadros de alta calidad que no presenten artefactos visuales y mantengan detalles importantes.\n",
    "\n",
    "+ Computación y Recursos: La generación de videos requiere significativos recursos computacionales y tiempo de procesamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcd2940",
   "metadata": {},
   "source": [
    "Este Jupyter Notebook será ejecutado en Google Colab en el siguiente [link](https://colab.research.google.com/drive/1D0i51brXROsJGf3YDM908PQMMiCl-8Nm?usp=sharing) debido a la necesidad de GPU y también porque mps no acepta redes Conv3D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0de799",
   "metadata": {},
   "source": [
    "## 2 - Pipeline de Diffusers para modelos de imagen a video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e3951b",
   "metadata": {},
   "source": [
    "Existen pocos modelos de imagen a video en el hub de Hugging Face. Veamos como usar uno de ellos.\n",
    "\n",
    "\n",
    "[Stable Video Diffusion (SVD) 1.1 Image-to-Video](https://huggingface.co/vdo/stable-video-diffusion-img2vid-xt-1-1) es un modelo de difusión que toma una imagen fija como marco de condicionamiento y genera un video a partir de ella. Este modelo fue entrenado para generar 25 fotogramas a una resolución de 1024x576 dados un marco de contexto del mismo tamaño, afinado a partir de SVD Image-to-Video.El ajuste se realizó con condicionamiento fijo a 6FPS para mejorar la consistencia de las salidas sin necesidad de ajustar hiperparámetros. Estas condiciones aún son ajustables y no se han eliminado. El modelo pesa unos 9Gb.\n",
    "\n",
    "Usaremos la misma imagen del [David de Miguel Angel](https://raw.githubusercontent.com/timothybrooks/instruct-pix2pix/main/imgs/example.jpg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcfb7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalaciones para colab\n",
    "\n",
    "!pip install transformers\n",
    "!pip install diffusers\n",
    "!pip install torch\n",
    "!pip install accelerate\n",
    "!pip install pillow\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0639d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para quitar warnings \n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c2865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las librerias\n",
    "\n",
    "from diffusers import DiffusionPipeline, EulerAncestralDiscreteScheduler\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests as req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d001c38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url de la imagen\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/timothybrooks/instruct-pix2pix/main/imgs/example.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf204f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualización\n",
    "\n",
    "imagen = Image.open(req.get(url, stream=True).raw)\n",
    "\n",
    "imagen.resize((400, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6958565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definición del modelo\n",
    "\n",
    "modelo = 'vdo/stable-video-diffusion-img2vid-xt-1-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c2b756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicializamos el pipeline\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(pretrained_model_name_or_path=modelo,\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a58264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambio de dispositivo y añadido del scheduler al pipeline\n",
    "\n",
    "pipeline.to('cuda')\n",
    "\n",
    "pipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(pipeline.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186f5cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descarga los pesos a la CPU y los cargar en la GPU solo cuando se realiza la pasada hacia adelante \n",
    "\n",
    "pipeline.enable_sequential_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0880f8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamada al pipeline, el cambio en dimensiones también baja el nivel de memoria necesaria\n",
    "\n",
    "frames = pipeline(image=imagen, \n",
    "                  num_inference_steps=60,\n",
    "                  height=512,\n",
    "                  width=512).frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbf1af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# herramienta para exportar el video, lo guarda y devuelve la ruta\n",
    "\n",
    "from diffusers.utils import export_to_video\n",
    "\n",
    "video = export_to_video(frames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dad212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descarga del archivo desde colab\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "files.download(video) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "598px",
    "left": "83px",
    "top": "0px",
    "width": "302.398px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
