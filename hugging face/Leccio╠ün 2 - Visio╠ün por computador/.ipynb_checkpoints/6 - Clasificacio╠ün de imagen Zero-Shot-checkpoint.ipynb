{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e1c37f",
   "metadata": {},
   "source": [
    "# 6 - Clasificación de imagen Zero-Shot\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/img_classification_zero_shot.webp\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027a3b18",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---Modelos-de-clasificación-de-imagen-Zero-Shot\" data-toc-modified-id=\"1---Modelos-de-clasificación-de-imagen-Zero-Shot-1\">1 - Modelos de clasificación de imagen Zero-Shot</a></span></li><li><span><a href=\"#2---Pipeline-de-Transformers-para-clasificación-de-imágenes-Zero-Shot\" data-toc-modified-id=\"2---Pipeline-de-Transformers-para-clasificación-de-imágenes-Zero-Shot-2\">2 - Pipeline de Transformers para clasificación de imágenes Zero-Shot</a></span></li><li><span><a href=\"#3---Usando-el-modelo-clasificador-Zero-Shot\" data-toc-modified-id=\"3---Usando-el-modelo-clasificador-Zero-Shot-3\">3 - Usando el modelo clasificador Zero-Shot</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1---Procesador-de-imágenes\" data-toc-modified-id=\"3.1---Procesador-de-imágenes-3.1\">3.1 - Procesador de imágenes</a></span></li><li><span><a href=\"#3.2---Modelo-clasificador-de-imágenes-Zero-Shot\" data-toc-modified-id=\"3.2---Modelo-clasificador-de-imágenes-Zero-Shot-3.2\">3.2 - Modelo clasificador de imágenes Zero-Shot</a></span></li><li><span><a href=\"#3.3---Resumen-del-código\" data-toc-modified-id=\"3.3---Resumen-del-código-3.3\">3.3 - Resumen del código</a></span></li></ul></li><li><span><a href=\"#4---Uso-del-modelo-Zero-Shot-desde-el-hub\" data-toc-modified-id=\"4---Uso-del-modelo-Zero-Shot-desde-el-hub-4\">4 - Uso del modelo Zero-Shot desde el hub</a></span></li><li><span><a href=\"#5---Más-modelos-de-clasificación-de-imágenes-Zero-Shot\" data-toc-modified-id=\"5---Más-modelos-de-clasificación-de-imágenes-Zero-Shot-5\">5 - Más modelos de clasificación de imágenes Zero-Shot</a></span><ul class=\"toc-item\"><li><span><a href=\"#5.1---Modelo-CLIP-Base\" data-toc-modified-id=\"5.1---Modelo-CLIP-Base-5.1\">5.1 - Modelo CLIP Base</a></span></li><li><span><a href=\"#5.2---Fashion-CLIP\" data-toc-modified-id=\"5.2---Fashion-CLIP-5.2\">5.2 - Fashion CLIP</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda78988",
   "metadata": {},
   "source": [
    "## 1 - Modelos de clasificación de imagen Zero-Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae80ca7",
   "metadata": {},
   "source": [
    "La clasificación de imágenes por Zero-Shot es una técnica en la que un modelo de aprendizaje automático puede clasificar imágenes en categorías que no estaban presentes en el conjunto de datos de entrenamiento. En otras palabras, el modelo puede reconocer y etiquetar clases de objetos que nunca ha visto antes durante su entrenamiento.\n",
    "\n",
    "\n",
    "**Conceptos clave de la clasificación Zero-Shot**:\n",
    "\n",
    "\n",
    "1. Transferencia de conocimiento: La idea principal es transferir conocimiento de clases conocidas, vistas durante el entrenamiento, a clases desconocidas, no vistas durante el entrenamiento, utilizando información adicional, como descripciones semánticas, atributos o embeddings de palabras.\n",
    "\n",
    "\n",
    "2. Espacio común de representación: Los modelos Zero-Shot generalmente mapean tanto las imágenes como las descripciones de clases a un espacio común de representación. En este espacio, las relaciones entre las imágenes y las clases pueden ser comparadas de manera efectiva.\n",
    "\n",
    "\n",
    "3. Fuentes de información semántica: Se utilizan fuentes de información semántica para describir las clases no vistas. Estas fuentes pueden incluir descripciones textuales, atributos visuales, embeddings de palabras, como los generados por Word2Vec o GloVe, y relaciones entre clases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cd14ad",
   "metadata": {},
   "source": [
    "**Flujo de trabajo**:\n",
    "\n",
    "\n",
    "1. Entrenamiento:\n",
    "\n",
    "    + Se entrena un modelo en un conjunto de datos con imágenes etiquetadas, donde las etiquetas están asociadas a descripciones semánticas o atributos.\n",
    "    + El modelo aprende a mapear imágenes a un espacio de características y a asociar estas características con las descripciones semánticas de las clases.\n",
    "\n",
    "\n",
    "2. Mapeo al Espacio Común:\n",
    "\n",
    "    + Tanto las imágenes como las descripciones de las clases se mapean a un espacio común utilizando el modelo entrenado. Esto puede implicar el uso de redes neuronales que procesen tanto imágenes como texto.\n",
    "\n",
    "\n",
    "3. Clasificación Zero-Shot:\n",
    "\n",
    "    + Para clasificar una imagen en una clase no vista, se mapea la imagen al espacio común y se encuentra la clase cuya descripción semántica esté más cercana a la representación de la imagen en ese espacio.\n",
    "    + Esto se puede hacer calculando la similitud, por ejemplo la similitud del coseno, entre la representación de la imagen y las representaciones de las descripciones de las clases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763373bc",
   "metadata": {},
   "source": [
    "**Ventajas y Desafíos**\n",
    "\n",
    "+ Ventajas:\n",
    "\n",
    "    + Flexibilidad: Puede manejar nuevas clases sin necesidad de reentrenamiento.\n",
    "    + Escalabilidad: Facilita la clasificación en un gran número de clases posibles.\n",
    "\n",
    "\n",
    "+ Desafíos:\n",
    "\n",
    "    + Precisión: Puede no ser tan precisa como la clasificación supervisada con datos de entrenamiento específicos para cada clase.\n",
    "    + Dependencia de descripciones semánticas: Requiere descripciones precisas y adecuadas de las clases no vistas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c869cc",
   "metadata": {},
   "source": [
    "## 2 - Pipeline de Transformers para clasificación de imágenes Zero-Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5becd335",
   "metadata": {},
   "source": [
    "Usaremos el modelo [SigLIP](https://huggingface.co/google/siglip-so400m-patch14-384) (shape-optimized model), creado por Google. Está preentrenado con el dataset [WebLi](https://paperswithcode.com/dataset/webli) (Web Language Image) a una resolución de 384x384. Fue introducido en el artículo [Sigmoid Loss for Language Image Pre-Training](https://arxiv.org/abs/2303.15343) por Zhai et al. \n",
    "\n",
    "El modelo está basado en un modelo ViT (Vision Transformer) y es una extensión del modelo [CLIP](https://openai.com/index/clip/), un modelo multimodal de texto e imagen. El cambio con respesto a CLIP es la función de pérdida, pasa de una [softmax](https://es.wikipedia.org/wiki/Funci%C3%B3n_SoftMax) a una función [sigmoide](https://es.wikipedia.org/wiki/Funci%C3%B3n_sigmoide). La pérdida sigmoide opera únicamente en pares de imagen-texto y no requiere una vista global de las similitudes por pares para la normalización. Esto permite aumentar aún más el tamaño del lote, además de mejorar el rendimiento con tamaños de lote más pequeños. Tiene un peso aproximado de 3.5Gb. \n",
    "\n",
    "Vamos a usarlo primero a través del pipeline de transformers, con la foto de un [loro](https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png) y le vamos a preguntar qué hay en la imagen. Las etiquetas candidatas del primer ejemplo serán: animales, humanos y paisaje, escritas en inglés puesto que los modelos entienden mejor este idioma.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818107db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd75cc79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a87d1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c1b2be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f0d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc33a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db25120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65954822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04c51519",
   "metadata": {},
   "source": [
    "La respuesta del pipeline es una lista de diccionarios con las etiquetas propuestas y sus respectivas probabilidades. Vemos que animales es la etiqueta más probable de las tres, con una probabilidad de 0.6%. Podemos pensar que la probabilidad es pequeña, pero ya nos permite etiquetar la imagen según las propuestas que hemos hecho. Hagamos otro ejemplo con la misma imagen para preguntar que tipo de imagen es: en blanco y negro, fotorealista o una pintura. Las escribimos también en inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8df06c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6662fad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d84d16d",
   "metadata": {},
   "source": [
    "En este caso la probabilidad de que la imagen sea en blanco y negro es del 21.4%, las otras dos etiquetas propuestas tienen muy baja probabilidad. Con este método podemos etiquetar las imágenes con nuestras propuestas teniendo como respuesta la probabilidad de pertenencia a esa etiqueta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f2b21a",
   "metadata": {},
   "source": [
    "## 3 - Usando el modelo clasificador Zero-Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28514bd4",
   "metadata": {},
   "source": [
    "Vamos a usar otra imagen y otras etiquetas. Volvamos a la imagen de los [gatos en el sofa](http://images.cocodataset.org/val2017/000000039769.jpg) con las etiquetas gato o perro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06523fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f989787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed5482a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c479acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b82b2506",
   "metadata": {},
   "source": [
    "### 3.1 - Procesador de imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cf782d",
   "metadata": {},
   "source": [
    "AutoProcessor es una clase que se utiliza para manejar el preprocesamiento de datos de manera automática y conveniente para una variedad de modelos. La clase AutoProcessor está diseñada para seleccionar y configurar automáticamente el preprocesador adecuado basado en el tipo de modelo especificado. Esto facilita el proceso de preprocesamiento de datos para el usuario, especialmente cuando se trabaja con diferentes modelos que pueden requerir diferentes tipos de preprocesamiento. En este caso, vamos a cargar el modelo SigLIP para procesar imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bec83af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76197d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "daa29917",
   "metadata": {},
   "source": [
    "**Componentes Principales de SiglipProcessor**\n",
    "\n",
    "\n",
    "1. **Image Processor** (SiglipImageProcessor)\n",
    "\n",
    "    + Encargado del preprocesamiento de imágenes.\n",
    "    + Realiza tareas como redimensionamiento, normalización y reescalado.\n",
    "\n",
    "\n",
    "2. **Tokenizer** (SiglipTokenizer)\n",
    "\n",
    "    + Encargado de la tokenización de texto.\n",
    "    + Convierte el texto en secuencias de tokens que el modelo puede procesar.\n",
    "\n",
    "\n",
    "\n",
    "**Detalles de SiglipImageProcessor**\n",
    "\n",
    "\n",
    "+ **_valid_processor_keys**: Lista de claves válidas para el procesador de imágenes, indicando las configuraciones y opciones que puede manejar.\n",
    "\n",
    "    + images: Imágenes de entrada.\n",
    "    + do_resize: Indica si se deben redimensionar las imágenes.\n",
    "    + size: Tamaño al que se deben redimensionar las imágenes (altura y anchura).\n",
    "    + resample: Método de remuestreo utilizado al redimensionar.\n",
    "    + do_rescale: Indica si se deben reescalar los valores de los píxeles.\n",
    "    + rescale_factor: Factor de reescalado para normalizar los valores de los píxeles.\n",
    "    + do_normalize: Indica si se deben normalizar las imágenes.\n",
    "    + image_mean: Media de los canales de color para la normalización.\n",
    "    + image_std: Desviación estándar de los canales de color para la normalización.\n",
    "    + return_tensors: Especifica si se deben devolver tensores.\n",
    "    + data_format: Formato de los datos de entrada.\n",
    "    + input_data_format: Formato de los datos de entrada.\n",
    "\n",
    "\n",
    "+ **Configuraciones Específicas**:\n",
    "\n",
    "    + do_normalize: true\n",
    "    + do_rescale: true\n",
    "    + do_resize: true\n",
    "    + image_mean: [0.5, 0.5, 0.5]\n",
    "    + image_std: [0.5, 0.5, 0.5]\n",
    "    + image_processor_type: \"SiglipImageProcessor\"\n",
    "    + resample: 3 (método de remuestreo, por ejemplo, bicúbico)\n",
    "    + rescale_factor: 0.00392156862745098 (1/255, para normalizar los valores de los píxeles)\n",
    "    + size: {\"height\": 384, \"width\": 384} (tamaño al que se deben redimensionar las imágenes)\n",
    "\n",
    "\n",
    "\n",
    "**Detalles de SiglipTokenizer**\n",
    "\n",
    "\n",
    "+ name_or_path: Ruta o nombre del modelo preentrenado ('google/siglip-so400m-patch14-384').\n",
    "\n",
    "+ vocab_size: Tamaño del vocabulario (32000).\n",
    "\n",
    "+ model_max_length: Longitud máxima de la secuencia de tokens (64).\n",
    "\n",
    "+ is_fast: Indica si se utiliza una implementación rápida del tokenizador (False).\n",
    "\n",
    "+ padding_side: Lado donde se añade el padding ('right').\n",
    "\n",
    "+ truncation_side: Lado donde se trunca el texto ('right').\n",
    "\n",
    "+ special_tokens: Tokens especiales utilizados en el tokenizador.\n",
    "\n",
    "    + eos_token: `</s>` (End of Sequence)\n",
    "    + unk_token: `<unk>` (Unknown Token)\n",
    "    + pad_token: `</s>` (Padding Token)\n",
    "\n",
    "+ clean_up_tokenization_spaces: Indica si se deben limpiar los espacios de tokenización (True).\n",
    "\n",
    "+ added_tokens_decoder: Decodificador de tokens añadidos manualmente.\n",
    "\n",
    "    + Token ID 1: AddedToken(`</s>`, rstrip=True, lstrip=True, single_word=False, normalized=False, special=True)\n",
    "    + Token ID 2: AddedToken(`<unk>`, rstrip=True, lstrip=True, single_word=False, normalized=False, special=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf8b4ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb283fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459113d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "546ee8eb",
   "metadata": {},
   "source": [
    "### 3.2 - Modelo clasificador de imágenes Zero-Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef69362",
   "metadata": {},
   "source": [
    "AutoModelForZeroShotImageClassification es una clase de la biblioteca transformers diseñada para la clasificación de imágenes Zero-Shot. Con esta clase vamos a poder cargar el modelo Zero-Shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d6d3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52f7e58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "594286af",
   "metadata": {},
   "source": [
    "El SiglipModel es un modelo dual que combina un transformador de texto y un transformador de visión, cada uno con sus propias capas de embeddings, encoders, y capas finales. Ambas partes del modelo utilizan mecanismos de atención y MLPs, perceptrones, para procesar sus respectivas entradas, texto e imágenes, y generar las respectivas salidas. El resumen de la arquitectura es como sigue:\n",
    "\n",
    "\n",
    "**Modelo de Texto (text_model)**:\n",
    "\n",
    "\n",
    "1. **Embeddings**:\n",
    "\n",
    "    + token_embedding: Una capa de embedding para los tokens de texto con una dimensión de vocabulario de 32000 y una dimensión de embedding de 1152.\n",
    "    + position_embedding: Una capa de embedding para las posiciones de los tokens en la secuencia, con una dimensión de 64 y una dimensión de embedding de 1152.\n",
    "\n",
    "\n",
    "2. **Encoder**:\n",
    "\n",
    "    + Compuesto por una lista de capas, específicamente 27 capas (0-26). Cada SiglipEncoderLayer contiene:\n",
    "        + self_attn (Self-Attention): Con proyecciones k_proj, v_proj, q_proj y out_proj, todas con dimensiones de entrada y salida de 1152.\n",
    "        + layer_norm1: Normalización por capas con dimensión de 1152.\n",
    "        + mlp (Multilayer Perceptron): Contiene una función de activación GELUTanh (Gaussian Error Linear Unit Tangent Hyperbolic), y dos capas totalmente conectadas (fc1 y fc2) con dimensiones 1152 a 4304 y 4304 a 1152, respectivamente.\n",
    "        + layer_norm2: Otra normalización por capas con dimensión de 1152.\n",
    "\n",
    "\n",
    "3. **Final Layer Norm**: Una normalización por capas final con dimensión de 1152.\n",
    "\n",
    "\n",
    "4. **Head**: Una capa lineal con dimensiones de entrada y salida de 1152.\n",
    "\n",
    "\n",
    "\n",
    "**Modelo de Visión (vision_model)**:\n",
    "\n",
    "\n",
    "1. **Embeddings**:\n",
    "\n",
    "    + patch_embedding: Una capa de convolución (Conv2d) para crear embeddings de paquetes de imágenes, con 3 canales de entrada y 1152 canales de salida, y un tamaño de kernel de 14x14 con stride de 14x14.\n",
    "    + position_embedding: Una capa de embedding para las posiciones de los paquetes en la imagen, con una dimensión de 729 y una dimensión de embedding de 1152.\n",
    "\n",
    "\n",
    "2. **Encoder**:\n",
    "\n",
    "    + Similar al encoder del modelo de texto, compuesto por 27 capas (0-26).\n",
    "    + Cada SiglipEncoderLayer contiene los mismos componentes que el encoder del modelo de texto.\n",
    "\n",
    "\n",
    "3. **Post Layer Norm**: Una normalización por capas posterior con dimensión de 1152.\n",
    "\n",
    "\n",
    "4. **Head**:\n",
    "\n",
    "    + SiglipMultiheadAttentionPoolingHead:\n",
    "        + attention: Una capa de atención con múltiples cabezas (MultiheadAttention), con una proyección de salida (out_proj) con dimensiones de entrada y salida de 1152.\n",
    "        + layernorm: Una normalización por capas con dimensión de 1152.\n",
    "        + mlp: Un MLP con una función de activación GELUTanh, y dos capas totalmente conectadas (fc1 y fc2) con dimensiones 1152 a 4304 y 4304 a 1152, respectivamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c39ec20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153644bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84506c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f010e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5839d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a46963a9",
   "metadata": {},
   "source": [
    "### 3.3 - Resumen del código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77befe4",
   "metadata": {},
   "source": [
    "Vamos a poner todo el código junto en una sola función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ae7232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd51cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa2a67ab",
   "metadata": {},
   "source": [
    "## 4 - Uso del modelo Zero-Shot desde el hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446034a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb2272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d07c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54061e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2501463a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b4860f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df668e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e0dedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de89044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9d111e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3aca31e1",
   "metadata": {},
   "source": [
    "## 5 - Más modelos de clasificación de imágenes Zero-Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeda860",
   "metadata": {},
   "source": [
    "### 5.1 - Modelo CLIP Base\n",
    "\n",
    "[CLIP](https://huggingface.co/openai/clip-vit-base-patch32) (Contrastive Language–Image Pre-Training) es un modelo desarrollado por OpenAI que combina procesamiento de texto e imágenes. Utiliza un enfoque de aprendizaje contrastivo para alinear representaciones de texto e imágenes en un espacio compartido. Es la base del modelo que acabamos de usar y tiene la misma estructura con dos componentes, el codificador de texto y el codificador de imagen, aunque pesa menos, unos 610Mb. Vamos a usarlo con el pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6df7c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af90f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227c78c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f322e8bd",
   "metadata": {},
   "source": [
    "En este caso, las probabilidades de ambas etiquetas salen mucho más altas. Aunque sigue haciéndolo correctamente, la probabilidad más alta es la calle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f305634",
   "metadata": {},
   "source": [
    "### 5.2 - Fashion CLIP\n",
    "\n",
    "Existen modelos entrenados para tareas específicas como [Fashion CLIP](https://huggingface.co/patrickjohncyh/fashion-clip). Este modelo está basado en CLIP, desarrollado para producir representaciones generales de productos para conceptos de moda. Aprovechando el punto de control preentrenado (ViT-B/32) publicado por OpenAI, se ha entrenado Fashion CLIP en un conjunto de datos de moda de alta calidad para estudiar si el ajuste fino específico del dominio de modelos tipo CLIP es suficiente para producir representaciones de productos que sean transferibles en zero-shot a conjuntos de datos y tareas completamente nuevos. Este modelo pesa unos 610Mb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd313085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efc67bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c4a2ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3bea16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07833b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7963d97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668bdc55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ea35d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "599px",
    "left": "72px",
    "top": "0px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
