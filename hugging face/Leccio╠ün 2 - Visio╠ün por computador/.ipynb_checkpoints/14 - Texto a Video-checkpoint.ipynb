{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d28fc8a1",
   "metadata": {},
   "source": [
    "# 14 - Texto a Video\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/text2video.webp\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e402d850",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---Modelos-de-texto-a-video\" data-toc-modified-id=\"1---Modelos-de-texto-a-video-1\">1 - Modelos de texto a video</a></span></li><li><span><a href=\"#2---Pipeline-de-Diffusers-para-modelos-de-texto-a-video\" data-toc-modified-id=\"2---Pipeline-de-Diffusers-para-modelos-de-texto-a-video-2\">2 - Pipeline de Diffusers para modelos de texto a video</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1---Zeroscope\" data-toc-modified-id=\"2.1---Zeroscope-2.1\">2.1 - Zeroscope</a></span></li><li><span><a href=\"#2.2---AnimateDiff\" data-toc-modified-id=\"2.2---AnimateDiff-2.2\">2.2 - AnimateDiff</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f075bdaa",
   "metadata": {},
   "source": [
    "## 1 - Modelos de texto a video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ec5ab1",
   "metadata": {},
   "source": [
    "Los modelos de texto a video generan secuencias de video a partir de descripciones textuales. Estos modelos representan un avance significativo en el campo de la síntesis de medios, permitiendo la creación de videos coherentes y visualmente realistas a partir de instrucciones verbales. \n",
    "\n",
    "\n",
    "**Componentes principales**:\n",
    "\n",
    "+ Procesamiento de lenguaje natural (NLP): El componente de NLP interpreta y entiende la descripción textual.\n",
    "\n",
    "+ Generación de video: Utiliza redes neuronales para generar cuadros de video secuenciales que representan la descripción textual.\n",
    "\n",
    "\n",
    "\n",
    "**Funcionamiento**\n",
    "\n",
    "+ Entrada: Una descripción textual detallada de la escena o acción que se desea generar en el video.\n",
    "\n",
    "+ Salida: Un video que visualiza la escena descrita en el texto.\n",
    "\n",
    "\n",
    "**Proceso de Generación**:\n",
    "\n",
    "+ Análisis del texto: El texto de entrada se analiza para extraer información clave sobre la escena, objetos, acciones, y su disposición espacial y temporal.\n",
    "\n",
    "+ Síntesis de video: A partir de esta información, el modelo genera una secuencia de cuadros que, al reproducirse en sucesión, forman un video coherente.\n",
    "\n",
    "\n",
    "\n",
    "**Modelos y Técnicas**:\n",
    "\n",
    "+ Transformers: Los modelos basados en transformadores pueden ser adaptados para generar videos mediante la creación de descripciones detalladas cuadro a cuadro.\n",
    "\n",
    "+ GANs: Modelos como VideoGAN utilizan GANs para generar videos realistas. La red generativa crea cuadros de video, mientras que la red discriminativa evalúa su realismo.\n",
    "\n",
    "+ VAEs: Los VAEs pueden ser utilizados para aprender una representación latente de los cuadros de video y generar nuevas secuencias de video basadas en esta representación.\n",
    "\n",
    "+ Modelos Híbridos: Combinan diferentes técnicas, como NLP y GANs, para mejorar la coherencia y calidad del video generado.\n",
    "\n",
    "\n",
    "\n",
    "**Aplicaciones**:\n",
    "\n",
    "+ Cine y animación: Generar escenas de películas o animaciones a partir de guiones escritos o crear versiones preliminares de escenas para la planificación y el diseño de producciones cinematográficas.\n",
    "\n",
    "+ Marketing y publicidad: Generar videos promocionales a partir de descripciones de productos o servicios o crear anuncios personalizados basados en descripciones de intereses o comportamientos del usuario.\n",
    "\n",
    "+ Educación y capacitación: Crear videos educativos a partir de textos explicativos, facilitando el aprendizaje visual o generar simulaciones para entrenamiento en diferentes campos, como la medicina o la ingeniería.\n",
    "\n",
    "+ Entretenimiento y medios sociales: Crear contenido de video original para plataformas de medios sociales a partir de descripciones textuales o generar videos en tiempo real basados en las entradas textuales de los usuarios.\n",
    "\n",
    "\n",
    "\n",
    "**Desafíos**\n",
    "\n",
    "+ Coherencia temporal: Mantener la coherencia y continuidad a lo largo de los cuadros de video para asegurar una reproducción fluida.\n",
    "\n",
    "+ Calidad visual: Generar cuadros de video de alta calidad que no presenten artefactos visuales y mantengan detalles importantes.\n",
    "\n",
    "+ Comprensión del contexto: Entender y representar correctamente el contexto y las relaciones espaciales y temporales descritas en el texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d856d2",
   "metadata": {},
   "source": [
    "Este Jupyter Notebook será ejecutado en Google Colab en el siguiente [link](https://colab.research.google.com/drive/1_ni8mKoSjH7SxFiYGJZZoM6ABVyp_KIj?usp=sharing) debido a la necesidad de GPU y también porque `mps` no acepta redes Conv3D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d65aec",
   "metadata": {},
   "source": [
    "## 2 - Pipeline de Diffusers para modelos de texto a video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe2eff1",
   "metadata": {},
   "source": [
    "### 2.1 - Zeroscope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8584304",
   "metadata": {},
   "source": [
    "Vamos a probar dos modelos distintos de generación de video desde texto. El primero es un modelo [zeroscope](https://huggingface.co/cerspense/zeroscope_v2_576w) optimizado para producir composiciones de alta calidad en formato 16:9 y una salida de video fluida. Este modelo fue entrenado utilizando 9923 clips y 29769 fotogramas etiquetados a 24 cuadros por segundo, con una resolución de 576x320. Tiene un peso aproximado de 3Gb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cfc8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalaciones para colab\n",
    "\n",
    "!pip install transformers\n",
    "!pip install diffusers\n",
    "!pip install torch\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402774e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para quitar warnings \n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cf7910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las librerias torch, el pipeline y el scheduler de diffusers\n",
    "\n",
    "import torch  \n",
    "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74334a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos el modelo\n",
    "\n",
    "modelo = 'cerspense/zeroscope_v2_576w'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea65150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicializamos el pipeline\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(pretrained_model_name_or_path=modelo, \n",
    "                                         torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd2a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambio de dispositivo y añadido del scheduler al pipeline\n",
    "\n",
    "pipe.to('cuda')\n",
    "\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5154b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texto para generar el video\n",
    "\n",
    "prompt = 'Darth Vader is surfing on waves with a light saber in his hand'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9de75ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generación del video frame a frame como array de numpy\n",
    "\n",
    "frames = pipe(prompt=prompt, \n",
    "              num_inference_steps=40, \n",
    "              height=320, \n",
    "              width=576, \n",
    "              num_frames=24).frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7cdc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# herramienta para exportar el video, lo guarda y devuelve la ruta\n",
    "\n",
    "from diffusers.utils import export_to_video\n",
    "\n",
    "video = export_to_video(frames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac183b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descarga del archivo desde colab\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "files.download(video) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16c416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# liberación de memoria\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "del pipe, frames, video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d1b65",
   "metadata": {},
   "source": [
    "### 2.2 - AnimateDiff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7e9b97",
   "metadata": {},
   "source": [
    "Probemos otro modelo, llamado [AnimateDiff](https://huggingface.co/ByteDance/AnimateDiff-Lightning), el cual es un adaptador de movimiento que se coloca sobre otro modelo a nuestra elección. Veamos como se usa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e998e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos el pipeline y el adaptador desde diffusers\n",
    "\n",
    "from diffusers import AnimateDiffPipeline, MotionAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b00d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descarga el repo desde el hub\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fd49ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos el modelo, el repositorio y el checkpoint del adaptador\n",
    "\n",
    "repo = 'ByteDance/AnimateDiff-Lightning'\n",
    "\n",
    "checkpoint = 'animatediff_lightning_8step_diffusers.safetensors'\n",
    "\n",
    "modelo_base = 'emilianJR/epiCRealism'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756dbff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicializamos el adaptador y el pipeline, con el cambio de dispositivo a GPU\n",
    "\n",
    "adaptador = MotionAdapter()\n",
    "\n",
    "adaptador.to('cuda', torch.float16)\n",
    "\n",
    "adaptador.load_state_dict(load_file(hf_hub_download(repo ,checkpoint), device='cuda'))\n",
    "\n",
    "\n",
    "\n",
    "pipe = AnimateDiffPipeline.from_pretrained(modelo_base, \n",
    "                                           motion_adapter=adaptador, \n",
    "                                           torch_dtype=torch.float16)\n",
    "\n",
    "pipe = pipe.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137c2015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos y establecemos el scheduler\n",
    "\n",
    "from diffusers import EulerDiscreteScheduler\n",
    "\n",
    "pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, \n",
    "                                                    timestep_spacing='trailing', \n",
    "                                                    beta_schedule='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be667b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texto para generar el video\n",
    "\n",
    "prompt = 'A girl dancing and smiling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamada al pipeline\n",
    "\n",
    "resultado =  pipe(prompt=prompt, \n",
    "                  guidance_scale=1.0, \n",
    "                  num_inference_steps=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b2622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruta de guardado\n",
    "\n",
    "ruta = '/tmp/animation.gif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd48c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardado como gif\n",
    "\n",
    "from diffusers.utils import export_to_gif\n",
    "\n",
    "export_to_gif(resultado.frames[0], ruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c708883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualización del gif\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "display(Image(data=open(ruta,'rb').read(), format='png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35aa025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descarga del archivo desde colab\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "files.download(ruta) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "598px",
    "left": "58px",
    "top": "0px",
    "width": "302.398px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
